{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd60a10-aba2-4f3e-b251-77d4f100179f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-jzepr01p\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-jzepr01p\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 48327c57182fdade7f7797d1eaad2d166de5c55b\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (1.23.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.28.0.dev0) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.28.0.dev0) (2019.11.28)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: librosa in /usr/local/lib/python3.9/dist-packages (0.10.0.post2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.23.4)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.56.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.0.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (4.4.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.9/dist-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.3.4)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.1)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.0->librosa) (66.1.1)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pooch<1.7,>=1.0->librosa) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from pooch<1.7,>=1.0->librosa) (23.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install-U yt-dlp==2023.1.6 matplotlib==3.6.0 datasets[audio] rich\n",
    "\n",
    "# install newest transformers build to be able to pass `inputs_embeds` through generate()\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f84841-b150-445f-8cf3-a9e5f6da2cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.0.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (66.1.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.26.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U torch torchaudio --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3b32b1-a434-479e-8c62-92e8da10e672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.9/dist-packages (0.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.23.4)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.1.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.4.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (18.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f7c26e4-1d78-4d2e-a49c-a578ba0b6d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from musiccaps import load_musiccaps\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "\n",
    "from rich import print as printr\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import evaluate\n",
    "import itertools\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, './sota-music-tagging-models/training')\n",
    "import model as sota_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485da5-3e40-42f1-873e-ccb244a74bc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load musiccaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8926e06-ef8c-40a7-8675-f4d1285d0afe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration google--MusicCaps-bedc2a0fd7888f2f\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    }
   ],
   "source": [
    "ds = load_musiccaps(\n",
    "    \"./music_data\",\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True,\n",
    ")\n",
    "\n",
    "music_files = {f.stem: f for f in Path('./music_npys/').iterdir()}\n",
    "\n",
    "with open('summarized_captions.json') as f:\n",
    "    summarized_captions = json.load(f)\n",
    "    \n",
    "ds = ds.add_column(\"summarized_captions\", list(summarized_captions.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15420704-a6bd-4315-b6b9-ba41fee54a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CaptionMusicDataset(Dataset):\n",
    "    \"\"\"Returns a torch Dataset of paired captions and music files\"\"\"\n",
    "\n",
    "    def __init__(self, muscaps_ds, music_files, preds_mode=False):\n",
    "        include_ytids = set(muscaps_ds['ytid']) & set(music_files.keys())\n",
    "        include_inds = [i for i, ytid in enumerate(muscaps_ds['ytid']) if ytid in include_ytids]\n",
    "        ds = muscaps_ds.select(include_inds)\n",
    "        assert len(ds) == len(music_files)\n",
    "\n",
    "        self.ytids_sorted = ds.sort(\"ytid\")[\"ytid\"]\n",
    "        self.caption_list = ds.sort(\"ytid\")[\"summarized_captions\"]\n",
    "        self.sorted_music_files = [music_files[ytid] for ytid in self.ytids_sorted]\n",
    "        self.preds_mode = preds_mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sorted_music_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        music = np.load(self.sorted_music_files[idx], allow_pickle=True)\n",
    "        music = np.stack([music[:80000], music[-80000:]])\n",
    "        \n",
    "        if self.preds_mode:\n",
    "            return list(self.caption_list[idx]), music, self.ytids_sorted[idx]\n",
    "        \n",
    "        caption = np.random.choice(self.caption_list[idx])\n",
    "\n",
    "        return caption, music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c18681-2a4f-41aa-b0ec-8d77a6880cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-9a9e3bf18ce5ccf8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-762f4c6e9e04b27d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-533892882886e54e.arrow\n"
     ]
    }
   ],
   "source": [
    "use_chat_aug = True\n",
    "\n",
    "with open('chataug.json', 'r') as fp:\n",
    "    chataug_captions = json.load(fp)\n",
    "    \n",
    "with open('musiccaps_split.json', 'r') as fp:\n",
    "    musiccaps_split = json.load(fp)\n",
    "\n",
    "train_ytids, valid_ytids, test_ytids = musiccaps_split['train'], musiccaps_split['valid'], musiccaps_split['test']\n",
    "\n",
    "train_ds = ds.filter(lambda x: x['ytid'] in train_ytids)\n",
    "valid_ds = ds.filter(lambda x: x['ytid'] in valid_ytids)\n",
    "test_ds = ds.filter(lambda x: x['ytid'] in test_ytids)\n",
    "\n",
    "train_music_files = {ytid: e for ytid, e in music_files.items() if ytid in train_ytids}\n",
    "valid_music_files = {ytid: e for ytid, e in music_files.items() if ytid in valid_ytids}\n",
    "test_music_files = {ytid: e for ytid, e in music_files.items() if ytid in test_ytids}\n",
    "\n",
    "training_data = CaptionMusicDataset(muscaps_ds=train_ds, music_files=train_music_files)\n",
    "valid_data = CaptionMusicDataset(muscaps_ds=valid_ds, music_files=valid_music_files)\n",
    "test_data = CaptionMusicDataset(muscaps_ds=test_ds, music_files=test_music_files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17f0b38f-9628-4cdb-a519-ecbfd4f6cf13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4384, 549, 549)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3d9b-6745-43f7-91b4-1860ca8ae81b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82925589-cbb3-4f35-9099-3eb29831a29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.fc(torch.nn.functional.relu(x))\n",
    "\n",
    "class B2T(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(B2T, self).__init__()\n",
    "        self.hcnn = sota_model.HarmonicCNNCropped().to(device)\n",
    "        state_dict = torch.load(f'sota-music-tagging-models/models/jamendo/hcnn/best_model.pth',\n",
    "                        map_location=device)\n",
    "        self.hcnn.load_state_dict(state_dict, strict=False)\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(256, 768),\n",
    "            nn.Dropout(0.7),\n",
    "            ResidualLinear(768),\n",
    "            nn.Dropout(0.5),\n",
    "            ResidualLinear(768),\n",
    "        )\n",
    "        \n",
    "    def forward(self, audio_array):\n",
    "        audio_features = (self.hcnn(audio_array[:, 0, :])+self.hcnn(audio_array[:, 1, :]))/2\n",
    "        audio_embedding = self.fc_net(audio_features)\n",
    "    \n",
    "        return audio_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e0d47c-b1e4-4837-bc6b-5f9b20d84ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(captions_batch):\n",
    "    input_ids = tokenizer(captions_batch)[\"input_ids\"]\n",
    "\n",
    "    # wrap in eos and add placeholder for music embedding/mask\n",
    "    input_ids = [torch.tensor([placeholder_id] + x + [eos_id]) for x in input_ids]\n",
    "    # pad with -100, this index is masked in the cross-entropy loss\n",
    "    input_ids_target = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=mask_id\n",
    "    ).to(device)\n",
    "\n",
    "    # index -100 isn't valid as model input however, since the token embedding lookup fails,\n",
    "    # so we need a second version as model input, with -100 replaced with another token (shouldn't matter which)\n",
    "    input_ids = input_ids_target.clone()\n",
    "    input_ids[input_ids == mask_id] = eos_id\n",
    "\n",
    "    return input_ids, input_ids_target\n",
    "\n",
    "\n",
    "def transform_input_ids(music_array, input_ids, input_ids_target):\n",
    "    music_emb_ind = 0  # 1 if using <bos>, otherwise 0\n",
    "    assert (input_ids[:, music_emb_ind] == placeholder_id).all()\n",
    "    assert (input_ids_target[:, music_emb_ind] == placeholder_id).all()\n",
    "\n",
    "    input_ids_target[:, music_emb_ind] = eos_id  # mask_id\n",
    "    input_ids[:, music_emb_ind] = eos_id  # temp placeholder to make the embedding lookup work\n",
    "    inputs_embeds = model.transformer.wte(input_ids)\n",
    "    inputs_embeds[:, music_emb_ind] = b2t(music_array.cuda())  # insert music embedding\n",
    "\n",
    "    return inputs_embeds, input_ids_target\n",
    "\n",
    "\n",
    "def strip_eos(pred):\n",
    "    \"\"\" \n",
    "    remove eos tokens from predicted captions \n",
    "    discards everything after the first <eos> that isn't the very first token\n",
    "    (the hf can only skip eos but not stop at eos) \n",
    "    \"\"\"\n",
    "    pred = [p.removeprefix(\"<|endoftext|>\") for p in pred]\n",
    "    pred = [p[: p.find(\"<|endoftext|>\")] if \"<|endoftext|>\" in p else p for p in pred]\n",
    "    return pred\n",
    "\n",
    "\n",
    "def eval(caption_batch, audio_batch, rm_eos=False, **kwargs):\n",
    "    model.eval()\n",
    "    b2t.eval()\n",
    "    input_ids, input_ids_target = tokenize(caption_batch)\n",
    "    inputs_embeds, _ = transform_input_ids(audio_batch, input_ids, input_ids_target)\n",
    "\n",
    "    # only include <bos> (optional) and music_embedding, don't include true caption\n",
    "    inputs_embeds = inputs_embeds[:, :1]\n",
    "    output_ids = model.generate(inputs_embeds=inputs_embeds, **kwargs)\n",
    "    pred = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "    pred = [p.replace(\"\\n\", \"\").strip() for p in pred]\n",
    "    return strip_eos(pred) if rm_eos else pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eafbbd8e-d913-4059-b28a-ee995e78de74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_step(inputs_embeds, input_ids_target, apply_grad):\n",
    "    model.train()\n",
    "    b2t.train()\n",
    "    loss = model.forward(inputs_embeds=inputs_embeds, labels=input_ids_target).loss\n",
    "    loss.backward()\n",
    "    \n",
    "    if apply_grad:\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def eval_step(string_info=\"\"):\n",
    "    \n",
    "    caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "    pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "    printr('[green bold]TRAIN TRUE: ' + caption_batch[0])\n",
    "    printr('[yellow]TRAIN PRED: ' + pred[0])\n",
    "    wlog('TRAIN TRUE: ' + caption_batch[0])\n",
    "    wlog('TRAIN PRED: ' + pred[0])\n",
    "    caption_batch, embedding_batch = next(iter(eval_valid_dataloader))\n",
    "    pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "    printr('[green bold]TEST TRUE: ' + caption_batch[0])\n",
    "    printr('[yellow]TEST PRED: ' + pred[0])\n",
    "    wlog('TEST TRUE: ' + caption_batch[0])\n",
    "    wlog('TEST PRED: ' + pred[0])\n",
    "    print()\n",
    "    \n",
    "def metrics_step(n=100, shuffles=10):\n",
    "    \n",
    "    lq = 0\n",
    "    \n",
    "    print(f\"Computing metrics for n={n}\")\n",
    "    wlog(f\"Computing metrics for n={n}\")\n",
    "    \n",
    "    shuffled_meteor_valid, shuffled_bleu_valid = 0., 0.\n",
    "    train_captions, train_preds = [], []\n",
    "    valid_captions, valid_preds = [], []\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "\n",
    "        caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        train_captions.append(caption_batch[0])\n",
    "        train_preds.append(pred[0])\n",
    "\n",
    "        caption_batch, embedding_batch = next(iter(eval_valid_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        valid_captions.append(caption_batch[0])\n",
    "        valid_preds.append(pred[0])\n",
    "        if \"The low quality recording\" in pred[0]:\n",
    "            lq += 1\n",
    "\n",
    "    meteor_train = meteor.compute(predictions=train_preds, references=train_captions)['meteor']\n",
    "    bleu_train = google_bleu.compute(predictions=train_preds, references=train_captions)['google_bleu']\n",
    "\n",
    "    meteor_valid = meteor.compute(predictions=valid_preds, references=valid_captions)['meteor']\n",
    "    bleu_valid = google_bleu.compute(predictions=valid_preds, references=valid_captions)['google_bleu']\n",
    "    \n",
    "    for j in range(shuffles):\n",
    "        shuffled_captions = sorted(valid_captions, key=lambda k: random.random())\n",
    "        shuffled_meteor_valid += (1./(shuffles))*meteor.compute(predictions=valid_preds, references=shuffled_captions)['meteor']\n",
    "        shuffled_bleu_valid += (1./(shuffles))*google_bleu.compute(predictions=valid_preds, references=shuffled_captions)['google_bleu']\n",
    "\n",
    "    spec_meteor = meteor_valid - shuffled_meteor_valid\n",
    "    spec_bleu = bleu_valid - shuffled_bleu_valid\n",
    "        \n",
    "    print(f\"Train meteor: {meteor_train:.4f}, Train bleu: {bleu_train:.4f}\")\n",
    "    wlog(f\"Train meteor: {meteor_train:.4f}, Train bleu: {bleu_train:.4f}\")\n",
    "    print(f\"Valid meteor: {meteor_valid:.4f}, Valid bleu: {bleu_valid:.4f}\")\n",
    "    wlog(f\"Valid meteor: {meteor_valid:.4f}, Valid bleu: {bleu_valid:.4f}\")\n",
    "    print(f\"Valid spec-meteor: {spec_meteor:.4f}, Valid spec-bleu: {spec_bleu:.4f}\")\n",
    "    wlog(f\"Valid spec-meteor: {spec_meteor:.4f}, Valid spec-bleu: {spec_bleu:.4f}\")\n",
    "    print(f\"Low quality recording count: {lq}\")\n",
    "    wlog(f\"Low quality recording count: {lq}\")\n",
    "    \n",
    "    return {'meteor_train': meteor_train,\n",
    "            'meteor_valid': meteor_valid,\n",
    "            'bleu_train': bleu_train,\n",
    "            'bleu_valid': bleu_valid,\n",
    "            'spec_meteor': spec_meteor,\n",
    "            'spec_bleu': spec_bleu} \n",
    "\n",
    "m_styles = {\"meteor_train\": (\"tab:orange\", \"solid\"),\n",
    "    \"meteor_valid\": (\"tab:orange\", \"dashed\"),\n",
    "    \"bleu_train\": (\"tab:blue\", \"solid\"),\n",
    "    \"bleu_valid\": (\"tab:blue\", \"dashed\"),\n",
    "    \"spec_bleu\": (\"tab:blue\", \"dashdot\"),\n",
    "    \"spec_meteor\": (\"tab:orange\", \"dashdot\")\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43aad078-36f3-4a47-9e2e-9c02371a52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def metrics_step(n=100, shuffles=10):\n",
    "    \n",
    "    print(f\"Computing metrics for n={n}\")\n",
    "    wlog(f\"Computing metrics for n={n}\")\n",
    "    \n",
    "    shuffled_meteor_valid, shuffled_bleu_valid = 0., 0.\n",
    "    train_captions, train_preds = [], []\n",
    "    valid_captions, valid_preds = [], []\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "\n",
    "        caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        train_captions.append(re.sub(r'[^\\w\\s]','',caption_batch[0]))\n",
    "        train_preds.append(re.sub(r'[^\\w\\s]','',pred[0]))\n",
    "\n",
    "        caption_batch, embedding_batch = next(iter(eval_valid_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        valid_captions.append(re.sub(r'[^\\w\\s]','',caption_batch[0]))\n",
    "        valid_preds.append(re.sub(r'[^\\w\\s]','',pred[0]))\n",
    "\n",
    "    meteor_train = meteor.compute(predictions=train_preds, references=train_captions)['meteor']\n",
    "    bleu_train = google_bleu.compute(predictions=train_preds, references=train_captions)['google_bleu']\n",
    "\n",
    "    meteor_valid = meteor.compute(predictions=valid_preds, references=valid_captions)['meteor']\n",
    "    bleu_valid = google_bleu.compute(predictions=valid_preds, references=valid_captions)['google_bleu']\n",
    "    \n",
    "    for j in range(shuffles):\n",
    "        shuffled_captions = sorted(valid_captions, key=lambda k: random.random())\n",
    "        shuffled_meteor_valid += (1./(shuffles))*meteor.compute(predictions=valid_preds, references=shuffled_captions)['meteor']\n",
    "        shuffled_bleu_valid += (1./(shuffles))*google_bleu.compute(predictions=valid_preds, references=shuffled_captions)['google_bleu']\n",
    "\n",
    "    spec_meteor = meteor_valid - shuffled_meteor_valid\n",
    "    spec_bleu = bleu_valid - shuffled_bleu_valid\n",
    "        \n",
    "    print(f\"Train meteor: {meteor_train:.4f}, Train bleu: {bleu_train:.4f}\")\n",
    "    wlog(f\"Train meteor: {meteor_train:.4f}, Train bleu: {bleu_train:.4f}\")\n",
    "    print(f\"Valid meteor: {meteor_valid:.4f}, Valid bleu: {bleu_valid:.4f}\")\n",
    "    wlog(f\"Valid meteor: {meteor_valid:.4f}, Valid bleu: {bleu_valid:.4f}\")\n",
    "    print(f\"Valid spec-meteor: {spec_meteor:.4f}, Valid spec-bleu: {spec_bleu:.4f}\")\n",
    "    wlog(f\"Valid spec-meteor: {spec_meteor:.4f}, Valid spec-bleu: {spec_bleu:.4f}\")\n",
    "    \n",
    "    return {'meteor_train': meteor_train,\n",
    "            'meteor_valid': meteor_valid,\n",
    "            'bleu_train': bleu_train,\n",
    "            'bleu_valid': bleu_valid,\n",
    "            'spec_meteor': spec_meteor,\n",
    "            'spec_bleu': spec_bleu} \n",
    "\n",
    "m_styles = {\"meteor_train\": (\"tab:orange\", \"solid\"),\n",
    "    \"meteor_valid\": (\"tab:orange\", \"dashed\"),\n",
    "    \"bleu_train\": (\"tab:blue\", \"solid\"),\n",
    "    \"bleu_valid\": (\"tab:blue\", \"dashed\"),\n",
    "    \"spec_bleu\": (\"tab:blue\", \"dashdot\"),\n",
    "    \"spec_meteor\": (\"tab:orange\", \"dashdot\")\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c6b337-a156-4125-80fe-27229603dd46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'enc_summarized_5e-05_5e-05_0.0001_chataug'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'gpt2' # gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.config.is_decoder = True # not sure if necessary\n",
    "\n",
    "mask_id = -100 # don't change, this is fixed in torch cross-entropy loss!\n",
    "eos_id = tokenizer.eos_token_id\n",
    "placeholder_id = -200\n",
    "b2t = B2T().cuda()\n",
    "hcnn_lr = 5e-5\n",
    "b2t_lr = 1e-4\n",
    "gpt2_finetune_lr = 5e-5\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "    # b2t needs to be the first parameter group\n",
    "    {'params': b2t.hcnn.layer4.parameters(), 'lr': hcnn_lr/2},\n",
    "    {'params': b2t.hcnn.layer5.parameters(), 'lr': hcnn_lr/2},\n",
    "    {'params': b2t.hcnn.layer6.parameters(), 'lr': hcnn_lr},\n",
    "    {'params': b2t.hcnn.layer7.parameters(), 'lr': hcnn_lr},\n",
    "    {'params': b2t.fc_net.parameters(), 'lr': b2t_lr},\n",
    "    \n",
    "    # disable AdamW weight decay for gpt2 layer finetuning\n",
    "    {'params': model.transformer.h[1].parameters(), 'lr': gpt2_finetune_lr, 'weight_decay': 0},\n",
    "    {'params': model.transformer.h[2].parameters(), 'lr': gpt2_finetune_lr, 'weight_decay': 0}\n",
    "])\n",
    "\n",
    "batch_size = 24\n",
    "num_epochs = 500\n",
    "epoch = 104\n",
    "gradient_acc_fact = 1\n",
    "gpt2_finetune_start_epoch = 0\n",
    "\n",
    "generation_params = dict(\n",
    "    max_new_tokens=64,\n",
    "    num_beams=4,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    bos_token_id=eos_id,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=mask_id,\n",
    "    early_stopping=True,\n",
    "    rm_eos=True,\n",
    ")\n",
    "\n",
    "losses = []\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "eval_train_dataloader = DataLoader(training_data, 1, shuffle=True)\n",
    "eval_valid_dataloader = DataLoader(valid_data, 1, shuffle=True)\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load(\"google_bleu\")\n",
    "metrics = {'step': [], 'bleu_train': [], 'bleu_valid': [], 'meteor_train': [], 'meteor_valid': [],\n",
    "          'spec_bleu': [], 'spec_meteor':[]}\n",
    "\n",
    "checkpoint_dir = Path('checkpoints')\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "load_pretrain = True\n",
    "keep_training = True\n",
    "\n",
    "pretrain_model =  \"checkpoints/chkp_enc_summarized_5e-05_5e-05_0.0001_chataug.pt\"\n",
    "if load_pretrain:\n",
    "    data_dict = torch.load(pretrain_model)\n",
    "    model.load_state_dict(data_dict['model'])\n",
    "    b2t.load_state_dict(data_dict['b2t'])\n",
    "    \n",
    "model_name_info = \"small\" if model_name == 'gpt2' else model_name\n",
    "pretraining_info = \"yes\" if load_pretrain and not keep_training else \"no\"\n",
    "chat_aug_info = \"_chataug\" if use_chat_aug else \"\"\n",
    "string_info = f\"enc_summarized_{hcnn_lr}_{gpt2_finetune_lr}_{b2t_lr}{chat_aug_info}\"\n",
    "\n",
    "def wlog(s):\n",
    "    f = open(f\"outputs/logs_{string_info}.txt\",'a')\n",
    "    f.write(s+\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "if keep_training:\n",
    "    with open(f'outputs/train_metrics_{string_info}.npy', 'rb') as f:\n",
    "        train_metrics = np.load(f, allow_pickle=True).item()\n",
    "    \n",
    "    losses = list(train_metrics['loss'])\n",
    "    metrics = {k: list(v) for k,v in train_metrics.items()}\n",
    "\n",
    "string_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb04ec-e6a7-4afd-afaa-c1e15df0ed5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epoch, num_epochs)):\n",
    "    wlog(f\"\\nEpoch {epoch}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(), \n",
    "        \"b2t\": b2t.state_dict(), \n",
    "        \"opt\": opt\n",
    "    }, checkpoint_dir / f\"chkp_{string_info}.pt\")\n",
    "    #}, checkpoint_dir / f\"chkp_{epoch}.pt\")\n",
    "    \n",
    "    if epoch>0 and epoch % 20 == 0:\n",
    "        print(\"Checkpoint saved\")\n",
    "        wlog(\"Checkpoint saved\")\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(), \n",
    "            \"b2t\": b2t.state_dict(), \n",
    "            \"opt\": opt\n",
    "        }, checkpoint_dir / f\"chkp_{string_info}_e{epoch}.pt\")\n",
    "\n",
    "    for step, (caption_batch, embedding_batch) in enumerate(tqdm(train_dataloader)):\n",
    "        # tokenize and prepare inputs for forward\n",
    "        input_ids, input_ids_target = tokenize(list(caption_batch))\n",
    "        inputs_embeds, input_ids_target = transform_input_ids(\n",
    "            embedding_batch, input_ids, input_ids_target\n",
    "        )\n",
    "\n",
    "        apply_grad_cond = step % gradient_acc_fact == 0\n",
    "        losses.append(\n",
    "            update_step(inputs_embeds, input_ids_target, apply_grad=apply_grad_cond)\n",
    "        )\n",
    "\n",
    "        if epoch % 5 == 0 and step % 500 == 0:\n",
    "            wlog(f\"Loss {np.mean(losses[-500:])}\\n\")\n",
    "            eval_step(string_info=string_info)\n",
    "                \n",
    "            plt.plot(losses, label='train_loss')\n",
    "            plt.savefig(f\"plots/plot_loss_{string_info}.png\")\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "            \n",
    "            if epoch % 10 == 0 and epoch>0:\n",
    "                \n",
    "                metrics_results = metrics_step(n=100)\n",
    "                metrics['step'].append(len(losses))\n",
    "                for m in ['meteor_train', 'meteor_valid', 'bleu_train', 'bleu_valid', 'spec_bleu', 'spec_meteor']:\n",
    "                    metrics[m].append(metrics_results[m])\n",
    "                    plt.plot(metrics['step'], metrics[m], label=m, linestyle=m_styles[m][1], color=m_styles[m][0])\n",
    "                plt.legend()\n",
    "                plt.savefig(f\"plots/plot_metrics_{string_info}.png\")\n",
    "                plt.show()\n",
    "\n",
    "                with open(f'outputs/train_metrics_{string_info}.npy', 'wb') as f:\n",
    "                    metrics_to_save = {k: np.array(a) for k, a in metrics.items()}\n",
    "                    metrics_to_save['loss'] = np.array(losses)\n",
    "                    np.save(f, metrics_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c89913-a442-40e1-b3a6-e6aa92219eb8",
   "metadata": {},
   "source": [
    "# Generate eval captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f98828e0-da74-4608-9528-493d1c462cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load('google_bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61ac29b8-10de-482d-910b-abc93d4d97d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450b4e4762724a1983e730d26b47ed80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_results_data = CaptionMusicDataset(muscaps_ds=test_ds, music_files=test_music_files,\n",
    "                                  preds_mode=True) \n",
    "eval_test_dataloader = DataLoader(test_results_data, 1, shuffle=False)\n",
    "\n",
    "eval_true_captions = []\n",
    "eval_pred_captions = []\n",
    "eval_ytids = []\n",
    "\n",
    "model_path = \"saved-models/best-enc-summarized.pt\"\n",
    "data_dict = torch.load(model_path)\n",
    "model.load_state_dict(data_dict['model'])\n",
    "b2t.load_state_dict(data_dict['b2t'])\n",
    "\n",
    "generation_params = dict(\n",
    "    max_new_tokens=200,\n",
    "    num_beams=8,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    bos_token_id=eos_id,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=mask_id,\n",
    "    early_stopping=True,\n",
    "    rm_eos=True,\n",
    ")\n",
    "\n",
    "for i, (caption_batch, embedding_batch, ytid_batch) in tqdm(enumerate(eval_test_dataloader)):\n",
    "    pred = eval(list(caption_batch[0]), embedding_batch, **generation_params)\n",
    "    eval_true_captions.append([c[0] for c in caption_batch])\n",
    "    eval_pred_captions.append(pred[0])\n",
    "    eval_ytids.append(ytid_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12835528-0218-480d-9400-3d84a7efcd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dict(\n",
    "    eval_true_captions=eval_true_captions,\n",
    "    eval_pred_captions=eval_pred_captions,\n",
    "    tracks_ids = eval_ytids,\n",
    "), open('outputs/preds_gpt2_enc_summarized.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80e9b85a-ec12-41da-b711-a6e1935e887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_eval_true_captions = [[re.sub(r'[^\\w\\s]','',x[0]).lower() for x in caption_list] for caption_list in eval_true_captions]\n",
    "lower_eval_pred_captions = [re.sub(r'[^\\w\\s]','',x).lower() for x in eval_pred_captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8e1136d-7fc5-4e1d-a58d-4f009e5cbfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002148431790944433 0.03212579655756952\n"
     ]
    }
   ],
   "source": [
    "gleu_score = google_bleu.compute(predictions=lower_eval_pred_captions, \n",
    "                                 references=lower_eval_true_captions)['google_bleu']\n",
    "meteor_score = meteor.compute(predictions=lower_eval_pred_captions, \n",
    "                              references=lower_eval_true_captions)['meteor']\n",
    "print(gleu_score, meteor_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339c047-d36b-4f5a-a42b-24c5bc398a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
