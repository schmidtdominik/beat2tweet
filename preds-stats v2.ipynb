{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f106bd-eec0-4ab2-a587-c07668c197ac",
   "metadata": {},
   "source": [
    "# Preds analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85d9c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (2.10.0)\n",
      "Requirement already satisfied: multiprocess in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (1.22.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: xxhash in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: packaging in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (0.12.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: responses<0.19 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: filelock in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: evaluate in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: xxhash in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: packaging in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (22.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (1.22.4)\n",
      "Requirement already satisfied: dill in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (2023.1.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (2.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (0.12.1)\n",
      "Requirement already satisfied: pandas in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (1.2.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: filelock in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa47ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: numpy in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from rouge_score) (1.22.4)\n",
      "Requirement already satisfied: nltk in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from nltk->rouge_score) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.1)\n",
      "Requirement already satisfied: tqdm in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.1)\n",
      "Requirement already satisfied: click in /Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7dec07c-f447-4c9a-868b-22e873c1b4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/corinacaraconcea/opt/anaconda3/envs/UCL1/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import evaluate\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm as tqdm\n",
    "from musiccaps import load_musiccaps\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7035f-1ca7-4913-b7b3-d4af47b89c0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2483bbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/corinacaraconcea/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/corinacaraconcea/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/corinacaraconcea/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load('google_bleu')\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8953ddb0-d512-46c9-b6b8-cd53be484247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset csv (/Users/corinacaraconcea/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ds = load_musiccaps(\n",
    "    \"./music_data\",\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True,\n",
    ")\n",
    "\n",
    "def clean_text_for_aspect_metrics(caption):\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    # split the sentences into words\n",
    "    desc = caption.split()\n",
    "    #converts to lower case\n",
    "    desc = [word.lower() for word in desc]\n",
    "    #remove punctuation from each token\n",
    "    desc = [word.translate(table) for word in desc]\n",
    "    #remove hanging 's and a \n",
    "    #desc = [word for word in desc if(len(word)>1)]\n",
    "    #remove tokens with numbers in them\n",
    "    #desc = [word for word in desc if(word.isalpha())]\n",
    "    #convert back to string\n",
    "    caption = ' '.join(desc)\n",
    "    return caption\n",
    "\n",
    "def preprocessing_remove_unk(text_input):\n",
    "\n",
    "    unk_flag = False\n",
    "\n",
    "    # remove punctuations\n",
    "    desc = re.sub(r'[^\\w\\s]',' ',text_input)\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "\n",
    "    # turn uppercase letters into lowercase ones\n",
    "    desc = text_input.lower()\n",
    "\n",
    "    # split into words\n",
    "    desc = desc.split(' ')\n",
    "\n",
    "    # remove the punctuations\n",
    "    text_no_punctuation = [word.translate(table) for word in desc]\n",
    "\n",
    "    if 'unk' in text_no_punctuation:\n",
    "        unk_flag = True\n",
    "\n",
    "    # join the caption words\n",
    "    caption = ' '.join(text_no_punctuation)\n",
    "    \n",
    "    return caption,unk_flag\n",
    "\n",
    "# get a list of music-related words to use for evaluation\n",
    "aspects = set()\n",
    "for x in ds:\n",
    "    aspect_str = x[\"aspect_list\"]\n",
    "    for t in \"[]\\\"'\":\n",
    "        aspect_str = aspect_str.replace(t, \"\")\n",
    "    aspects.update(aspect_str.split(\", \"))\n",
    "# clean aspects\n",
    "aspects = {clean_text_for_aspect_metrics(a) for a in aspects if len(a) > 2}\n",
    "    \n",
    "def wrap_in_space(s):\n",
    "    return ' ' + s + ' '\n",
    "    \n",
    "# filter\n",
    "all_captions = clean_text_for_aspect_metrics(' '.join(ds[i]['caption'] for i in range(len(ds))))\n",
    "aspect_counts = {a: all_captions.count(wrap_in_space(a)) for a in aspects}\n",
    "aspects = {a for a in aspects if aspect_counts[a] > 10}\n",
    "aspects -= {'the'}\n",
    "\n",
    "def compute_aspects_metric(true, pred):\n",
    "    true = wrap_in_space(clean_text_for_aspect_metrics(true))\n",
    "    pred = wrap_in_space(clean_text_for_aspect_metrics(pred))\n",
    "    \n",
    "    aspects_in_true = {a for a in aspects if wrap_in_space(a) in true}\n",
    "    aspects_in_pred = {a for a in aspects if wrap_in_space(a) in pred}\n",
    "    \n",
    "    precision = len(aspects_in_true&aspects_in_pred)/np.maximum(len(aspects_in_pred),1)\n",
    "    recall = len(aspects_in_true&aspects_in_pred)/np.maximum(len(aspects_in_true), 1)\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e2b1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_outputs(data_path):\n",
    "    \n",
    "    with open(data_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # multiple references for one captions\n",
    "    true_captions = data['eval_true_captions']\n",
    "    # single prediction\n",
    "    predicted_captions = data['eval_pred_captions']\n",
    "\n",
    "    return true_captions,predicted_captions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53916323",
   "metadata": {},
   "source": [
    "# Compute metrics for the non-summarized dataset (noaug + chataug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "301d7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_captions,predicted_captions):\n",
    "    true_clean = []\n",
    "    pred_clean = []\n",
    "    aspect_precision_list = []\n",
    "    aspect_recall_list = []\n",
    "    for i, (true, pred) in tqdm(enumerate(zip(true_captions, predicted_captions))):\n",
    "        # preprocess captions and predictions to remove punctuations and <unk> tokens\n",
    "        pred,pred_unk_flag = preprocessing_remove_unk(pred)\n",
    "        if pred_unk_flag == False:\n",
    "\n",
    "            pred_clean.append(pred)\n",
    "            true = preprocessing_remove_unk(true)[0]\n",
    "            true_clean.append(true)\n",
    "\n",
    "            # compute aspect metrics:\n",
    "            precision,recall = compute_aspects_metric(true, pred)\n",
    "            aspect_precision_list.append(precision)\n",
    "            aspect_recall_list.append(recall)\n",
    "\n",
    "    # print(len(pred_clean))\n",
    "    # print(len(true_clean))\n",
    "    total_google_bleu = google_bleu.compute(predictions = pred_clean,references = true_clean)\n",
    "    total_rouge = rouge.compute(predictions = pred_clean,references = true_clean)\n",
    "    total_meteor = meteor.compute(predictions = pred_clean,references = true_clean)\n",
    "\n",
    "    # compute spec metrics\n",
    "    n_shuffles = 20\n",
    "    shuffled_gleu_score, shuffled_meteor_score,shuffled_rouge_score = 0, 0, 0\n",
    "    for _ in tqdm(range(n_shuffles)):\n",
    "        true_shuffled = sorted(true_clean, key=lambda k: random.random())\n",
    "        shuffled_gleu_score += 1./n_shuffles * google_bleu.compute(predictions=pred_clean, references=true_shuffled)['google_bleu']\n",
    "        shuffled_meteor_score += 1./n_shuffles * meteor.compute(predictions=pred_clean, references=true_shuffled)['meteor']\n",
    "        shuffled_rouge_score += 1./n_shuffles * rouge.compute(predictions=pred_clean, references=true_shuffled)['rougeL']\n",
    "    spec_meteor = total_meteor[\"meteor\"]-shuffled_meteor_score\n",
    "    spec_gleu = total_google_bleu[\"google_bleu\"]-shuffled_gleu_score\n",
    "    spec_rouge = total_rouge[\"rougeL\"] - shuffled_rouge_score\n",
    "\n",
    "\n",
    "\n",
    "    return total_google_bleu, total_rouge, total_meteor,aspect_precision_list,aspect_recall_list,spec_meteor,spec_gleu,spec_rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4de9dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(data_paths,methods):\n",
    "    for i, (data_path, method) in enumerate(zip(data_paths,methods)):\n",
    "       true_captions,predicted_captions = import_outputs(data_path)\n",
    "       google_bleu_score, rouge_score, meteor_score,precision,recall,spec_meteor,spec_gleu,spec_rouge = compute_metrics(true_captions,predicted_captions)\n",
    "\n",
    "       precision = np.array(precision)\n",
    "       recall = np.array(recall)\n",
    "       \n",
    "       print(method,\"test Google BLEU score:\",str(np.round(google_bleu_score[\"google_bleu\"],3)))\n",
    "       print(method,\"test ROUGE score:\",str(np.round(rouge_score[\"rougeL\"],3)))\n",
    "       print(method,\"test METEOR score:\",str(np.round(meteor_score[\"meteor\"],3)))\n",
    "       print(method,\"test spec - Google BLEU score:\",str(np.round(spec_gleu,3)))\n",
    "       print(method,\"test spec - ROUGE score:\",str(np.round(spec_rouge,3)))\n",
    "       print(method,\"test spec - METEOR score:\",str(np.round(spec_meteor,3)))\n",
    "       print(method,\"test aspect precision score:\",str(np.round(np.mean(precision),3)),'+/-',str(np.round(np.std(precision),3)))\n",
    "       print(method,\"test aspect recall score:\",str(np.round(np.mean(recall),3)),'+/-',str(np.round(np.std(recall),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c5f209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [\"outputs/preds_gpt2_enc_noaug.json\",\n",
    "              \"outputs/preds_gpt2_enc_chataug.json\",\n",
    "              \"outputs/preds_lstm_noattn_noaug.json\",\n",
    "              \"outputs/preds_lstm_attn_noaug.json\"]\n",
    "\n",
    "\n",
    "methods = [\"GPT-2 fine-tuned encoder no aug\",\n",
    "           \"GPT-2 fine-tuned encoder with ChatAug\",\n",
    "           \"LSTM no attention no aug\",\n",
    "           \"LSTM attention no aug\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d96f53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "549it [00:00, 1090.29it/s]\n",
      "100%|██████████| 20/20 [00:40<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 fine-tuned encoder no aug test Google BLEU score: 0.091\n",
      "GPT-2 fine-tuned encoder no aug test ROUGE score: 0.212\n",
      "GPT-2 fine-tuned encoder no aug test METEOR score: 0.208\n",
      "GPT-2 fine-tuned encoder no aug test spec - Google BLEU score: 0.018\n",
      "GPT-2 fine-tuned encoder no aug test spec - ROUGE score: 0.026\n",
      "GPT-2 fine-tuned encoder no aug test spec - METEOR score: 0.034\n",
      "GPT-2 fine-tuned encoder no aug test aspect precision score: 0.157 +/- 0.165\n",
      "GPT-2 fine-tuned encoder no aug test aspect recall score: 0.185 +/- 0.175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "549it [00:00, 1073.44it/s]\n",
      "100%|██████████| 20/20 [00:40<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 fine-tuned encoder with ChatAug test Google BLEU score: 0.093\n",
      "GPT-2 fine-tuned encoder with ChatAug test ROUGE score: 0.221\n",
      "GPT-2 fine-tuned encoder with ChatAug test METEOR score: 0.213\n",
      "GPT-2 fine-tuned encoder with ChatAug test spec - Google BLEU score: 0.017\n",
      "GPT-2 fine-tuned encoder with ChatAug test spec - ROUGE score: 0.026\n",
      "GPT-2 fine-tuned encoder with ChatAug test spec - METEOR score: 0.033\n",
      "GPT-2 fine-tuned encoder with ChatAug test aspect precision score: 0.152 +/- 0.157\n",
      "GPT-2 fine-tuned encoder with ChatAug test aspect recall score: 0.176 +/- 0.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "549it [00:00, 1278.28it/s]\n",
      "100%|██████████| 20/20 [00:32<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM no attention no aug test Google BLEU score: 0.084\n",
      "LSTM no attention no aug test ROUGE score: 0.208\n",
      "LSTM no attention no aug test METEOR score: 0.185\n",
      "LSTM no attention no aug test spec - Google BLEU score: 0.006\n",
      "LSTM no attention no aug test spec - ROUGE score: 0.01\n",
      "LSTM no attention no aug test spec - METEOR score: 0.011\n",
      "LSTM no attention no aug test aspect precision score: 0.149 +/- 0.159\n",
      "LSTM no attention no aug test aspect recall score: 0.162 +/- 0.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "549it [00:00, 1327.74it/s]\n",
      "100%|██████████| 20/20 [00:30<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM attention no aug test Google BLEU score: 0.084\n",
      "LSTM attention no aug test ROUGE score: 0.211\n",
      "LSTM attention no aug test METEOR score: 0.186\n",
      "LSTM attention no aug test spec - Google BLEU score: 0.007\n",
      "LSTM attention no aug test spec - ROUGE score: 0.014\n",
      "LSTM attention no aug test spec - METEOR score: 0.015\n",
      "LSTM attention no aug test aspect precision score: 0.155 +/- 0.154\n",
      "LSTM attention no aug test aspect recall score: 0.167 +/- 0.148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_metrics(data_paths,methods)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62ab8dde",
   "metadata": {},
   "source": [
    "# Compute metrics for the summarized dataset (noaug + chataug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1e520fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_summaries_gpt2(true_captions,predicted_captions):\n",
    "    true_clean = []\n",
    "    pred_clean = []\n",
    "    aspect_precision_list = []\n",
    "    aspect_recall_list = []\n",
    "\n",
    "    for i, (true, pred) in tqdm(enumerate(zip(true_captions, predicted_captions))):\n",
    "        # preprocess captions and predictions to remove punctuations and <unk> tokens\n",
    "        pred,pred_unk_flag = preprocessing_remove_unk(pred)\n",
    "        if pred_unk_flag == False:\n",
    "            pred_clean.append(pred)\n",
    "            if isinstance(true, list):\n",
    "                true = [preprocessing_remove_unk(caption)[0] for caption in true]\n",
    "                concatenated_true = true[0] + true[1] + true[2]\n",
    "            else:\n",
    "                true = preprocessing_remove_unk(true)[0]\n",
    "            true_clean.append(true)\n",
    "\n",
    "            # compute aspect metrics:\n",
    "            precision,recall = compute_aspects_metric(concatenated_true, pred)\n",
    "            aspect_precision_list.append(precision)\n",
    "            aspect_recall_list.append(recall)\n",
    "\n",
    "    print(len(pred_clean))\n",
    "    print(len(true_clean))\n",
    "    total_google_bleu = google_bleu.compute(predictions = pred_clean,references = true_clean)\n",
    "    total_rouge = rouge.compute(predictions = pred_clean,references = true_clean)\n",
    "    total_meteor = meteor.compute(predictions = pred_clean,references = true_clean)\n",
    "\n",
    "    # compute spec metrics\n",
    "    n_shuffles = 20\n",
    "    shuffled_gleu_score, shuffled_meteor_score,shuffled_rouge_score = 0, 0, 0\n",
    "    for _ in tqdm(range(n_shuffles)):\n",
    "        true_shuffled = sorted(true_clean, key=lambda k: random.random())\n",
    "        shuffled_gleu_score += 1./n_shuffles * google_bleu.compute(predictions=pred_clean, references=true_shuffled)['google_bleu']\n",
    "        shuffled_meteor_score += 1./n_shuffles * meteor.compute(predictions=pred_clean, references=true_shuffled)['meteor']\n",
    "        shuffled_rouge_score += 1./n_shuffles * rouge.compute(predictions=pred_clean, references=true_shuffled)['rougeL']\n",
    "    spec_meteor = total_meteor[\"meteor\"]-shuffled_meteor_score\n",
    "    spec_gleu = total_google_bleu[\"google_bleu\"]-shuffled_gleu_score\n",
    "    spec_rouge = total_rouge[\"rougeL\"] - shuffled_rouge_score\n",
    "    \n",
    "    return total_google_bleu, total_rouge, total_meteor,aspect_precision_list, aspect_recall_list,spec_meteor,spec_gleu,spec_rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6fa65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths_summarized_gpt2= [\"outputs/preds_gpt2_enc_summarized.json\"]\n",
    "\n",
    "methods_summarized_gpt2 = [\"GPT-2 fine-tuned encoder and summarized dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a6a5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics2(data_paths,methods):\n",
    "    for i, (data_path, method) in enumerate(zip(data_paths,methods)):\n",
    "       true_captions,predicted_captions = import_outputs(data_path)\n",
    "       google_bleu_score, rouge_score, meteor_score, precision, recall,spec_meteor,spec_gleu,spec_rouge= compute_metrics_summaries_gpt2(true_captions,predicted_captions)\n",
    "\n",
    "       print(method,\"test GLEU score:\",str(np.round(google_bleu_score[\"google_bleu\"],3)))\n",
    "       print(method,\"test google ROUGE score:\",str(np.round(rouge_score[\"rougeL\"],3)))\n",
    "       print(method,\"test google METEOR score:\",str(np.round(meteor_score[\"meteor\"],3)))\n",
    "       print(method,\"test spec - Google BLEU score:\",str(np.round(spec_gleu,3)))\n",
    "       print(method,\"test spec - ROUGE score:\",str(np.round(spec_rouge,3)))\n",
    "       print(method,\"test spec - METEOR score:\",str(np.round(spec_meteor,3)))\n",
    "       print(method,\"test aspect precision score:\",str(np.round(np.mean(precision),3)),'+/-',str(np.round(np.std(precision),3)))\n",
    "       print(method,\"test aspect recall score:\",str(np.round(np.mean(recall),3)),'+/-',str(np.round(np.std(recall),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40c8cacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "549it [00:00, 841.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n",
      "549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:04<00:00,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 fine-tuned encoder and summarized dataset test GLEU score: 0.104\n",
      "GPT-2 fine-tuned encoder and summarized dataset test google ROUGE score: 0.25\n",
      "GPT-2 fine-tuned encoder and summarized dataset test google METEOR score: 0.248\n",
      "GPT-2 fine-tuned encoder and summarized dataset test spec - Google BLEU score: 0.021\n",
      "GPT-2 fine-tuned encoder and summarized dataset test spec - ROUGE score: 0.032\n",
      "GPT-2 fine-tuned encoder and summarized dataset test spec - METEOR score: 0.046\n",
      "GPT-2 fine-tuned encoder and summarized dataset test aspect precision score: 0.192 +/- 0.169\n",
      "GPT-2 fine-tuned encoder and summarized dataset test aspect recall score: 0.146 +/- 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_metrics2(data_paths_summarized_gpt2,methods_summarized_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "382aff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_summaries_lstm(true_captions,predicted_captions):\n",
    "    true_clean = []\n",
    "    pred_clean = []\n",
    "    aspect_recall_list = []\n",
    "    aspect_precision_list = []\n",
    "\n",
    "    for i, (true, pred) in tqdm(enumerate(zip(true_captions, predicted_captions))):\n",
    "        # the model outputs 3 identical predictions so we need to only append one\n",
    "        pred,pred_unk_flag = preprocessing_remove_unk(pred)\n",
    "        if pred_unk_flag == False:\n",
    "            # preprocess captions and predictions to remove punctuations and <unk> tokens\n",
    "            true = preprocessing_remove_unk(true)[0]\n",
    "            true_clean.append(true)\n",
    "            if i%3==0 :\n",
    "                pred_clean.append(pred)\n",
    "\n",
    "    nested_summarized_true_captions = []\n",
    "    nested_summarized_true_captions = [[true_clean[i], true_clean[i+1], true_clean[i+2]] for i in range(0, len(true_clean), 3)]\n",
    "\n",
    "    # print(pred_clean)\n",
    "    # compute aspect metrics:\n",
    "    for  i, (true, pred) in tqdm(enumerate(zip(nested_summarized_true_captions, pred_clean))):\n",
    "        concat_true = true[0] + true[1] +true[2]\n",
    "        precision,recall = compute_aspects_metric(concat_true, pred)\n",
    "        aspect_precision_list.append(precision)\n",
    "        aspect_recall_list.append(recall)\n",
    "\n",
    "\n",
    "    print(len(pred_clean))\n",
    "    print(len(nested_summarized_true_captions))\n",
    "    total_google_bleu = google_bleu.compute(predictions = pred_clean,references = nested_summarized_true_captions)\n",
    "    total_rouge = rouge.compute(predictions = pred_clean,references = nested_summarized_true_captions)\n",
    "    total_meteor = meteor.compute(predictions = pred_clean,references = nested_summarized_true_captions)\n",
    "\n",
    "    # print(pred_clean[2])\n",
    "    # print(nested_summarized_true_captions[2])\n",
    "\n",
    "    # compute spec metrics\n",
    "    n_shuffles = 20\n",
    "    shuffled_gleu_score, shuffled_meteor_score,shuffled_rouge_score = 0, 0, 0\n",
    "    for _ in tqdm(range(n_shuffles)):\n",
    "        true_shuffled = sorted(nested_summarized_true_captions, key=lambda k: random.random())\n",
    "        shuffled_gleu_score += 1./n_shuffles * google_bleu.compute(predictions=pred_clean, references=true_shuffled)['google_bleu']\n",
    "        shuffled_meteor_score += 1./n_shuffles * meteor.compute(predictions=pred_clean, references=true_shuffled)['meteor']\n",
    "        shuffled_rouge_score += 1./n_shuffles * rouge.compute(predictions=pred_clean, references=true_shuffled)['rougeL']\n",
    "    spec_meteor = total_meteor[\"meteor\"]-shuffled_meteor_score\n",
    "    spec_gleu = total_google_bleu[\"google_bleu\"]-shuffled_gleu_score\n",
    "    spec_rouge = total_rouge[\"rougeL\"] - shuffled_rouge_score\n",
    "\n",
    "    \n",
    "    return total_google_bleu, total_rouge, total_meteor,aspect_precision_list,aspect_recall_list, spec_meteor, spec_gleu, spec_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8030d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths_summarized_lstm= [\"outputs/preds_lstm_noattn_summaries.json\",\n",
    "                        \"outputs/preds_lstm_attn_summaries.json\"]\n",
    "\n",
    "methods_summarized_lstm = [\"LSTM no attention and summarized dataset\",\n",
    "                           \"LSTM with attention and summarized dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2f72937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics3(data_paths,methods):\n",
    "    for i, (data_path, method) in enumerate(zip(data_paths,methods)):\n",
    "       true_captions,predicted_captions = import_outputs(data_path)\n",
    "       google_bleu_score, rouge_score, meteor_score, precision,recall, spec_meteor, spec_gleu, spec_rouge = compute_metrics_summaries_lstm(true_captions,predicted_captions)\n",
    "\n",
    "       precision = np.array(precision)\n",
    "       recall = np.array(recall)\n",
    "       \n",
    "\n",
    "       print(method,\"test GLEU score:\",str(np.round(google_bleu_score[\"google_bleu\"],3)))\n",
    "       print(method,\"test google ROUGE score:\",str(np.round(rouge_score[\"rougeL\"],3)))\n",
    "       print(method,\"test google METEOR score:\",str(np.round(meteor_score[\"meteor\"],3)))\n",
    "       print(method,\"test spec - Google BLEU score:\",str(np.round(spec_gleu,3)))\n",
    "       print(method,\"test spec - ROUGE score:\",str(np.round(spec_rouge,3)))\n",
    "       print(method,\"test spec - METEOR score:\",str(np.round(spec_meteor,3)))\n",
    "       print(method,\"test aspect precision score:\",str(np.round(np.mean(precision),3)),'+/-',str(np.round(np.std(precision),3)))\n",
    "       print(method,\"test aspect recall score:\",str(np.round(np.mean(recall),3)),'+/-',str(np.round(np.std(recall),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d8c5187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1647it [00:00, 34162.93it/s]\n",
      "548it [00:00, 912.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548\n",
      "548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:58<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM no attention and summarized dataset test GLEU score: 0.111\n",
      "LSTM no attention and summarized dataset test google ROUGE score: 0.27\n",
      "LSTM no attention and summarized dataset test google METEOR score: 0.247\n",
      "LSTM no attention and summarized dataset test spec - Google BLEU score: 0.019\n",
      "LSTM no attention and summarized dataset test spec - ROUGE score: 0.031\n",
      "LSTM no attention and summarized dataset test spec - METEOR score: 0.038\n",
      "LSTM no attention and summarized dataset test aspect precision score: 0.199 +/- 0.172\n",
      "LSTM no attention and summarized dataset test aspect recall score: 0.139 +/- 0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1647it [00:00, 33227.76it/s]\n",
      "543it [00:00, 912.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n",
      "543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:00<00:00,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM with attention and summarized dataset test GLEU score: 0.107\n",
      "LSTM with attention and summarized dataset test google ROUGE score: 0.265\n",
      "LSTM with attention and summarized dataset test google METEOR score: 0.243\n",
      "LSTM with attention and summarized dataset test spec - Google BLEU score: 0.015\n",
      "LSTM with attention and summarized dataset test spec - ROUGE score: 0.025\n",
      "LSTM with attention and summarized dataset test spec - METEOR score: 0.032\n",
      "LSTM with attention and summarized dataset test aspect precision score: 0.198 +/- 0.179\n",
      "LSTM with attention and summarized dataset test aspect recall score: 0.132 +/- 0.119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_metrics3(data_paths_summarized_lstm,methods_summarized_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854792d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b87bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UCL1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 10:02:19) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9151f2c10dd9c04d670c6ab9407273fd33ff73d6858514519ebe1bfa000ee63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
