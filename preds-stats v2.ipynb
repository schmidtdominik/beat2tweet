{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f106bd-eec0-4ab2-a587-c07668c197ac",
   "metadata": {},
   "source": [
    "# Preds analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa47ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dec07c-f447-4c9a-868b-22e873c1b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import evaluate\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm as tqdm\n",
    "from musiccaps import load_musiccaps\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7035f-1ca7-4913-b7b3-d4af47b89c0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load('google_bleu')\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8953ddb0-d512-46c9-b6b8-cd53be484247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds = load_musiccaps(\n",
    "    \"./music_data\",\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True,\n",
    ")\n",
    "\n",
    "def clean_text_for_aspect_metrics(caption):\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    # split the sentences into words\n",
    "    desc = caption.split()\n",
    "    #converts to lower case\n",
    "    desc = [word.lower() for word in desc]\n",
    "    #remove punctuation from each token\n",
    "    desc = [word.translate(table) for word in desc]\n",
    "    #remove hanging 's and a \n",
    "    #desc = [word for word in desc if(len(word)>1)]\n",
    "    #remove tokens with numbers in them\n",
    "    #desc = [word for word in desc if(word.isalpha())]\n",
    "    #convert back to string\n",
    "    caption = ' '.join(desc)\n",
    "    return caption\n",
    "\n",
    "def preprocessing_remove_unk(text_input):\n",
    "\n",
    "    unk_flag = False\n",
    "\n",
    "    # remove punctuations\n",
    "    desc = re.sub(r'[^\\w\\s]',' ',text_input)\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "\n",
    "    # turn uppercase letters into lowercase ones\n",
    "    desc = text_input.lower()\n",
    "\n",
    "    # split into words\n",
    "    desc = desc.split(' ')\n",
    "\n",
    "    # remove the punctuations\n",
    "    text_no_punctuation = [word.translate(table) for word in desc]\n",
    "\n",
    "    if 'unk' in text_no_punctuation:\n",
    "        unk_flag = True\n",
    "\n",
    "    # join the caption words\n",
    "    caption = ' '.join(text_no_punctuation)\n",
    "    \n",
    "    return caption,unk_flag\n",
    "\n",
    "# get a list of music-related words to use for evaluation\n",
    "aspects = set()\n",
    "for x in ds:\n",
    "    aspect_str = x[\"aspect_list\"]\n",
    "    for t in \"[]\\\"'\":\n",
    "        aspect_str = aspect_str.replace(t, \"\")\n",
    "    aspects.update(aspect_str.split(\", \"))\n",
    "# clean aspects\n",
    "aspects = {clean_text_for_aspect_metrics(a) for a in aspects if len(a) > 2}\n",
    "    \n",
    "def wrap_in_space(s):\n",
    "    return ' ' + s + ' '\n",
    "    \n",
    "# filter\n",
    "all_captions = clean_text_for_aspect_metrics(' '.join(ds[i]['caption'] for i in range(len(ds))))\n",
    "aspect_counts = {a: all_captions.count(wrap_in_space(a)) for a in aspects}\n",
    "aspects = {a for a in aspects if aspect_counts[a] > 10}\n",
    "aspects -= {'the'}\n",
    "\n",
    "def compute_aspects_metric(true, pred):\n",
    "    true = wrap_in_space(clean_text_for_aspect_metrics(true))\n",
    "    pred = wrap_in_space(clean_text_for_aspect_metrics(pred))\n",
    "    \n",
    "    aspects_in_true = {a for a in aspects if wrap_in_space(a) in true}\n",
    "    aspects_in_pred = {a for a in aspects if wrap_in_space(a) in pred}\n",
    "    \n",
    "    precision = len(aspects_in_true&aspects_in_pred)/np.maximum(len(aspects_in_pred),1)\n",
    "    recall = len(aspects_in_true&aspects_in_pred)/np.maximum(len(aspects_in_true), 1)\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_outputs(data_path):\n",
    "    \n",
    "    with open(data_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # multiple references for one captions\n",
    "    true_captions = data['eval_true_captions']\n",
    "    # single prediction\n",
    "    predicted_captions = data['eval_pred_captions']\n",
    "\n",
    "    return true_captions,predicted_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53916323",
   "metadata": {},
   "source": [
    "# Compute metrics for the non-summarized dataset (noaug + chataug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_captions,predicted_captions):\n",
    "    true_clean = []\n",
    "    pred_clean = []\n",
    "    aspect_precision_list = []\n",
    "    aspect_recall_list = []\n",
    "    for i, (true, pred) in tqdm(enumerate(zip(true_captions, predicted_captions))):\n",
    "        # preprocess captions and predictions to remove punctuations and <unk> tokens\n",
    "        pred,pred_unk_flag = preprocessing_remove_unk(pred)\n",
    "        if pred_unk_flag == False:\n",
    "\n",
    "            pred_clean.append(pred)\n",
    "            true = preprocessing_remove_unk(true)[0]\n",
    "            true_clean.append(true)\n",
    "\n",
    "            # compute aspect metrics:\n",
    "            precision,recall = compute_aspects_metric(true, pred)\n",
    "            aspect_precision_list.append(precision)\n",
    "            aspect_recall_list.append(recall)\n",
    "\n",
    "    # print(len(pred_clean))\n",
    "    # print(len(true_clean))\n",
    "    total_google_bleu = google_bleu.compute(predictions = pred_clean,references = true_clean)\n",
    "    total_rouge = rouge.compute(predictions = pred_clean,references = true_clean)\n",
    "    total_meteor = meteor.compute(predictions = pred_clean,references = true_clean)\n",
    "\n",
    "    # compute spec metrics\n",
    "    n_shuffles = 20\n",
    "    shuffled_gleu_score, shuffled_meteor_score,shuffled_rouge_score = 0, 0, 0\n",
    "    for _ in tqdm(range(n_shuffles)):\n",
    "        true_shuffled = sorted(true_clean, key=lambda k: random.random())\n",
    "        shuffled_gleu_score += 1./n_shuffles * google_bleu.compute(predictions=pred_clean, references=true_shuffled)['google_bleu']\n",
    "        shuffled_meteor_score += 1./n_shuffles * meteor.compute(predictions=pred_clean, references=true_shuffled)['meteor']\n",
    "        shuffled_rouge_score += 1./n_shuffles * rouge.compute(predictions=pred_clean, references=true_shuffled)['rougeL']\n",
    "    spec_meteor = total_meteor[\"meteor\"]-shuffled_meteor_score\n",
    "    spec_gleu = total_google_bleu[\"google_bleu\"]-shuffled_gleu_score\n",
    "    spec_rouge = total_rouge[\"rougeL\"] - shuffled_rouge_score\n",
    "\n",
    "\n",
    "\n",
    "    return total_google_bleu, total_rouge, total_meteor,aspect_precision_list,aspect_recall_list,spec_meteor,spec_gleu,spec_rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(data_paths,methods):\n",
    "    for i, (data_path, method) in enumerate(zip(data_paths,methods)):\n",
    "       true_captions,predicted_captions = import_outputs(data_path)\n",
    "       google_bleu_score, rouge_score, meteor_score,precision,recall,spec_meteor,spec_gleu,spec_rouge = compute_metrics(true_captions,predicted_captions)\n",
    "\n",
    "       precision = np.array(precision)\n",
    "       recall = np.array(recall)\n",
    "       \n",
    "       print(method,\"test Google BLEU score:\",str(np.round(google_bleu_score[\"google_bleu\"],3)))\n",
    "       print(method,\"test ROUGE score:\",str(np.round(rouge_score[\"rougeL\"],3)))\n",
    "       print(method,\"test METEOR score:\",str(np.round(meteor_score[\"meteor\"],3)))\n",
    "       print(method,\"test spec - Google BLEU score:\",str(np.round(spec_gleu,3)))\n",
    "       print(method,\"test spec - ROUGE score:\",str(np.round(spec_rouge,3)))\n",
    "       print(method,\"test spec - METEOR score:\",str(np.round(spec_meteor,3)))\n",
    "       print(method,\"test aspect precision score:\",str(np.round(np.mean(precision),3)),'+/-',str(np.round(np.std(precision),3)))\n",
    "       print(method,\"test aspect recall score:\",str(np.round(np.mean(recall),3)),'+/-',str(np.round(np.std(recall),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [\"outputs/preds_gpt2_enc_noaug.json\",\n",
    "              \"outputs/preds_gpt2_enc_chataug.json\",\n",
    "              \"outputs/preds_lstm_noattn_noaug.json\",\n",
    "              \"outputs/preds_lstm_attn_noaug.json\"]\n",
    "\n",
    "\n",
    "methods = [\"GPT-2 fine-tuned encoder no aug\",\n",
    "           \"GPT-2 fine-tuned encoder with ChatAug\",\n",
    "           \"LSTM no attention no aug\",\n",
    "           \"LSTM attention no aug\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(data_paths,methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab8dde",
   "metadata": {},
   "source": [
    "# Compute metrics for the summarized dataset (noaug + chataug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e520fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_summaries_gpt2(true_captions,predicted_captions):\n",
    "    true_clean = []\n",
    "    pred_clean = []\n",
    "    aspect_precision_list = []\n",
    "    aspect_recall_list = []\n",
    "\n",
    "    for i, (true, pred) in tqdm(enumerate(zip(true_captions, predicted_captions))):\n",
    "        # preprocess captions and predictions to remove punctuations and <unk> tokens\n",
    "        pred,pred_unk_flag = preprocessing_remove_unk(pred)\n",
    "        if pred_unk_flag == False:\n",
    "            pred_clean.append(pred)\n",
    "            if isinstance(true, list):\n",
    "                true = [preprocessing_remove_unk(caption)[0] for caption in true]\n",
    "                concatenated_true = true[0] + true[1] + true[2]\n",
    "            else:\n",
    "                true = preprocessing_remove_unk(true)[0]\n",
    "            true_clean.append(true)\n",
    "\n",
    "            # compute aspect metrics:\n",
    "            precision,recall = compute_aspects_metric(concatenated_true, pred)\n",
    "            aspect_precision_list.append(precision)\n",
    "            aspect_recall_list.append(recall)\n",
    "\n",
    "    print(len(pred_clean))\n",
    "    print(len(true_clean))\n",
    "    total_google_bleu = google_bleu.compute(predictions = pred_clean,references = true_clean)\n",
    "    total_rouge = rouge.compute(predictions = pred_clean,references = true_clean)\n",
    "    total_meteor = meteor.compute(predictions = pred_clean,references = true_clean)\n",
    "\n",
    "    # compute spec metrics\n",
    "    n_shuffles = 20\n",
    "    shuffled_gleu_score, shuffled_meteor_score,shuffled_rouge_score = 0, 0, 0\n",
    "    for _ in tqdm(range(n_shuffles)):\n",
    "        true_shuffled = sorted(true_clean, key=lambda k: random.random())\n",
    "        shuffled_gleu_score += 1./n_shuffles * google_bleu.compute(predictions=pred_clean, references=true_shuffled)['google_bleu']\n",
    "        shuffled_meteor_score += 1./n_shuffles * meteor.compute(predictions=pred_clean, references=true_shuffled)['meteor']\n",
    "        shuffled_rouge_score += 1./n_shuffles * rouge.compute(predictions=pred_clean, references=true_shuffled)['rougeL']\n",
    "    spec_meteor = total_meteor[\"meteor\"]-shuffled_meteor_score\n",
    "    spec_gleu = total_google_bleu[\"google_bleu\"]-shuffled_gleu_score\n",
    "    spec_rouge = total_rouge[\"rougeL\"] - shuffled_rouge_score\n",
    "    \n",
    "    return total_google_bleu, total_rouge, total_meteor,aspect_precision_list, aspect_recall_list,spec_meteor,spec_gleu,spec_rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths_summarized_gpt2= [\"outputs/preds_gpt2_enc_summarized.json\"]\n",
    "\n",
    "methods_summarized_gpt2 = [\"GPT-2 fine-tuned encoder and summarized dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics2(data_paths,methods):\n",
    "    for i, (data_path, method) in enumerate(zip(data_paths,methods)):\n",
    "       true_captions,predicted_captions = import_outputs(data_path)\n",
    "       google_bleu_score, rouge_score, meteor_score, precision, recall,spec_meteor,spec_gleu,spec_rouge= compute_metrics_summaries_gpt2(true_captions,predicted_captions)\n",
    "\n",
    "       print(method,\"test GLEU score:\",str(np.round(google_bleu_score[\"google_bleu\"],3)))\n",
    "       print(method,\"test google ROUGE score:\",str(np.round(rouge_score[\"rougeL\"],3)))\n",
    "       print(method,\"test google METEOR score:\",str(np.round(meteor_score[\"meteor\"],3)))\n",
    "       print(method,\"test spec - Google BLEU score:\",str(np.round(spec_gleu,3)))\n",
    "       print(method,\"test spec - ROUGE score:\",str(np.round(spec_rouge,3)))\n",
    "       print(method,\"test spec - METEOR score:\",str(np.round(spec_meteor,3)))\n",
    "       print(method,\"test aspect precision score:\",str(np.round(np.mean(precision),3)),'+/-',str(np.round(np.std(precision),3)))\n",
    "       print(method,\"test aspect recall score:\",str(np.round(np.mean(recall),3)),'+/-',str(np.round(np.std(recall),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics2(data_paths_summarized_gpt2,methods_summarized_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382aff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_summaries_lstm(true_captions,predicted_captions):\n",
    "    true_clean = []\n",
    "    pred_clean = []\n",
    "    aspect_recall_list = []\n",
    "    aspect_precision_list = []\n",
    "\n",
    "    for i, (true, pred) in tqdm(enumerate(zip(true_captions, predicted_captions))):\n",
    "        # the model outputs 3 identical predictions so we need to only append one\n",
    "        pred,pred_unk_flag = preprocessing_remove_unk(pred)\n",
    "        if pred_unk_flag == False:\n",
    "            # preprocess captions and predictions to remove punctuations and <unk> tokens\n",
    "            true = preprocessing_remove_unk(true)[0]\n",
    "            true_clean.append(true)\n",
    "            if i%3==0 :\n",
    "                pred_clean.append(pred)\n",
    "\n",
    "    nested_summarized_true_captions = []\n",
    "    nested_summarized_true_captions = [[true_clean[i], true_clean[i+1], true_clean[i+2]] for i in range(0, len(true_clean), 3)]\n",
    "\n",
    "    # print(pred_clean)\n",
    "    # compute aspect metrics:\n",
    "    for  i, (true, pred) in tqdm(enumerate(zip(nested_summarized_true_captions, pred_clean))):\n",
    "        concat_true = true[0] + true[1] +true[2]\n",
    "        precision,recall = compute_aspects_metric(concat_true, pred)\n",
    "        aspect_precision_list.append(precision)\n",
    "        aspect_recall_list.append(recall)\n",
    "\n",
    "\n",
    "    print(len(pred_clean))\n",
    "    print(len(nested_summarized_true_captions))\n",
    "    total_google_bleu = google_bleu.compute(predictions = pred_clean,references = nested_summarized_true_captions)\n",
    "    total_rouge = rouge.compute(predictions = pred_clean,references = nested_summarized_true_captions)\n",
    "    total_meteor = meteor.compute(predictions = pred_clean,references = nested_summarized_true_captions)\n",
    "\n",
    "    # print(pred_clean[2])\n",
    "    # print(nested_summarized_true_captions[2])\n",
    "\n",
    "    # compute spec metrics\n",
    "    n_shuffles = 20\n",
    "    shuffled_gleu_score, shuffled_meteor_score,shuffled_rouge_score = 0, 0, 0\n",
    "    for _ in tqdm(range(n_shuffles)):\n",
    "        true_shuffled = sorted(nested_summarized_true_captions, key=lambda k: random.random())\n",
    "        shuffled_gleu_score += 1./n_shuffles * google_bleu.compute(predictions=pred_clean, references=true_shuffled)['google_bleu']\n",
    "        shuffled_meteor_score += 1./n_shuffles * meteor.compute(predictions=pred_clean, references=true_shuffled)['meteor']\n",
    "        shuffled_rouge_score += 1./n_shuffles * rouge.compute(predictions=pred_clean, references=true_shuffled)['rougeL']\n",
    "    spec_meteor = total_meteor[\"meteor\"]-shuffled_meteor_score\n",
    "    spec_gleu = total_google_bleu[\"google_bleu\"]-shuffled_gleu_score\n",
    "    spec_rouge = total_rouge[\"rougeL\"] - shuffled_rouge_score\n",
    "\n",
    "    \n",
    "    return total_google_bleu, total_rouge, total_meteor,aspect_precision_list,aspect_recall_list, spec_meteor, spec_gleu, spec_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8030d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths_summarized_lstm= [\"outputs/preds_lstm_noattn_summaries.json\",\n",
    "                        \"outputs/preds_lstm_attn_summaries.json\"]\n",
    "\n",
    "methods_summarized_lstm = [\"LSTM no attention and summarized dataset\",\n",
    "                           \"LSTM with attention and summarized dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f72937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics3(data_paths,methods):\n",
    "    for i, (data_path, method) in enumerate(zip(data_paths,methods)):\n",
    "       true_captions,predicted_captions = import_outputs(data_path)\n",
    "       google_bleu_score, rouge_score, meteor_score, precision,recall, spec_meteor, spec_gleu, spec_rouge = compute_metrics_summaries_lstm(true_captions,predicted_captions)\n",
    "\n",
    "       precision = np.array(precision)\n",
    "       recall = np.array(recall)\n",
    "       \n",
    "\n",
    "       print(method,\"test GLEU score:\",str(np.round(google_bleu_score[\"google_bleu\"],3)))\n",
    "       print(method,\"test google ROUGE score:\",str(np.round(rouge_score[\"rougeL\"],3)))\n",
    "       print(method,\"test google METEOR score:\",str(np.round(meteor_score[\"meteor\"],3)))\n",
    "       print(method,\"test spec - Google BLEU score:\",str(np.round(spec_gleu,3)))\n",
    "       print(method,\"test spec - ROUGE score:\",str(np.round(spec_rouge,3)))\n",
    "       print(method,\"test spec - METEOR score:\",str(np.round(spec_meteor,3)))\n",
    "       print(method,\"test aspect precision score:\",str(np.round(np.mean(precision),3)),'+/-',str(np.round(np.std(precision),3)))\n",
    "       print(method,\"test aspect recall score:\",str(np.round(np.mean(recall),3)),'+/-',str(np.round(np.std(recall),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics3(data_paths_summarized_lstm,methods_summarized_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854792d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b87bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9151f2c10dd9c04d670c6ab9407273fd33ff73d6858514519ebe1bfa000ee63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
