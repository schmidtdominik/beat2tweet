{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd60a10-aba2-4f3e-b251-77d4f100179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U yt-dlp==2023.1.6 matplotlib==3.6.0 datasets[audio] rich\n",
    "\n",
    "# install newest transformers build to be able to pass `inputs_embeds` through generate()\n",
    "# !pip install --upgrade git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426135f-00ed-4215-9b20-ee38b2c0448a",
   "metadata": {},
   "source": [
    "**Relevant huggingface gpt2 code**\n",
    "\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py\n",
    "- https://github.com/huggingface/transformers/issues/6535\n",
    "- bos/eos discussion: https://github.com/huggingface/transformers/issues/3311"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0641d-28fc-4adf-9ec8-0af23d008b25",
   "metadata": {},
   "source": [
    "**Todo**\n",
    "\n",
    "- Rerun embedding generation for musiccaps/make sure it's the same as jamendo and none of the changes in mtg_jam.. broke it (forward hook should use input or output correctly)\n",
    "- embedding concat vs sum vs mean\n",
    "- metrics (with different generation methods/args)\n",
    "- tokenization might still be a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485da5-3e40-42f1-873e-ccb244a74bc3",
   "metadata": {},
   "source": [
    "# Load musiccaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bb0885-c2da-43e2-8df4-468ee5f97594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiccaps import load_musiccaps\n",
    "import numpy as np\n",
    "from rich import print as printr\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import math\n",
    "from rich import print as printr\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6684ae9-0e53-415d-8be8-a67f90f7030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_muscaps_with_embeddings(ds, embeddings):\n",
    "    '''Some clips weren't downloaded so we couldn't embed them, get rid of that'''\n",
    "    exclude_ids = set()\n",
    "    for i in range(len(ds)):\n",
    "        if ds[i]['ytid'] not in embeddings.keys():\n",
    "            exclude_ids.add(i)\n",
    "    ds = ds.select(\n",
    "        (\n",
    "            i for i in range(len(ds)) \n",
    "            if i not in set(exclude_ids)\n",
    "        )\n",
    "    )\n",
    "    assert len(ds) == len(embeddings)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8926e06-ef8c-40a7-8675-f4d1285d0afe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration google--MusicCaps-7925612b943f961b\n",
      "Found cached dataset csv (/home/dominik/.cache/huggingface/datasets/google___csv/google--MusicCaps-7925612b943f961b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    }
   ],
   "source": [
    "ds = load_musiccaps(\n",
    "    './music_data',\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True\n",
    ")\n",
    "embeddings = np.load('embeddings.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15420704-a6bd-4315-b6b9-ba41fee54a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionEmbedding(Dataset):\n",
    "    '''Returns a torch Dataset of paired captions and embeddings'''\n",
    "    def __init__(self, muscaps_ds, embeddings):\n",
    "        ds = filter_muscaps_with_embeddings(muscaps_ds, embeddings)\n",
    "        self.captions = ds.sort(column='ytid')['caption']\n",
    "        sorted_embs = [value for _, value in sorted(embeddings.items())]\n",
    "        self.embeddings = torch.from_numpy(np.stack(sorted_embs)).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        emb = self.embeddings[idx]\n",
    "        assert len(emb) == 512\n",
    "        emb = (emb[:256]+emb[256:])/2\n",
    "        \n",
    "        return self.captions[idx], emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51464487-9c40-4dc5-8436-372b612e4d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object filter_muscaps_with_embeddings.<locals>.<genexpr> at 0x7f97e75793c0> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "dataset = CaptionEmbedding(muscaps_ds=ds, embeddings=embeddings)\n",
    "train_frac = 0.8\n",
    "training_data, test_data = random_split(dataset, [train_frac, 1-train_frac])\n",
    "\n",
    "# quick check did not mess up ordering of caption-embedding pairs\n",
    "# for cap, emb in tqdm(dataset):\n",
    "#     for i in range(len(ds)):\n",
    "#         if cap == ds[i]['caption']:\n",
    "#             assert torch.allclose(emb,torch.from_numpy(embeddings[ds[i]['ytid']]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce82080-8e03-45ae-8266-6e0f2af6843c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of music-related words to use for evaluation\n",
    "aspects = []\n",
    "for x in ds:\n",
    "    aspect_str = x['aspect_list']\n",
    "    for t in ('[]\"\\''):\n",
    "        aspect_str = aspect_str.replace(t, '')\n",
    "    aspects.extend(aspect_str.split(', '))\n",
    "    \n",
    "from collections import Counter\n",
    "# only pick aspects that show up somewhat frequently\n",
    "aspects = {s for s, count in Counter(aspects).most_common() if count >= 25}\n",
    "len(aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f7426-5bb1-46b7-8c09-28e264b73ca4",
   "metadata": {},
   "source": [
    "# Load Jamendo tag/embedding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d573dacc-3e93-4d85-a200-951dbae79772",
   "metadata": {},
   "outputs": [],
   "source": [
    "JAMENDO_TAGS = np.array(['genre---alternative','genre---ambient','genre---atmospheric','genre---chillout','genre---classical','genre---dance','genre---downtempo','genre---easylistening','genre---electronic','genre---experimental','genre---folk','genre---funk','genre---hiphop','genre---house','genre---indie','genre---instrumentalpop','genre---jazz','genre---lounge','genre---metal','genre---newage','genre---orchestral','genre---pop','genre---popfolk','genre---poprock','genre---reggae','genre---rock','genre---soundtrack','genre---techno','genre---trance','genre---triphop','genre---world','instrument---acousticguitar','instrument---bass','instrument---computer','instrument---drummachine','instrument---drums','instrument---electricguitar','instrument---electricpiano','instrument---guitar','instrument---keyboard','instrument---piano','instrument---strings','instrument---synthesizer','instrument---violin','instrument---voice','mood/theme---emotional','mood/theme---energetic','mood/theme---film','mood/theme---happy','mood/theme---relaxing'])\n",
    "\n",
    "def get_top_tags(scores, k=3, threshold=.4):\n",
    "    assert scores.shape == (2, 50)\n",
    "    scores = (scores[0]+scores[1])/2\n",
    "    indices = np.where(scores>threshold)[0]\n",
    "    sorted_indices = indices[np.argsort(-scores[indices])[:k]]\n",
    "    return JAMENDO_TAGS[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3509c24f-e18d-49c2-b7f9-c7bccf6ce69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'jam_embeddings/tags_35.json'\n",
      "avg number of pred tags = 1.1215934627170583, fraction of samples with 0 pred tags = 0.21151453245426688\n"
     ]
    }
   ],
   "source": [
    "jam_tags = {}\n",
    "jam_pred_tags = {}\n",
    "jam_embeddings = {}\n",
    "jam_scores = {}\n",
    "\n",
    "jam_embeddings_dir = Path('./jam_embeddings')\n",
    "\n",
    "for i in range(100):\n",
    "    try:\n",
    "        with open(jam_embeddings_dir / f'tags_{i:02d}.json') as f:\n",
    "            jam_tags.update(json.load(f))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    data_dict = np.load(jam_embeddings_dir / f'embeddings_{i:02d}.npy', allow_pickle=True)\n",
    "    jam_embeddings.update(data_dict.item())\n",
    "    data_dict = np.load(jam_embeddings_dir / f'tag_scores_{i:02d}.npy', allow_pickle=True)\n",
    "    jam_scores.update(data_dict.item())\n",
    "    \n",
    "for k, v in jam_scores.items():\n",
    "    jam_pred_tags[k] = get_top_tags(v, k=3, threshold=0.4)\n",
    "    \n",
    "pred_tag_counts = np.array([len(v) for v in jam_pred_tags.values()])\n",
    "print(f'avg number of pred tags = {(pred_tag_counts).mean()}, fraction of samples with 0 pred tags = {(pred_tag_counts==0).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00172235-5771-4751-b92f-95f48a4781ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg number of jamendo tags = 4.659095552047543, fraction of samples with 0 jamendo tags = 0.0\n"
     ]
    }
   ],
   "source": [
    "pred_tag_counts = np.array([len(v) for v in jam_tags.values()])\n",
    "print(f'avg number of jamendo tags = {(pred_tag_counts).mean()}, fraction of samples with 0 jamendo tags = {(pred_tag_counts==0).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35e87a80-bd02-48a6-8b1e-a98572d3b91b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genre---progressiverock', 'genre---rock', 'genre---rocknroll']\n",
      "['genre---rock']\n",
      "\n",
      "['genre---progressiverock', 'genre---rock', 'genre---rocknroll']\n",
      "['genre---rock']\n",
      "\n",
      "['genre---progressiverock', 'genre---rock', 'genre---rocknroll']\n",
      "['genre---rock']\n",
      "\n",
      "['genre---progressiverock', 'genre---rock', 'genre---rocknroll']\n",
      "['genre---rock' 'instrument---drums']\n",
      "\n",
      "['genre---progressiverock', 'genre---rock', 'genre---rocknroll']\n",
      "['genre---rock' 'instrument---drums' 'instrument---bass']\n",
      "\n",
      "['genre---classicrock', 'genre---hardrock', 'genre---rocknroll']\n",
      "['genre---rock']\n",
      "\n",
      "['genre---classicrock', 'genre---hardrock', 'genre---rocknroll']\n",
      "['genre---rock']\n",
      "\n",
      "['genre---classicrock', 'genre---hardrock', 'genre---rocknroll']\n",
      "['genre---rock']\n",
      "\n",
      "['genre---classicrock', 'genre---hardrock', 'genre---rocknroll']\n",
      "['genre---rock']\n",
      "\n",
      "['genre---classicrock', 'genre---hardrock', 'genre---rocknroll']\n",
      "['genre---rock']\n",
      "\n",
      "['genre---electronic']\n",
      "['genre---electronic' 'genre---dance']\n",
      "\n",
      "['genre---electronic']\n",
      "['genre---electronic']\n",
      "\n",
      "['genre---electronic']\n",
      "['genre---electronic']\n",
      "\n",
      "['genre---electronic']\n",
      "['genre---electronic' 'genre---trance']\n",
      "\n",
      "['genre---electronic']\n",
      "['genre---electronic']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in list(jam_tags.keys())[:15]:\n",
    "    print(jam_tags[x])\n",
    "    print(jam_pred_tags[x])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "231898e7-18db-429c-8675-72749d95e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jam_shuffle_order = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a498d2eb-0ef5-4c39-9141-d02df4091f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JamendoTagDataset(Dataset):\n",
    "    def __init__(self, jam_tags, jam_pred_tags, jam_embeddings):\n",
    "        \n",
    "        self.keys = sorted(jam_tags.keys())\n",
    "        self.jam_tags = jam_tags\n",
    "        self.jam_pred_tags = jam_pred_tags\n",
    "        self.jam_embeddings = jam_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.jam_tags)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.keys[idx]\n",
    "        \n",
    "        tags = self.jam_tags[id]\n",
    "        categories = defaultdict(set)\n",
    "        for t in tags:\n",
    "            assert '---' in t\n",
    "            categories[t[:t.find('---')]].add(t[t.find('---')+3:])\n",
    "            \n",
    "        result = []\n",
    "        for k in sorted(categories.keys()):\n",
    "            cat_tags = list(categories[k])\n",
    "            \n",
    "            if jam_shuffle_order:\n",
    "                ts = random.sample(cat_tags, len(cat_tags))\n",
    "            else:\n",
    "                ts = sorted(cat_tags)\n",
    "            result.append(k + ': ' + ', '.join(ts))\n",
    "        tags_cap = '; '.join(result)\n",
    "            \n",
    "        #tags = [t.replace('---', ': ') for t in tags]\n",
    "        #tags = [t[t.find('---')+3:] if '---' in t else t for t in tags]\n",
    "        #random.shuffle(tags)\n",
    "        #tags_cap = ', '.join(tags)\n",
    "        \n",
    "        emb = self.jam_embeddings[id]\n",
    "        assert emb.shape == (2, 256)\n",
    "        emb = (emb[0]+emb[1])/2\n",
    "        \n",
    "        return tags_cap, torch.from_numpy(emb).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3974b191-383f-4c75-af5d-c15dc5ae19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jam_dataset = JamendoTagDataset(jam_tags, jam_pred_tags, jam_embeddings)\n",
    "train_frac = 0.9\n",
    "training_data, test_data = random_split(jam_dataset, [train_frac, 1-train_frac])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b8ca9-edaf-44bf-9dd9-f575d2c63188",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc1d14-1afd-4352-a64b-5876fbc45883",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "encoder_forward = model.encoder.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49344501-8f42-404c-86c4-00192ab45d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBS = torch.zeros(1, 1, 768).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527252e-33f3-4510-9259-1e387defd319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patched_forward(*args, **kwargs):\n",
    "    result = encoder_forward(*args, **kwargs) # this is just to appease the HuggingFace gods\n",
    "    result.last_hidden_state = (EMBS).repeat(1, 197, 1) # overwrite with actual embedding we use\n",
    "    return result\n",
    "\n",
    "# the original model uses a vision transformer in the encoder forward, so we get rid of that \n",
    "# and use the embeddings we have for the music\n",
    "\n",
    "model.encoder.forward = patched_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ccea63-6853-4d62-bfd8-7728f8838c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.forward(torch.zeros(1, 3, 224, 224).to(device), labels=torch.zeros(1, 1).long().to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3d9b-6745-43f7-91b4-1860ca8ae81b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55718f8-fe01-463a-99ef-ab3dc223ed3f",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "target should be:\n",
    "\n",
    "`\"<pad> caption <eos> <mask...>\"` (first element is dropped in transformer.forward)\n",
    "\n",
    "input should be:\n",
    "\n",
    "`\"<music-emb> caption <eos> <pad...>\"`\n",
    "\n",
    "where\n",
    "\n",
    "- `<bos>` = `<eos>` (for gpt2, see https://github.com/huggingface/transformers/issues/2026)\n",
    "- `<mask>` is -100 (masked in cross-entropy, see https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- `<pad>` is arbitrary\n",
    "- `<music-emb>` is the encoded music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82925589-cbb3-4f35-9099-3eb29831a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.fc(torch.nn.functional.relu(x))\n",
    "\n",
    "class B2T(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 768),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33e0d47c-b1e4-4837-bc6b-5f9b20d84ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(captions_batch):\n",
    "    input_ids = tokenizer(captions_batch)['input_ids']\n",
    "    \n",
    "     # wrap in eos and add placeholder for music embedding/mask\n",
    "    input_ids = [torch.tensor([eos_id, placeholder_id] + x + [eos_id]) for x in input_ids]\n",
    "    # pad with -100, this index is masked in the cross-entropy loss\n",
    "    input_ids_target = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids,\n",
    "        batch_first=True,\n",
    "        padding_value=mask_id\n",
    "    ).to(device)\n",
    "    \n",
    "    # index -100 isn't valid as model input however, since the token embedding lookup fails\n",
    "    # so we need a second version as model input, with -100 replaced with another token (shouldn't matter which)\n",
    "    input_ids = input_ids_target.clone()\n",
    "    input_ids[input_ids==mask_id] = eos_id\n",
    "    \n",
    "    return input_ids, input_ids_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "442e9e4a-b92d-47e3-a535-f47a25c8313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_ids(music_embedding, input_ids, input_ids_target):\n",
    "    music_emb_ind = 1\n",
    "    assert (input_ids[:, music_emb_ind]==placeholder_id).all()\n",
    "    assert (input_ids_target[:, music_emb_ind]==placeholder_id).all()\n",
    "\n",
    "    input_ids_target[:, music_emb_ind] = tokenizer.vocab['>'] # mask_id\n",
    "    input_ids[:, music_emb_ind] = eos_id # temp placeholder to make the embedding lookup work\n",
    "    inputs_embeds = model.transformer.wte(input_ids)\n",
    "    \n",
    "    inputs_embeds[:, music_emb_ind] = b2t(music_embedding)\n",
    "    \n",
    "    return inputs_embeds, input_ids_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f686686e-36ef-4c57-b9cd-0f26704c746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def manual_generate_single(inputs_embeds, max_length, do_sample):\n",
    "    \"\"\" Autoregressively generate max_len tokens based on the embedded prompt. \"\"\"\n",
    "    result = []\n",
    "    log_probs = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        logits = model.forward(inputs_embeds=inputs_embeds).logits[:, -1]\n",
    "\n",
    "        distr = torch.distributions.Categorical(logits=logits)\n",
    "        token_inds = distr.sample() if do_sample else logits.argmax(-1)\n",
    "        log_probs.append(distr.log_prob(token_inds))\n",
    "        \n",
    "        result.append(token_inds)\n",
    "\n",
    "        inputs_embeds = torch.cat([\n",
    "            inputs_embeds,\n",
    "            model.transformer.wte(token_inds).unsqueeze(1)\n",
    "        ], dim=1)\n",
    "        \n",
    "    log_probs = torch.stack(log_probs, dim=1)\n",
    "    ppl = 2**(-(1/len(log_probs))*log_probs.sum(-1))\n",
    "        \n",
    "    return torch.stack(result, dim=1), ppl\n",
    "\n",
    "@torch.no_grad()\n",
    "def manual_generate(inputs_embeds, iters, max_length, do_sample):\n",
    "    \"\"\" Repeatedly generate samples using manual_generate_single and return the ones with the lowest perplexity. \"\"\"\n",
    "    preds = []\n",
    "    ppls = []\n",
    "    \n",
    "    for i in range(iters):\n",
    "        pred, ppl = manual_generate_single(inputs_embeds, max_length, do_sample)\n",
    "        preds.append(pred)\n",
    "        ppls.append(ppl)\n",
    "    \n",
    "    preds = torch.stack(preds)\n",
    "    ppls = torch.stack(ppls)\n",
    "    \n",
    "    max_ppl_inds = ppls.argmin(0)\n",
    "    best_preds = preds[max_ppl_inds, np.arange(preds.shape[1])]\n",
    "    \n",
    "    return best_preds, ppls.min(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0ee227b-a433-4158-a86e-30b39001594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_eos(pred):\n",
    "    pred = [p.removeprefix('<|endoftext|>') for p in pred]\n",
    "    pred = [p[:p.find('<|endoftext|>')] if '<|endoftext|>' in p else p for p in pred]\n",
    "    return pred\n",
    "\n",
    "def eval(caption_batch, embedding_batch, use_manual_generation=False, rm_eos=False, **kwargs):\n",
    "    model.eval()\n",
    "    input_ids, input_ids_target = tokenize(caption_batch)\n",
    "    inputs_embeds, _ = transform_input_ids(\n",
    "        embedding_batch,\n",
    "        input_ids,\n",
    "        input_ids_target\n",
    "    )\n",
    "    inputs_embeds = inputs_embeds[:, :2]\n",
    "    \n",
    "    if use_manual_generation:\n",
    "        output_ids, ppl = manual_generate(inputs_embeds, **kwargs)\n",
    "    else:\n",
    "        output_ids = model.generate(inputs_embeds=inputs_embeds, **kwargs)\n",
    "    pred = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "    pred = [p.replace('\\n', '').strip() for p in pred]\n",
    "    return strip_eos(pred) if rm_eos else pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23c6b337-a156-4125-80fe-27229603dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2' # gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.config.is_decoder = True\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "b2t = B2T().cuda()\n",
    "\n",
    "mask_id = -100 # don't change, this is fixed in torch cross-entropy loss!\n",
    "eos_id = tokenizer.eos_token_id\n",
    "# bos_id = eos_id\n",
    "placeholder_id = -200\n",
    "\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': b2t.parameters(), 'lr': 0.00025},\n",
    "    # disable AdamW weight decay for gpt2 layer finetuning!\n",
    "    #{'params': model.parameters(), 'lr': 0, 'weight_decay': 0},\n",
    "    {'params': model.transformer.h[1].parameters(), 'lr': 0, 'weight_decay': 0},\n",
    "    {'params': model.transformer.h[2].parameters(), 'lr': 0, 'weight_decay': 0},\n",
    "    #{'params': model.transformer.h[3].parameters(), 'lr': 0, 'weight_decay': 0},\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 5000\n",
    "epoch = 0\n",
    "\n",
    "losses = []\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "eval_train_dataloader = DataLoader(training_data, 1, shuffle=True)\n",
    "eval_test_dataloader = DataLoader(test_data, 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a967eb-05d4-42a9-b871-4f5f4b5e3bc6",
   "metadata": {},
   "source": [
    "todo:\n",
    "- maybe use bos token at beginning and use attention_mask to mask out hidden states from the very first generated token? but: generate doesn't use attention_mask, which is weird? instead it has a pad_token_id/mask_token_id though? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e8319f31-ccd9-45c6-99d5-43d8a6adb2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_params_hf = dict(\n",
    "    max_new_tokens=64,\n",
    "    num_beams=4,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    bos_token_id=eos_id,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=mask_id,\n",
    "    early_stopping=True,\n",
    "    \n",
    "    use_manual_generation=False,\n",
    "    rm_eos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "03a454d8-4962-438a-a820-84ce77a9ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step(inputs_embeds, input_ids_target, apply_grad):\n",
    "    model.train()\n",
    "    loss = model.forward(inputs_embeds=inputs_embeds, labels=input_ids_target).loss\n",
    "    loss.backward()\n",
    "    \n",
    "    if apply_grad:\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def eval_step():\n",
    "    caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "    pred = eval(caption_batch, embedding_batch, **generation_params_hf)\n",
    "    \n",
    "    printr('[green bold]TRAIN TRUE: ' + caption_batch[0])\n",
    "    printr('[blue]TRAIN PRED-HF: ' + pred[0])\n",
    "    #pred = eval(caption_batch, embedding_batch, **generation_params_ours)\n",
    "    #printr('[blue]TRAIN PRED-OUR: ' + pred[0])\n",
    "\n",
    "    caption_batch, embedding_batch = next(iter(eval_test_dataloader))\n",
    "    pred = eval(caption_batch, embedding_batch, **generation_params_hf)\n",
    "    printr('[green bold]TEST TRUE: ' + caption_batch[0])\n",
    "    printr('[blue]TEST PRED-HF: ' + pred[0])\n",
    "    #pred = eval(caption_batch, embedding_batch, **generation_params_ours)\n",
    "    #printr('[blue]TEST PRED-OUR: ' + pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "12bb04ec-e6a7-4afd-afaa-c1e15df0ed5a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c107f1b13e334491bfd99e0a7553884b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4967 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfroze gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3b43da00ae449ea3d2686ea5da4866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3786 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">TRAIN TRUE: genre: classical; instrument: piano</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTRAIN TRUE: genre: classical; instrument: piano\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">TRAIN PRED-HF: genre: classical; instrument: piano</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mTRAIN PRED-HF: genre: classical; instrument: piano\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">TEST TRUE: genre: dub, electronic, reggae; instrument: bass, drums, electricguitar</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTEST TRUE: genre: dub, electronic, reggae; instrument: bass, drums, electricguitar\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">TEST PRED-HF: genre: dub, reggae; instrument: bass, drums, electricguitar</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mTEST PRED-HF: genre: dub, reggae; instrument: bass, drums, electricguitar\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 21\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs_embeds, input_ids_target \u001b[38;5;241m=\u001b[39m transform_input_ids(\n\u001b[1;32m     14\u001b[0m     embedding_batch,\n\u001b[1;32m     15\u001b[0m     input_ids,\n\u001b[1;32m     16\u001b[0m     input_ids_target\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m apply_grad_cond \u001b[38;5;241m=\u001b[39m (epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (step\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m8\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapply_grad_cond\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m     eval_step()\n",
      "Cell \u001b[0;32mIn[81], line 3\u001b[0m, in \u001b[0;36mupdate_step\u001b[0;34m(inputs_embeds, input_ids_target, apply_grad)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_step\u001b[39m(inputs_embeds, input_ids_target, apply_grad):\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 3\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids_target\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m      4\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m apply_grad:\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1075\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1075\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:899\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    889\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    890\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    891\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    896\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    897\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    387\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    388\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 389\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    398\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:330\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    328\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:200\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    197\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epoch, num_epochs)):\n",
    "    \n",
    "    torch.save({'model': model.state_dict(), 'b2t': b2t.state_dict(), 'opt': opt}, f'./checkpoints/chkp_{epoch}.pt')    \n",
    "    \n",
    "    if epoch >= 3:\n",
    "        print('Unfroze gpt2')\n",
    "        for pg in opt.param_groups[1:]:\n",
    "            pg['lr'] = 0.3*5e-5\n",
    "    \n",
    "    for step, (caption_batch, embedding_batch) in enumerate(tqdm(train_dataloader)):\n",
    "        # tokenize and prepare inputs for forward\n",
    "        input_ids, input_ids_target = tokenize(caption_batch)\n",
    "        inputs_embeds, input_ids_target = transform_input_ids(\n",
    "            embedding_batch,\n",
    "            input_ids,\n",
    "            input_ids_target\n",
    "        )\n",
    "        \n",
    "        apply_grad_cond = (epoch < 10) or (step%8==0)\n",
    "        \n",
    "        losses.append(update_step(inputs_embeds, input_ids_target, apply_grad=apply_grad_cond))\n",
    "        \n",
    "        if step % 300 == 0:\n",
    "            eval_step()\n",
    "            plt.plot(losses)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "873d6f62-2f70-4bf4-a2f0-ba959d8dab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_eval_test_dataloader = DataLoader(test_data, 1, shuffle=True)\n",
    "eval_true_captions = []\n",
    "eval_pred_captions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b3eea662-6dd1-4e3f-ab68-5adef14597d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36770f06ac1546cbb73bb82abedd2807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for j in range(1000):\n",
    "    data_dict = torch.load(f'./checkpoints/chkp_{j}.pt')\n",
    "    model.load_state_dict(data_dict['model'])\n",
    "    b2t.load_state_dict(data_dict['b2t'])\n",
    "\n",
    "    true, preds = [], []\n",
    "    for i, (caption_batch, embedding_batch) in enumerate(tqdm(final_eval_test_dataloader)):\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params_hf)\n",
    "        true.append(caption_batch[0])\n",
    "        preds.append(pred[0])\n",
    "        if i >= 300:\n",
    "            break\n",
    "        \n",
    "    eval_true_captions.append(true)\n",
    "    eval_pred_captions.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c5364d2e-c4e1-44a6-87f5-7048b8f94a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "google_bleu = evaluate.load(\"google_bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ddc0471-8e56-4ae5-87d5-1d8d6a83d0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1248a76-9212-49d6-9e37-3b7721ca13eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dict(\n",
    "    eval_true_captions=eval_true_captions,\n",
    "    eval_pred_captions=eval_pred_captions\n",
    "), open('preds.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed98a0f8-66fd-4d0e-88b3-e9e3d5e0f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu = []\n",
    "\n",
    "\n",
    "for p, t in zip(eval_pred_captions, eval_true_captions):\n",
    "    gleu.append(google_bleu.compute(predictions=p, references=t)['google_bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "da123791-1fdd-4e22-bc2c-e1916a9d8f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f96dd8899d0>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1bd0ba-e237-4222-899b-dcce7060bcb6",
   "metadata": {},
   "source": [
    "**Options**\n",
    "\n",
    "- different gpt2 sizes\n",
    "- gpt2 self-att vs gpt-2 cross-att (image captioning)\n",
    "- which gpt2 layers to finetune\n",
    "- let b2t output a bunch of 768 dimensional vectors to attend to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025fe70c-1e43-45be-990c-92a44e3ab7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4acba2-ab63-49a4-be8f-16534ddb4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = json.load(open('preds.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3fff1-0e1e-4876-915b-ece7b763c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE: genre: ambient, soundtrack, classical; instrument: piano\n",
      "PRED: genre: soundtrack, classical, ambient; instrument: piano\n",
      "\n",
      "TRUE: genre: pop, popfolk, world; instrument: percussion, acousticguitar, cajon, bass, piano\n",
      "PRED: genre: folk, popfolk; instrument: ukulele; mood/theme: happy\n",
      "\n",
      "TRUE: genre: electronic; instrument: piano; mood/theme: film\n",
      "PRED: genre: classical, soundtrack; instrument: piano; mood/theme: film\n",
      "\n",
      "TRUE: genre: classical, soundtrack, atmospheric; instrument: organ\n",
      "PRED: genre: electronic\n",
      "\n",
      "TRUE: genre: reggae, ska\n",
      "PRED: genre: reggae\n",
      "\n",
      "TRUE: genre: rock\n",
      "PRED: genre: poprock, pop; instrument: electricguitar, bass, drums\n",
      "\n",
      "TRUE: genre: folk, instrumentalpop\n",
      "PRED: genre: popfolk, folk; instrument: acousticguitar, ukulele; mood/theme: folkrock\n",
      "\n",
      "TRUE: genre: electronic, chillout, ambient, easylistening\n",
      "PRED: genre: techno, minimaltechno; mood/theme: deep\n",
      "\n",
      "TRUE: genre: hiphop, rap\n",
      "PRED: genre: electronic\n",
      "\n",
      "TRUE: genre: experimental, blues; instrument: electricguitar, synthesizer, woodblock, hammond, voice, drums, washboard, bass, marimba; mood/theme: romantic\n",
      "PRED: genre: rock, poprock; instrument: electricguitar, bass, drums\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('TRUE:', preds['eval_true_captions'][-1][i])\n",
    "    print('PRED:', preds['eval_pred_captions'][-1][i])    \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp2",
   "language": "python",
   "name": "tp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
