{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cd60a10-aba2-4f3e-b251-77d4f100179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U yt-dlp==2023.1.6 matplotlib==3.6.0 datasets[audio] rich\n",
    "\n",
    "# install newest transformers build to be able to pass `inputs_embeds` through generate()\n",
    "# !pip install --upgrade git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426135f-00ed-4215-9b20-ee38b2c0448a",
   "metadata": {},
   "source": [
    "**Relevant huggingface gpt2 code**\n",
    "\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py\n",
    "- https://github.com/huggingface/transformers/issues/6535"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485da5-3e40-42f1-873e-ccb244a74bc3",
   "metadata": {},
   "source": [
    "# Load musiccaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bb0885-c2da-43e2-8df4-468ee5f97594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiccaps import load_musiccaps\n",
    "import numpy as np\n",
    "from rich import print as printr\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import math\n",
    "from rich import print as printr\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6684ae9-0e53-415d-8be8-a67f90f7030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_muscaps_with_embeddings(ds, embeddings):\n",
    "    '''Some clips weren't downloaded so we couldn't embed them, get rid of that'''\n",
    "    exclude_ids = set()\n",
    "    for i in range(len(ds)):\n",
    "        if ds[i]['ytid'] not in embeddings.keys():\n",
    "            exclude_ids.add(i)\n",
    "    ds = ds.select(\n",
    "        (\n",
    "            i for i in range(len(ds)) \n",
    "            if i not in set(exclude_ids)\n",
    "        )\n",
    "    )\n",
    "    assert len(ds) == len(embeddings)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8926e06-ef8c-40a7-8675-f4d1285d0afe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration google--MusicCaps-7925612b943f961b\n",
      "Found cached dataset csv (/home/dominik/.cache/huggingface/datasets/google___csv/google--MusicCaps-7925612b943f961b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    }
   ],
   "source": [
    "ds = load_musiccaps(\n",
    "    './music_data',\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True\n",
    ")\n",
    "embeddings = np.load('embeddings.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15420704-a6bd-4315-b6b9-ba41fee54a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionEmbedding(Dataset):\n",
    "    '''Returns a torch Dataset of paired captions and embeddings'''\n",
    "    def __init__(self, muscaps_ds, embeddings):\n",
    "        ds = filter_muscaps_with_embeddings(muscaps_ds, embeddings)\n",
    "        self.captions = ds.sort(column='ytid')['caption']\n",
    "        sorted_embs = [value for _, value in sorted(embeddings.items())]\n",
    "        self.embeddings = torch.from_numpy(np.stack(sorted_embs)).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.captions[idx], self.embeddings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51464487-9c40-4dc5-8436-372b612e4d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object filter_muscaps_with_embeddings.<locals>.<genexpr> at 0x7f8d187bbac0> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "dataset = CaptionEmbedding(muscaps_ds=ds, embeddings=embeddings)\n",
    "\n",
    "# quick check did not mess up ordering of caption-embedding pairs\n",
    "# for cap, emb in tqdm(dataset):\n",
    "#     for i in range(len(ds)):\n",
    "#         if cap == ds[i]['caption']:\n",
    "#             assert torch.allclose(emb,torch.from_numpy(embeddings[ds[i]['ytid']]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce82080-8e03-45ae-8266-6e0f2af6843c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspects = []\n",
    "for x in ds:\n",
    "    aspect_str = x['aspect_list']\n",
    "    for t in ('[]\"\\''):\n",
    "        aspect_str = aspect_str.replace(t, '')\n",
    "    aspects.extend(aspect_str.split(', '))\n",
    "    \n",
    "from collections import Counter\n",
    "aspects = {s for s, count in Counter(aspects).most_common() if count >= 20}\n",
    "len(aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3d9b-6745-43f7-91b4-1860ca8ae81b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55718f8-fe01-463a-99ef-ab3dc223ed3f",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "target should be:\n",
    "\n",
    "`\"<bos> <mask> caption <eos> <mask...>\"` (first element is dropped in transformer.forward)\n",
    "\n",
    "input should be:\n",
    "\n",
    "`\"<bos> <music-emb> caption <eos> <pad...>\"`\n",
    "\n",
    "where\n",
    "\n",
    "- `<bos>` = `<eos>` (for gpt2)\n",
    "- `<mask>` is -100\n",
    "- `<pad>` is arbitrary\n",
    "- `<music-emb>` is the encoded music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73423de6-f9e5-4130-b22a-8e3cf07851e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(captions_batch):\n",
    "    input_ids = tokenizer(captions_batch)['input_ids']\n",
    "    eos = tokenizer.eos_token_id\n",
    "     # wrap in eos token (see https://github.com/huggingface/transformers/issues/2026)\n",
    "    input_ids = [torch.tensor([eos] + x + [eos]) for x in input_ids]\n",
    "    # pad with -100, this index is masked in the cross-entropy loss\n",
    "    # (see https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "    input_ids_target = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=-100).to(device)\n",
    "    \n",
    "    # index -100 isn't valid as model input however, since the token embedding lookup fails\n",
    "    # so we need a second version as input with -100 replaced with another token (shouldn't matter which)\n",
    "    input_ids = input_ids_target.clone()\n",
    "    input_ids[input_ids==-100] = eos\n",
    "    \n",
    "    # the model input will be prefixed with the music embedding, \n",
    "    # so we need to prefix the target too with some token to get the shapes to match\n",
    "    # (maybe masked -100 is better)\n",
    "    input_ids_target = torch.cat([\n",
    "        torch.full((len(input_ids_target), 1), fill_value=-100, device=device),\n",
    "        input_ids_target,\n",
    "    ], dim=1)\n",
    "    \n",
    "    return input_ids, input_ids_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82925589-cbb3-4f35-9099-3eb29831a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class B2T(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(512, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f961a86b-0b65-48f1-827e-9eb053712537",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2' # gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "b2t = B2T().cuda()\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "    {'params': b2t.parameters(), 'lr': 0.0005},\n",
    "])\n",
    "\n",
    "generation_params = dict(\n",
    "    max_length=32,\n",
    "    num_beams=4,\n",
    "    do_sample=True,\n",
    "    temperature=0.95,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "losses = []\n",
    "\n",
    "dataset = CaptionEmbedding(muscaps_ds=ds, embeddings=embeddings)\n",
    "\n",
    "train_frac = 0.8\n",
    "training_data, test_data = random_split(dataset, [train_frac, 1-train_frac])\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "eval_train_dataloader = DataLoader(training_data, 1, shuffle=True)\n",
    "eval_test_dataloader = DataLoader(test_data, 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f686686e-36ef-4c57-b9cd-0f26704c746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def manual_generate_single(inputs_embeds, max_len, sample):\n",
    "    result = []\n",
    "    log_probs = []\n",
    "\n",
    "    for i in range(max_len):\n",
    "        logits = model.forward(inputs_embeds=inputs_embeds).logits[:, -1]\n",
    "\n",
    "        distr = torch.distributions.Categorical(logits=logits)\n",
    "        token_inds = distr.sample() if sample else logits.argmax(-1)\n",
    "        log_probs.append(distr.log_prob(token_inds))\n",
    "        \n",
    "        result.append(token_inds)\n",
    "\n",
    "        inputs_embeds = torch.cat([\n",
    "            inputs_embeds,\n",
    "            model.transformer.wte(token_inds).unsqueeze(1)\n",
    "        ], dim=1)\n",
    "        \n",
    "    log_probs = torch.stack(log_probs, dim=1)\n",
    "    ppl = 2**(-(1/len(log_probs))*log_probs.sum(-1))\n",
    "        \n",
    "    return torch.stack(result, dim=1), ppl\n",
    "\n",
    "@torch.no_grad()\n",
    "def manual_generate(iters, *args, **kwargs):\n",
    "    preds = []\n",
    "    ppls = []\n",
    "    \n",
    "    for i in range(iters):\n",
    "        pred, ppl = manual_generate_single(*args, **kwargs)\n",
    "        preds.append(pred)\n",
    "        ppls.append(ppl)\n",
    "    \n",
    "    preds = torch.stack(preds)\n",
    "    ppls = torch.stack(ppls)\n",
    "    \n",
    "    max_ppl_inds = ppls.argmax(0)\n",
    "    best_preds = preds[max_ppl_inds, np.arange(preds.shape[1])]\n",
    "    \n",
    "    return ppls.max(0), best_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0ee227b-a433-4158-a86e-30b39001594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data_loader, use_manual_generation=False):\n",
    "    model.eval()\n",
    "    caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "    input_ids, input_ids_target = tokenize(caption_batch)\n",
    "    \n",
    "    inputs_embeds = torch.cat([\n",
    "        b2t(embedding_batch).unsqueeze(1),\n",
    "        model.transformer.wte(input_ids),\n",
    "    ], dim=1)\n",
    "    \n",
    "    if use_manual_generation:\n",
    "        output_ids, ppl = manual_generate(1, inputs_embeds, 64, True)\n",
    "    else:\n",
    "        output_ids = model.generate(inputs_embeds=inputs_embeds, **generation_params)\n",
    "    pred = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "    printr('[blue bold]PRED: ' + pred.replace('\\n', ' '))\n",
    "    printr('[green bold]TRUE: ' + caption_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12bb04ec-e6a7-4afd-afaa-c1e15df0ed5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16a2a32aa7b4bb8bed5d382c7c60760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460daf10f0274d2a9be35bf2d5b857d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MarkupError",
     "evalue": "closing tag '[/note]' at position 313 doesn't match any open tag",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMarkupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meval_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_manual_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28meval\u001b[39m(eval_train_dataloader)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28meval\u001b[39m(eval_test_dataloader, use_manual_generation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[28], line 16\u001b[0m, in \u001b[0;36meval\u001b[0;34m(data_loader, use_manual_generation)\u001b[0m\n\u001b[1;32m     14\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_params)\n\u001b[1;32m     15\u001b[0m pred \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mprintr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[blue bold]PRED: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m printr(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[green bold]TRUE: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m caption_batch[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/rich/__init__.py:74\u001b[0m, in \u001b[0;36mprint\u001b[0;34m(sep, end, file, flush, *objects)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsole\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Console\n\u001b[1;32m     73\u001b[0m write_console \u001b[38;5;241m=\u001b[39m get_console() \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m Console(file\u001b[38;5;241m=\u001b[39mfile)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrite_console\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/rich/console.py:1668\u001b[0m, in \u001b[0;36mConsole.print\u001b[0;34m(self, sep, end, style, justify, overflow, no_wrap, emoji, markup, highlight, width, height, crop, soft_wrap, new_line_start, *objects)\u001b[0m\n\u001b[1;32m   1666\u001b[0m render_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_hooks[:]\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m-> 1668\u001b[0m     renderables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_renderables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjustify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjustify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m        \u001b[49m\u001b[43memoji\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memoji\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmarkup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhighlight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhighlight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1677\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m render_hooks:\n\u001b[1;32m   1678\u001b[0m         renderables \u001b[38;5;241m=\u001b[39m hook\u001b[38;5;241m.\u001b[39mprocess_renderables(renderables)\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/rich/console.py:1532\u001b[0m, in \u001b[0;36mConsole._collect_renderables\u001b[0;34m(self, objects, sep, end, justify, emoji, markup, highlight)\u001b[0m\n\u001b[1;32m   1529\u001b[0m renderable \u001b[38;5;241m=\u001b[39m rich_cast(renderable)\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(renderable, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1531\u001b[0m     append_text(\n\u001b[0;32m-> 1532\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_str\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrenderable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memoji\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memoji\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarkup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhighlighter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_highlighter\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1535\u001b[0m     )\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(renderable, Text):\n\u001b[1;32m   1537\u001b[0m     append_text(renderable)\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/rich/console.py:1424\u001b[0m, in \u001b[0;36mConsole.render_str\u001b[0;34m(self, text, style, justify, overflow, emoji, markup, highlight, highlighter)\u001b[0m\n\u001b[1;32m   1421\u001b[0m highlight_enabled \u001b[38;5;241m=\u001b[39m highlight \u001b[38;5;129;01mor\u001b[39;00m (highlight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_highlight)\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markup_enabled:\n\u001b[0;32m-> 1424\u001b[0m     rich_text \u001b[38;5;241m=\u001b[39m \u001b[43mrender_markup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m        \u001b[49m\u001b[43memoji\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memoji_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m        \u001b[49m\u001b[43memoji_variant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emoji_variant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1430\u001b[0m     rich_text\u001b[38;5;241m.\u001b[39mjustify \u001b[38;5;241m=\u001b[39m justify\n\u001b[1;32m   1431\u001b[0m     rich_text\u001b[38;5;241m.\u001b[39moverflow \u001b[38;5;241m=\u001b[39m overflow\n",
      "File \u001b[0;32m~/anaconda3/envs/tp2/lib/python3.9/site-packages/rich/markup.py:161\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(markup, style, emoji, emoji_variant)\u001b[0m\n\u001b[1;32m    159\u001b[0m         start, open_tag \u001b[38;5;241m=\u001b[39m pop_style(style_name)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MarkupError(\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosing tag \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;241m.\u001b[39mmarkup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mposition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match any open tag\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# implicit close\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mMarkupError\u001b[0m: closing tag '[/note]' at position 313 doesn't match any open tag"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for step, (caption_batch, embedding_batch) in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids, input_ids_target = tokenize(caption_batch)\n",
    "        \n",
    "        inputs_embeds = torch.cat([\n",
    "            b2t(embedding_batch).unsqueeze(1),\n",
    "            model.transformer.wte(input_ids),\n",
    "        ], dim=1)\n",
    "        \n",
    "        model.train()\n",
    "        loss = model.forward(inputs_embeds=inputs_embeds, labels=input_ids_target).loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            eval(eval_train_dataloader, use_manual_generation=True)\n",
    "            eval(eval_train_dataloader)\n",
    "            eval(eval_test_dataloader, use_manual_generation=True)\n",
    "            eval(eval_test_dataloader)\n",
    "            print('\\n')\n",
    "\n",
    "        if step % 200 == 199:\n",
    "            plt.plot(losses)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff022ee0-672f-4ea0-a56e-df913fc14572",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade0399d-b9a9-4532-8bba-5680bbc3d44f",
   "metadata": {},
   "source": [
    "# random experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03a3d199-a333-4807-b30c-b90cd090b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.tensor(tokenizer.encode('List of 10 cute animals: dog, cat, '))\n",
    "inputs_embeds = model.transformer.wte(seq.cuda()).unsqueeze(0)\n",
    "#inputs_embeds = torch.cat([weird_word_embedding, inputs_embeds], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756841be-52ed-405f-85f5-3996dcf5b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.batch_encode_plus(['List of 10 cute animals: dog, cat, '], return_tensors='pt')\n",
    "input_ids = encoded['input_ids'].cuda()\n",
    "attention_mask = encoded['attention_mask'].cuda()\n",
    "inputs_embeds = model.transformer.wte(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed52c940-4508-49f4-bf1e-b8116786610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ursine, horse, elephant, goat, fish, sheep, pig, chicken, piglet, mouse, rat, rat-tat-tat']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you don't pass input_ids here, the output is the same, minus the prompt\n",
    "outputs = model.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, **generation_params)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3375be97-5779-41b2-9215-cb8922eed52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['List of 10 cute animals: dog, cat, urchin, fish, frog, rabbit, snake, squid, and bird.\\n\\n1. Dog']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(input_ids, **generation_params)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp2",
   "language": "python",
   "name": "tp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
