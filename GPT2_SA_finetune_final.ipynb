{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cd60a10-aba2-4f3e-b251-77d4f100179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U yt-dlp==2023.1.6 matplotlib==3.6.0 datasets[audio] rich\n",
    "\n",
    "# install newest transformers build to be able to pass `inputs_embeds` through generate()\n",
    "# !pip install --upgrade git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426135f-00ed-4215-9b20-ee38b2c0448a",
   "metadata": {},
   "source": [
    "**Relevant huggingface gpt2 code**\n",
    "\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py\n",
    "- https://github.com/huggingface/transformers/issues/6535"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485da5-3e40-42f1-873e-ccb244a74bc3",
   "metadata": {},
   "source": [
    "# Load musiccaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bb0885-c2da-43e2-8df4-468ee5f97594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiccaps import load_musiccaps\n",
    "import numpy as np\n",
    "from rich import print as printr\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import math\n",
    "from rich import print as printr\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6684ae9-0e53-415d-8be8-a67f90f7030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_muscaps_with_embeddings(ds, embeddings):\n",
    "    '''Some clips weren't downloaded so we couldn't embed them, get rid of that'''\n",
    "    exclude_ids = set()\n",
    "    for i in range(len(ds)):\n",
    "        if ds[i]['ytid'] not in embeddings.keys():\n",
    "            exclude_ids.add(i)\n",
    "    ds = ds.select(\n",
    "        (\n",
    "            i for i in range(len(ds)) \n",
    "            if i not in set(exclude_ids)\n",
    "        )\n",
    "    )\n",
    "    assert len(ds) == len(embeddings)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8926e06-ef8c-40a7-8675-f4d1285d0afe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration google--MusicCaps-7925612b943f961b\n",
      "Found cached dataset csv (/home/dominik/.cache/huggingface/datasets/google___csv/google--MusicCaps-7925612b943f961b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    }
   ],
   "source": [
    "ds = load_musiccaps(\n",
    "    './music_data',\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True\n",
    ")\n",
    "embeddings = np.load('embeddings.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15420704-a6bd-4315-b6b9-ba41fee54a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionEmbedding(Dataset):\n",
    "    '''Returns a torch Dataset of paired captions and embeddings'''\n",
    "    def __init__(self, muscaps_ds, embeddings):\n",
    "        ds = filter_muscaps_with_embeddings(muscaps_ds, embeddings)\n",
    "        self.captions = ds.sort(column='ytid')['caption']\n",
    "        sorted_embs = [value for _, value in sorted(embeddings.items())]\n",
    "        self.embeddings = torch.from_numpy(np.stack(sorted_embs)).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.captions[idx], self.embeddings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51464487-9c40-4dc5-8436-372b612e4d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object filter_muscaps_with_embeddings.<locals>.<genexpr> at 0x7f9e83f3da50> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "dataset = CaptionEmbedding(muscaps_ds=ds, embeddings=embeddings)\n",
    "\n",
    "# quick check did not mess up ordering of caption-embedding pairs\n",
    "# for cap, emb in tqdm(dataset):\n",
    "#     for i in range(len(ds)):\n",
    "#         if cap == ds[i]['caption']:\n",
    "#             assert torch.allclose(emb,torch.from_numpy(embeddings[ds[i]['ytid']]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce82080-8e03-45ae-8266-6e0f2af6843c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of music-related words to use for evaluation\n",
    "aspects = []\n",
    "for x in ds:\n",
    "    aspect_str = x['aspect_list']\n",
    "    for t in ('[]\"\\''):\n",
    "        aspect_str = aspect_str.replace(t, '')\n",
    "    aspects.extend(aspect_str.split(', '))\n",
    "    \n",
    "from collections import Counter\n",
    "# only pick aspects that show up somewhat frequently\n",
    "aspects = {s for s, count in Counter(aspects).most_common() if count >= 25}\n",
    "len(aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3d9b-6745-43f7-91b4-1860ca8ae81b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55718f8-fe01-463a-99ef-ab3dc223ed3f",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "target should be:\n",
    "\n",
    "`\"<bos> <mask> caption <eos> <mask...>\"` (first element is dropped in transformer.forward)\n",
    "\n",
    "input should be:\n",
    "\n",
    "`\"<bos> <music-emb> caption <eos> <pad...>\"`\n",
    "\n",
    "where\n",
    "\n",
    "- `<bos>` = `<eos>` (for gpt2, see https://github.com/huggingface/transformers/issues/2026)\n",
    "- `<mask>` is -100 (masked in cross-entropy, see https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- `<pad>` is arbitrary\n",
    "- `<music-emb>` is the encoded music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82925589-cbb3-4f35-9099-3eb29831a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class B2T(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f961a86b-0b65-48f1-827e-9eb053712537",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2' # gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "b2t = B2T().cuda()\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "    {'params': b2t.parameters(), 'lr': 0.00025},\n",
    "    # disable AdamW weight decay for gpt2 layer finetuning!\n",
    "    {'params': model.transformer.h[0].attn.parameters(), 'lr': 0, 'weight_decay': 0},\n",
    "])\n",
    "\n",
    "train_frac = 0.8\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "losses = []\n",
    "dataset = CaptionEmbedding(muscaps_ds=ds, embeddings=embeddings)\n",
    "training_data, test_data = random_split(dataset, [train_frac, 1-train_frac])\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "eval_train_dataloader = DataLoader(training_data, 1, shuffle=True)\n",
    "eval_test_dataloader = DataLoader(test_data, 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33e0d47c-b1e4-4837-bc6b-5f9b20d84ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_id = -100 # don't change, this is fixed in torch cross-entropy loss!\n",
    "eos_id = tokenizer.eos_token_id\n",
    "placeholder_id = -200\n",
    "\n",
    "def tokenize(captions_batch):\n",
    "    input_ids = tokenizer(captions_batch)['input_ids']\n",
    "    \n",
    "     # wrap in eos and add placeholder for music embedding/mask\n",
    "    input_ids = [torch.tensor([eos_id, placeholder_id] + x + [eos_id]) for x in input_ids]\n",
    "    # pad with -100, this index is masked in the cross-entropy loss\n",
    "    input_ids_target = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids,\n",
    "        batch_first=True,\n",
    "        padding_value=mask_id\n",
    "    ).to(device)\n",
    "    \n",
    "    # index -100 isn't valid as model input however, since the token embedding lookup fails\n",
    "    # so we need a second version as model input, with -100 replaced with another token (shouldn't matter which)\n",
    "    input_ids = input_ids_target.clone()\n",
    "    input_ids[input_ids==mask_id] = eos_id\n",
    "    \n",
    "    return input_ids, input_ids_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "442e9e4a-b92d-47e3-a535-f47a25c8313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_ids(music_embedding, input_ids, input_ids_target):\n",
    "    assert (input_ids[:, 1]==placeholder_id).all()\n",
    "    assert (input_ids_target[:, 1]==placeholder_id).all()\n",
    "\n",
    "    input_ids_target[:, 1] = mask_id\n",
    "    input_ids[:, 1] = eos_id # temp placeholder to make the embedding lookup work\n",
    "    inputs_embeds = model.transformer.wte(input_ids)\n",
    "    \n",
    "    music_embedding = b2t(music_embedding)\n",
    "    inputs_embeds[:, 1] = music_embedding\n",
    "    \n",
    "    return inputs_embeds, input_ids_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f686686e-36ef-4c57-b9cd-0f26704c746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def manual_generate_single(inputs_embeds, max_length, do_sample):\n",
    "    \"\"\" Autoregressively generate max_len tokens based on the embedded prompt. \"\"\"\n",
    "    result = []\n",
    "    log_probs = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        logits = model.forward(inputs_embeds=inputs_embeds).logits[:, -1]\n",
    "\n",
    "        distr = torch.distributions.Categorical(logits=logits)\n",
    "        token_inds = distr.sample() if do_sample else logits.argmax(-1)\n",
    "        log_probs.append(distr.log_prob(token_inds))\n",
    "        \n",
    "        result.append(token_inds)\n",
    "\n",
    "        inputs_embeds = torch.cat([\n",
    "            inputs_embeds,\n",
    "            model.transformer.wte(token_inds).unsqueeze(1)\n",
    "        ], dim=1)\n",
    "        \n",
    "    log_probs = torch.stack(log_probs, dim=1)\n",
    "    ppl = 2**(-(1/len(log_probs))*log_probs.sum(-1))\n",
    "        \n",
    "    return torch.stack(result, dim=1), ppl\n",
    "\n",
    "@torch.no_grad()\n",
    "def manual_generate(inputs_embeds, iters, max_length, do_sample):\n",
    "    \"\"\" Repeatedly generate samples using manual_generate_single and return the ones with the highest perplexity. \"\"\"\n",
    "    preds = []\n",
    "    ppls = []\n",
    "    \n",
    "    for i in range(iters):\n",
    "        pred, ppl = manual_generate_single(inputs_embeds, max_length, do_sample)\n",
    "        preds.append(pred)\n",
    "        ppls.append(ppl)\n",
    "    \n",
    "    preds = torch.stack(preds)\n",
    "    ppls = torch.stack(ppls)\n",
    "    \n",
    "    max_ppl_inds = ppls.argmax(0)\n",
    "    best_preds = preds[max_ppl_inds, np.arange(preds.shape[1])]\n",
    "    \n",
    "    return best_preds, ppls.max(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0ee227b-a433-4158-a86e-30b39001594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(caption_batch, embedding_batch, use_manual_generation=False, **kwargs):\n",
    "    model.eval()\n",
    "    input_ids, input_ids_target = tokenize(caption_batch)\n",
    "    inputs_embeds, input_ids_target = transform_input_ids(\n",
    "        embedding_batch,\n",
    "        input_ids,\n",
    "        input_ids_target\n",
    "    )\n",
    "    \n",
    "    if use_manual_generation:\n",
    "        output_ids, ppl = manual_generate(inputs_embeds, **kwargs)\n",
    "    else:\n",
    "        output_ids = model.generate(inputs_embeds=inputs_embeds, **kwargs)\n",
    "    pred = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "    \n",
    "    pred = [p.replace('\\n', '').strip() for p in pred]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "893cec0b-8b31-4ae2-af1d-4615d7e1b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_params_hf = dict(\n",
    "    max_length=48,\n",
    "    num_beams=4,\n",
    "    do_sample=True,\n",
    "    temperature=0.95,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    use_manual_generation=False\n",
    ")\n",
    "\n",
    "generation_params_ours = dict(\n",
    "    max_length=48,\n",
    "    iters=1,\n",
    "    do_sample=True,\n",
    "    use_manual_generation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb04ec-e6a7-4afd-afaa-c1e15df0ed5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633a390b30a34dca99dc2b588853e653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736caebb223e47c1879166bb4a88fd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">TRAIN TRUE: This techno song features programmed percussion playing at a fast tempo. After two bars, the percussion</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">plays a crash cymbal and pauses. There are layers of synth sounds. One layer is playing a high pitched repetitive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">melody. Another layer is playing the lower octaves of the same melody but the sound is different. Another synth is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">playing the bass parts using the same melody. This is an instrumental song with a techno feel. This song can be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">used in an advertisement for a luxury item.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTRAIN TRUE: This techno song features programmed percussion playing at a fast tempo. After two bars, the percussion\u001b[0m\n",
       "\u001b[1;32mplays a crash cymbal and pauses. There are layers of synth sounds. One layer is playing a high pitched repetitive \u001b[0m\n",
       "\u001b[1;32mmelody. Another layer is playing the lower octaves of the same melody but the sound is different. Another synth is \u001b[0m\n",
       "\u001b[1;32mplaying the bass parts using the same melody. This is an instrumental song with a techno feel. This song can be \u001b[0m\n",
       "\u001b[1;32mused in an advertisement for a luxury item.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">TRAIN PRED-A: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;|endoftext|&gt;</span><span style=\"color: #000080; text-decoration-color: #000080\">This is a rush transcript. Copy may not be in its final form.AMY GOODMAN: This is </span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">Democracy Now!, democracynow.org, The War and Peace Report. I'm Amy Goodman. I'm Amy Goodman, with</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mTRAIN PRED-A: \u001b[0m\u001b[1;34m<\u001b[0m\u001b[1;34m|endoftext|\u001b[0m\u001b[1;34m>\u001b[0m\u001b[34mThis is a rush transcript. Copy may not be in its final form.AMY GOODMAN: This is \u001b[0m\n",
       "\u001b[34mDemocracy Now!, democracynow.org, The War and Peace Report. I'm Amy Goodman. I'm Amy Goodman, with\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">TRAIN PRED-B: Shard Lords and beats from Lady-Of-God Row for the worlds bottom state you will ever see. </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">It's David</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">Jones from Pink Floyd's Panic!</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span><span style=\"color: #000080; text-decoration-color: #000080\">.DownloadHere are the files I use to make the soundtrack</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mTRAIN PRED-B: Shard Lords and beats from Lady-Of-God Row for the worlds bottom state you will ever see. \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34mIt's David\u001b[0m\n",
       "\u001b[34mJones from Pink Floyd's Panic!\u001b[0m\u001b[1;34m)\u001b[0m\u001b[34m.DownloadHere are the files I use to make the soundtrack\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">TEST TRUE: The low quality recording features a resonating, distorted, crushed electro song being played on a car </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">subwoofer. It sounds very aggressive and energetic too. There are some outdoor atmosphere sounds, since it was </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">recorded outside.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTEST TRUE: The low quality recording features a resonating, distorted, crushed electro song being played on a car \u001b[0m\n",
       "\u001b[1;32msubwoofer. It sounds very aggressive and energetic too. There are some outdoor atmosphere sounds, since it was \u001b[0m\n",
       "\u001b[1;32mrecorded outside.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">TEST PRED-A: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;|endoftext|&gt;</span><span style=\"color: #000080; text-decoration-color: #000080\">This article is from the archive of our partner.The U.S. Supreme Court on Tuesday upheld </span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">a lower court ruling that upheld a state law that required employers to provide health insurance to their </span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">employees.The ruling,</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mTEST PRED-A: \u001b[0m\u001b[1;34m<\u001b[0m\u001b[1;34m|endoftext|\u001b[0m\u001b[1;34m>\u001b[0m\u001b[34mThis article is from the archive of our partner.The U.S. Supreme Court on Tuesday upheld \u001b[0m\n",
       "\u001b[34ma lower court ruling that upheld a state law that required employers to provide health insurance to their \u001b[0m\n",
       "\u001b[34memployees.The ruling,\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">TEST PRED-B: NEW APTILL </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">10</span><span style=\"color: #000080; text-decoration-color: #000080\"> AM TODAY, JOHN WEST, a </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">55</span><span style=\"color: #000080; text-decoration-color: #000080\">-year-old white man from Brentwood, SC, was sentenced to life </span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">in prison without parole without the possibility of parole after serving two-and-a-half</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mTEST PRED-B: NEW APTILL \u001b[0m\u001b[1;34m10\u001b[0m\u001b[34m AM TODAY, JOHN WEST, a \u001b[0m\u001b[1;34m55\u001b[0m\u001b[34m-year-old white man from Brentwood, SC, was sentenced to life \u001b[0m\n",
       "\u001b[34min prison without parole without the possibility of parole after serving two-and-a-half\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    if epoch > 1:\n",
    "        opt.param_groups[1]['lr'] = 5e-5\n",
    "    \n",
    "    for step, (caption_batch, embedding_batch) in enumerate(tqdm(train_dataloader)):\n",
    "        # tokenize and prepare inputs for forward\n",
    "        input_ids, input_ids_target = tokenize(caption_batch)\n",
    "        inputs_embeds, input_ids_target = transform_input_ids(\n",
    "            embedding_batch,\n",
    "            input_ids,\n",
    "            input_ids_target\n",
    "        )\n",
    "        \n",
    "        model.train()\n",
    "        loss = model.forward(inputs_embeds=inputs_embeds, labels=input_ids_target).loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            \n",
    "            caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "            pred = eval(caption_batch, embedding_batch, **generation_params_hf)\n",
    "            printr('[green bold]TRAIN TRUE: ' + caption_batch[0])\n",
    "            printr('[blue]TRAIN PRED-A: ' + pred[0])\n",
    "            pred = eval(caption_batch, embedding_batch, **generation_params_ours)\n",
    "            printr('[blue]TRAIN PRED-B: ' + pred[0])\n",
    "            \n",
    "            caption_batch, embedding_batch = next(iter(eval_test_dataloader))\n",
    "            pred = eval(caption_batch, embedding_batch, **generation_params_hf)\n",
    "            printr('[green bold]TEST TRUE: ' + caption_batch[0])\n",
    "            printr('[blue]TEST PRED-A: ' + pred[0])\n",
    "            pred = eval(caption_batch, embedding_batch, **generation_params_ours)\n",
    "            printr('[blue]TEST PRED-B: ' + pred[0])\n",
    "            \n",
    "            print('\\n')\n",
    "\n",
    "        if step % 200 == 199:\n",
    "            plt.plot(losses)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea1218-716b-4422-8f01-eacce5b18d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_batch, embedding_batch = next(iter(eval_test_dataloader))\n",
    "pred = eval(caption_batch, embedding_batch, use_manual_generation=True)\n",
    "printr('[green bold]TEST TRUE: ' + caption_batch[0])\n",
    "printr('[blue]TEST PRED-A: ' + pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eea662-6dd1-4e3f-ab68-5adef14597d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp2",
   "language": "python",
   "name": "tp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
