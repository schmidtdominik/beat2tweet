{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd60a10-aba2-4f3e-b251-77d4f100179f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install-U yt-dlp==2023.1.6 matplotlib==3.6.0 datasets[audio] rich\n",
    "\n",
    "# install newest transformers build to be able to pass `inputs_embeds` through generate()\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f84841-b150-445f-8cf3-a9e5f6da2cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U torch torchaudio --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b32b1-a434-479e-8c62-92e8da10e672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c26e4-1d78-4d2e-a49c-a578ba0b6d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from musiccaps import load_musiccaps\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "\n",
    "from rich import print as printr\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import evaluate\n",
    "import itertools\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, './sota-music-tagging-models/training')\n",
    "import model as sota_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485da5-3e40-42f1-873e-ccb244a74bc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load musiccaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8926e06-ef8c-40a7-8675-f4d1285d0afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = load_musiccaps(\n",
    "    \"./music_data\",\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True,\n",
    ")\n",
    "\n",
    "music_files = {f.stem: f for f in Path('./music_npys/').iterdir()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15420704-a6bd-4315-b6b9-ba41fee54a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CaptionMusicDataset(Dataset):\n",
    "    \"\"\"Returns a torch Dataset of paired captions and music files\"\"\"\n",
    "\n",
    "    def __init__(self, muscaps_ds, music_files, chat_aug=False, aug_captions=None, preds_mode=False):\n",
    "        include_ytids = set(muscaps_ds['ytid']) & set(music_files.keys())\n",
    "        include_inds = [i for i, ytid in enumerate(muscaps_ds['ytid']) if ytid in include_ytids]\n",
    "        ds = muscaps_ds.select(include_inds)\n",
    "        assert len(ds) == len(music_files)\n",
    "\n",
    "        self.ytids_sorted = ds.sort(\"ytid\")[\"ytid\"]\n",
    "        self.captions = ds.sort(\"ytid\")[\"caption\"]\n",
    "        self.sorted_music_files = [music_files[ytid] for ytid in self.ytids_sorted]\n",
    "        self.chat_aug = chat_aug\n",
    "        if chat_aug:\n",
    "            self.aug_captions = [aug_captions[ytid] for ytid in self.ytids_sorted]\n",
    "            self.n_aug_captions = len(self.aug_captions[0])\n",
    "        self.preds_mode = preds_mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        music = np.load(self.sorted_music_files[idx], allow_pickle=True)\n",
    "        caption = self.captions[idx]\n",
    "        if self.chat_aug:\n",
    "            if torch.rand(1).item() < 1 - 1./self.n_aug_captions: \n",
    "                caption = np.random.choice(self.aug_captions[idx])\n",
    "                \n",
    "        music = np.stack([music[:80000], music[-80000:]])\n",
    "        \n",
    "        if self.preds_mode:\n",
    "            return caption, music, self.ytids_sorted[idx]\n",
    "\n",
    "        return caption, music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c18681-2a4f-41aa-b0ec-8d77a6880cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_chat_aug = True\n",
    "\n",
    "with open('chataug.json', 'r') as fp:\n",
    "    chataug_captions = json.load(fp)\n",
    "    \n",
    "with open('musiccaps_split.json', 'r') as fp:\n",
    "    musiccaps_split = json.load(fp)\n",
    "\n",
    "train_ytids, valid_ytids, test_ytids = musiccaps_split['train'], musiccaps_split['valid'], musiccaps_split['test']\n",
    "\n",
    "train_ds = ds.filter(lambda x: x['ytid'] in train_ytids)\n",
    "valid_ds = ds.filter(lambda x: x['ytid'] in valid_ytids)\n",
    "test_ds = ds.filter(lambda x: x['ytid'] in test_ytids)\n",
    "\n",
    "train_music_files = {ytid: e for ytid, e in music_files.items() if ytid in train_ytids}\n",
    "valid_music_files = {ytid: e for ytid, e in music_files.items() if ytid in valid_ytids}\n",
    "test_music_files = {ytid: e for ytid, e in music_files.items() if ytid in test_ytids}\n",
    "\n",
    "training_data = CaptionMusicDataset(muscaps_ds=train_ds, music_files=train_music_files, \n",
    "                                    chat_aug=use_chat_aug, aug_captions=chataug_captions)\n",
    "valid_data = CaptionMusicDataset(muscaps_ds=valid_ds, music_files=valid_music_files)\n",
    "test_data = CaptionMusicDataset(muscaps_ds=test_ds, music_files=test_music_files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0b38f-9628-4cdb-a519-ecbfd4f6cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3d9b-6745-43f7-91b4-1860ca8ae81b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82925589-cbb3-4f35-9099-3eb29831a29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.fc(torch.nn.functional.relu(x))\n",
    "\n",
    "class B2T(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(B2T, self).__init__()\n",
    "        self.hcnn = sota_model.HarmonicCNNCropped().to(device)\n",
    "        state_dict = torch.load(f'sota-music-tagging-models/models/jamendo/hcnn/best_model.pth',\n",
    "                        map_location=device)\n",
    "        self.hcnn.load_state_dict(state_dict, strict=False)\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(256, 768),\n",
    "            nn.Dropout(0.6),\n",
    "            ResidualLinear(768),\n",
    "            nn.Dropout(0.4),\n",
    "            ResidualLinear(768),\n",
    "        )\n",
    "        \n",
    "    def forward(self, audio_array):\n",
    "        audio_features = (self.hcnn(audio_array[:, 0, :])+self.hcnn(audio_array[:, 1, :]))/2\n",
    "        audio_embedding = self.fc_net(audio_features)\n",
    "    \n",
    "        return audio_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0d47c-b1e4-4837-bc6b-5f9b20d84ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(captions_batch):\n",
    "    input_ids = tokenizer(captions_batch)[\"input_ids\"]\n",
    "\n",
    "    # wrap in eos and add placeholder for music embedding/mask\n",
    "    input_ids = [torch.tensor([placeholder_id] + x + [eos_id]) for x in input_ids]\n",
    "    # pad with -100, this index is masked in the cross-entropy loss\n",
    "    input_ids_target = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=mask_id\n",
    "    ).to(device)\n",
    "\n",
    "    # index -100 isn't valid as model input however, since the token embedding lookup fails,\n",
    "    # so we need a second version as model input, with -100 replaced with another token (shouldn't matter which)\n",
    "    input_ids = input_ids_target.clone()\n",
    "    input_ids[input_ids == mask_id] = eos_id\n",
    "\n",
    "    return input_ids, input_ids_target\n",
    "\n",
    "\n",
    "def transform_input_ids(music_array, input_ids, input_ids_target):\n",
    "    music_emb_ind = 0  # 1 if using <bos>, otherwise 0\n",
    "    assert (input_ids[:, music_emb_ind] == placeholder_id).all()\n",
    "    assert (input_ids_target[:, music_emb_ind] == placeholder_id).all()\n",
    "\n",
    "    input_ids_target[:, music_emb_ind] = eos_id  # mask_id\n",
    "    input_ids[:, music_emb_ind] = eos_id  # temp placeholder to make the embedding lookup work\n",
    "    inputs_embeds = model.transformer.wte(input_ids)\n",
    "    inputs_embeds[:, music_emb_ind] = b2t(music_array.cuda())  # insert music embedding\n",
    "\n",
    "    return inputs_embeds, input_ids_target\n",
    "\n",
    "\n",
    "def strip_eos(pred):\n",
    "    \"\"\" \n",
    "    remove eos tokens from predicted captions \n",
    "    discards everything after the first <eos> that isn't the very first token\n",
    "    (the hf can only skip eos but not stop at eos) \n",
    "    \"\"\"\n",
    "    pred = [p.removeprefix(\"<|endoftext|>\") for p in pred]\n",
    "    pred = [p[: p.find(\"<|endoftext|>\")] if \"<|endoftext|>\" in p else p for p in pred]\n",
    "    return pred\n",
    "\n",
    "\n",
    "def eval(caption_batch, audio_batch, rm_eos=False, **kwargs):\n",
    "    model.eval()\n",
    "    b2t.eval()\n",
    "    input_ids, input_ids_target = tokenize(caption_batch)\n",
    "    inputs_embeds, _ = transform_input_ids(audio_batch, input_ids, input_ids_target)\n",
    "\n",
    "    # only include <bos> (optional) and music_embedding, don't include true caption\n",
    "    inputs_embeds = inputs_embeds[:, :1]\n",
    "    output_ids = model.generate(inputs_embeds=inputs_embeds, **kwargs)\n",
    "    pred = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "    pred = [p.replace(\"\\n\", \"\").strip() for p in pred]\n",
    "    return strip_eos(pred) if rm_eos else pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbbd8e-d913-4059-b28a-ee995e78de74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_step(inputs_embeds, input_ids_target, apply_grad):\n",
    "    model.train()\n",
    "    b2t.train()\n",
    "    loss = model.forward(inputs_embeds=inputs_embeds, labels=input_ids_target).loss\n",
    "    loss.backward()\n",
    "    \n",
    "    if apply_grad:\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def eval_step(string_info=\"\"):\n",
    "    \n",
    "    caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "    pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "    printr('[green bold]TRAIN TRUE: ' + caption_batch[0])\n",
    "    printr('[yellow]TRAIN PRED: ' + pred[0])\n",
    "    wlog('TRAIN TRUE: ' + caption_batch[0])\n",
    "    wlog('TRAIN PRED: ' + pred[0])\n",
    "    caption_batch, embedding_batch = next(iter(eval_valid_dataloader))\n",
    "    pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "    printr('[green bold]TEST TRUE: ' + caption_batch[0])\n",
    "    printr('[yellow]TEST PRED: ' + pred[0])\n",
    "    wlog('TEST TRUE: ' + caption_batch[0])\n",
    "    wlog('TEST PRED: ' + pred[0])\n",
    "    print()\n",
    "    \n",
    "def metrics_step(n=100, shuffles=10):\n",
    "    \n",
    "    lq = 0\n",
    "    \n",
    "    print(f\"Computing metrics for n={n}\")\n",
    "    wlog(f\"Computing metrics for n={n}\")\n",
    "    \n",
    "    shuffled_meteor_valid, shuffled_bleu_valid = 0., 0.\n",
    "    train_captions, train_preds = [], []\n",
    "    valid_captions, valid_preds = [], []\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "\n",
    "        caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        train_captions.append(caption_batch[0])\n",
    "        train_preds.append(pred[0])\n",
    "\n",
    "        caption_batch, embedding_batch = next(iter(eval_valid_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        valid_captions.append(caption_batch[0])\n",
    "        valid_preds.append(pred[0])\n",
    "        if \"The low quality recording\" in pred[0]:\n",
    "            lq += 1\n",
    "\n",
    "    meteor_train = meteor.compute(predictions=train_preds, references=train_captions)['meteor']\n",
    "    bleu_train = google_bleu.compute(predictions=train_preds, references=train_captions)['google_bleu']\n",
    "\n",
    "    meteor_valid = meteor.compute(predictions=valid_preds, references=valid_captions)['meteor']\n",
    "    bleu_valid = google_bleu.compute(predictions=valid_preds, references=valid_captions)['google_bleu']\n",
    "    \n",
    "    for j in range(shuffles):\n",
    "        shuffled_captions = sorted(valid_captions, key=lambda k: random.random())\n",
    "        shuffled_meteor_valid += (1./(shuffles))*meteor.compute(predictions=valid_preds, references=shuffled_captions)['meteor']\n",
    "        shuffled_bleu_valid += (1./(shuffles))*google_bleu.compute(predictions=valid_preds, references=shuffled_captions)['google_bleu']\n",
    "\n",
    "    spec_meteor = meteor_valid - shuffled_meteor_valid\n",
    "    spec_bleu = bleu_valid - shuffled_bleu_valid\n",
    "        \n",
    "    print(f\"Train meteor: {meteor_train:.4f}, Train bleu: {bleu_train:.4f}\")\n",
    "    wlog(f\"Train meteor: {meteor_train:.4f}, Train bleu: {bleu_train:.4f}\")\n",
    "    print(f\"Valid meteor: {meteor_valid:.4f}, Valid bleu: {bleu_valid:.4f}\")\n",
    "    wlog(f\"Valid meteor: {meteor_valid:.4f}, Valid bleu: {bleu_valid:.4f}\")\n",
    "    print(f\"Valid spec-meteor: {spec_meteor:.4f}, Valid spec-bleu: {spec_bleu:.4f}\")\n",
    "    wlog(f\"Valid spec-meteor: {spec_meteor:.4f}, Valid spec-bleu: {spec_bleu:.4f}\")\n",
    "    print(f\"Low quality recording count: {lq}\")\n",
    "    wlog(f\"Low quality recording count: {lq}\")\n",
    "    \n",
    "    return {'meteor_train': meteor_train,\n",
    "            'meteor_valid': meteor_valid,\n",
    "            'bleu_train': bleu_train,\n",
    "            'bleu_valid': bleu_valid,\n",
    "            'spec_meteor': spec_meteor,\n",
    "            'spec_bleu': spec_bleu} \n",
    "\n",
    "m_styles = {\"meteor_train\": (\"tab:orange\", \"solid\"),\n",
    "    \"meteor_valid\": (\"tab:orange\", \"dashed\"),\n",
    "    \"bleu_train\": (\"tab:blue\", \"solid\"),\n",
    "    \"bleu_valid\": (\"tab:blue\", \"dashed\"),\n",
    "    \"spec_bleu\": (\"tab:blue\", \"dashdot\"),\n",
    "    \"spec_meteor\": (\"tab:orange\", \"dashdot\")\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aad078-36f3-4a47-9e2e-9c02371a52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def metrics_step(n=100, shuffles=10):\n",
    "    \n",
    "    lq = 0\n",
    "    \n",
    "    print(f\"Computing metrics for n={n}\")\n",
    "    wlog(f\"Computing metrics for n={n}\")\n",
    "    \n",
    "    shuffled_meteor_valid, shuffled_bleu_valid = 0., 0.\n",
    "    train_captions, train_preds = [], []\n",
    "    valid_captions, valid_preds = [], []\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "\n",
    "        caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        train_captions.append(re.sub(r'[^\\w\\s]','',caption_batch[0]))\n",
    "        train_preds.append(re.sub(r'[^\\w\\s]','',pred[0]))\n",
    "\n",
    "        caption_batch, embedding_batch = next(iter(eval_valid_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        valid_captions.append(re.sub(r'[^\\w\\s]','',caption_batch[0]))\n",
    "        valid_preds.append(re.sub(r'[^\\w\\s]','',pred[0]))\n",
    "        if \"The low quality recording\" in pred[0]:\n",
    "            lq += 1\n",
    "\n",
    "    meteor_train = meteor.compute(predictions=train_preds, references=train_captions)['meteor']\n",
    "    bleu_train = google_bleu.compute(predictions=train_preds, references=train_captions)['google_bleu']\n",
    "\n",
    "    meteor_valid = meteor.compute(predictions=valid_preds, references=valid_captions)['meteor']\n",
    "    bleu_valid = google_bleu.compute(predictions=valid_preds, references=valid_captions)['google_bleu']\n",
    "    \n",
    "    for j in range(shuffles):\n",
    "        shuffled_captions = sorted(valid_captions, key=lambda k: random.random())\n",
    "        shuffled_meteor_valid += (1./(shuffles))*meteor.compute(predictions=valid_preds, references=shuffled_captions)['meteor']\n",
    "        shuffled_bleu_valid += (1./(shuffles))*google_bleu.compute(predictions=valid_preds, references=shuffled_captions)['google_bleu']\n",
    "\n",
    "    spec_meteor = meteor_valid - shuffled_meteor_valid\n",
    "    spec_bleu = bleu_valid - shuffled_bleu_valid\n",
    "        \n",
    "    print(f\"Train meteor: {meteor_train:.4f}, Train bleu: {bleu_train:.4f}\")\n",
    "    wlog(f\"Train meteor: {meteor_train:.4f}, Train bleu: {bleu_train:.4f}\")\n",
    "    print(f\"Valid meteor: {meteor_valid:.4f}, Valid bleu: {bleu_valid:.4f}\")\n",
    "    wlog(f\"Valid meteor: {meteor_valid:.4f}, Valid bleu: {bleu_valid:.4f}\")\n",
    "    print(f\"Valid spec-meteor: {spec_meteor:.4f}, Valid spec-bleu: {spec_bleu:.4f}\")\n",
    "    wlog(f\"Valid spec-meteor: {spec_meteor:.4f}, Valid spec-bleu: {spec_bleu:.4f}\")\n",
    "    print(f\"Low quality recording count: {lq}\")\n",
    "    wlog(f\"Low quality recording count: {lq}\")\n",
    "    \n",
    "    return {'meteor_train': meteor_train,\n",
    "            'meteor_valid': meteor_valid,\n",
    "            'bleu_train': bleu_train,\n",
    "            'bleu_valid': bleu_valid,\n",
    "            'spec_meteor': spec_meteor,\n",
    "            'spec_bleu': spec_bleu} \n",
    "\n",
    "m_styles = {\"meteor_train\": (\"tab:orange\", \"solid\"),\n",
    "    \"meteor_valid\": (\"tab:orange\", \"dashed\"),\n",
    "    \"bleu_train\": (\"tab:blue\", \"solid\"),\n",
    "    \"bleu_valid\": (\"tab:blue\", \"dashed\"),\n",
    "    \"spec_bleu\": (\"tab:blue\", \"dashdot\"),\n",
    "    \"spec_meteor\": (\"tab:orange\", \"dashdot\")\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6b337-a156-4125-80fe-27229603dd46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'gpt2' # gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.config.is_decoder = True # not sure if necessary\n",
    "\n",
    "mask_id = -100 # don't change, this is fixed in torch cross-entropy loss!\n",
    "eos_id = tokenizer.eos_token_id\n",
    "placeholder_id = -200\n",
    "b2t = B2T().cuda()\n",
    "hcnn_lr = 5e-5\n",
    "b2t_lr = 1e-4\n",
    "gpt2_finetune_lr = 5e-5\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "    # b2t needs to be the first parameter group\n",
    "    {'params': b2t.hcnn.layer4.parameters(), 'lr': hcnn_lr},\n",
    "    {'params': b2t.hcnn.layer5.parameters(), 'lr': hcnn_lr},\n",
    "    {'params': b2t.hcnn.layer6.parameters(), 'lr': hcnn_lr},\n",
    "    {'params': b2t.hcnn.layer7.parameters(), 'lr': hcnn_lr},\n",
    "    {'params': b2t.fc_net.parameters(), 'lr': b2t_lr},\n",
    "    \n",
    "    # disable AdamW weight decay for gpt2 layer finetuning\n",
    "    {'params': model.transformer.h[1].parameters(), 'lr': gpt2_finetune_lr, 'weight_decay': 0},\n",
    "    {'params': model.transformer.h[2].parameters(), 'lr': gpt2_finetune_lr, 'weight_decay': 0}\n",
    "])\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 500\n",
    "epoch = 207\n",
    "gradient_acc_fact = 1\n",
    "gpt2_finetune_start_epoch = 0\n",
    "\n",
    "generation_params = dict(\n",
    "    max_new_tokens=64,\n",
    "    num_beams=8,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    bos_token_id=eos_id,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=mask_id,\n",
    "    early_stopping=True,\n",
    "    rm_eos=True,\n",
    ")\n",
    "\n",
    "losses = []\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "eval_train_dataloader = DataLoader(training_data, 1, shuffle=True)\n",
    "eval_valid_dataloader = DataLoader(valid_data, 1, shuffle=True)\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load(\"google_bleu\")\n",
    "metrics = {'step': [], 'bleu_train': [], 'bleu_valid': [], 'meteor_train': [], 'meteor_valid': [],\n",
    "          'spec_bleu': [], 'spec_meteor':[]}\n",
    "\n",
    "checkpoint_dir = Path('checkpoints')\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "load_pretrain = True\n",
    "keep_training = True\n",
    "\n",
    "pretrain_model =  \"checkpoints/chkp_trainenchook_5e-05_5e-05_0.0001_chataug.pt\"\n",
    "if load_pretrain:\n",
    "    data_dict = torch.load(pretrain_model)\n",
    "    model.load_state_dict(data_dict['model'])\n",
    "    b2t.load_state_dict(data_dict['b2t'])\n",
    "    \n",
    "model_name_info = \"small\" if model_name == 'gpt2' else model_name\n",
    "pretraining_info = \"yes\" if load_pretrain and not keep_training else \"no\"\n",
    "chat_aug_info = \"_chataug\" if use_chat_aug else \"\"\n",
    "string_info = f\"trainenchook_{hcnn_lr}_{gpt2_finetune_lr}_{b2t_lr}{chat_aug_info}\"\n",
    "\n",
    "def wlog(s):\n",
    "    f = open(f\"outputs/logs_{string_info}.txt\",'a')\n",
    "    f.write(s+\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "if keep_training:\n",
    "    with open(f'outputs/train_metrics_{string_info}.npy', 'rb') as f:\n",
    "        train_metrics = np.load(f, allow_pickle=True).item()\n",
    "    \n",
    "    losses = list(train_metrics['loss'])\n",
    "    metrics = {k: list(v) for k,v in train_metrics.items()}\n",
    "\n",
    "string_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb04ec-e6a7-4afd-afaa-c1e15df0ed5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epoch, num_epochs)):\n",
    "    wlog(f\"\\nEpoch {epoch}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(), \n",
    "        \"b2t\": b2t.state_dict(), \n",
    "        \"opt\": opt\n",
    "    }, checkpoint_dir / f\"chkp_{string_info}.pt\")\n",
    "    #}, checkpoint_dir / f\"chkp_{epoch}.pt\")\n",
    "    \n",
    "    if epoch>0 and epoch % 20 == 0:\n",
    "        print(\"Checkpoint saved\")\n",
    "        wlog(\"Checkpoint saved\")\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(), \n",
    "            \"b2t\": b2t.state_dict(), \n",
    "            \"opt\": opt\n",
    "        }, checkpoint_dir / f\"chkp_{string_info}_e{epoch}.pt\")\n",
    "\n",
    "    for step, (caption_batch, embedding_batch) in enumerate(tqdm(train_dataloader)):\n",
    "        # tokenize and prepare inputs for forward\n",
    "        input_ids, input_ids_target = tokenize(list(caption_batch))\n",
    "        inputs_embeds, input_ids_target = transform_input_ids(\n",
    "            embedding_batch, input_ids, input_ids_target\n",
    "        )\n",
    "\n",
    "        apply_grad_cond = step % gradient_acc_fact == 0\n",
    "        losses.append(\n",
    "            update_step(inputs_embeds, input_ids_target, apply_grad=apply_grad_cond)\n",
    "        )\n",
    "\n",
    "        if epoch % 5 == 0 and step % 500 == 0:\n",
    "            wlog(f\"Loss {np.mean(losses[-500:])}\\n\")\n",
    "            eval_step(string_info=string_info)\n",
    "                \n",
    "            plt.plot(losses, label='train_loss')\n",
    "            plt.savefig(f\"plots/plot_loss_{string_info}.png\")\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "            \n",
    "            if epoch % 10 == 0 and epoch>0:\n",
    "                \n",
    "                metrics_results = metrics_step(n=100)\n",
    "                metrics['step'].append(len(losses))\n",
    "                for m in ['meteor_train', 'meteor_valid', 'bleu_train', 'bleu_valid', 'spec_bleu', 'spec_meteor']:\n",
    "                    metrics[m].append(metrics_results[m])\n",
    "                    plt.plot(metrics['step'], metrics[m], label=m, linestyle=m_styles[m][1], color=m_styles[m][0])\n",
    "                plt.legend()\n",
    "                plt.savefig(f\"plots/plot_metrics_{string_info}.png\")\n",
    "                plt.show()\n",
    "\n",
    "                with open(f'outputs/train_metrics_{string_info}.npy', 'wb') as f:\n",
    "                    metrics_to_save = {k: np.array(a) for k, a in metrics.items()}\n",
    "                    metrics_to_save['loss'] = np.array(losses)\n",
    "                    np.save(f, metrics_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c89913-a442-40e1-b3a6-e6aa92219eb8",
   "metadata": {},
   "source": [
    "# Generate eval captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98828e0-da74-4608-9528-493d1c462cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load('google_bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac29b8-10de-482d-910b-abc93d4d97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_true_captions = []\n",
    "eval_pred_captions = []\n",
    "eval_ytids = []\n",
    "\n",
    "model_path = \"saved-models/best-enc-noaug.pt\"\n",
    "data_dict = torch.load(model_path)\n",
    "model.load_state_dict(data_dict['model'])\n",
    "b2t.load_state_dict(data_dict['b2t'])\n",
    "\n",
    "test_results_data = CaptionMusicDataset(muscaps_ds=test_ds, music_files=test_music_files, preds_mode=True) \n",
    "eval_test_dataloader = DataLoader(test_results_data, 1, shuffle=False)\n",
    "\n",
    "generation_params = dict(\n",
    "    max_new_tokens=200,\n",
    "    num_beams=8,\n",
    "    do_sample=True,\n",
    "    temperature=1.05,\n",
    "    bos_token_id=eos_id,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=mask_id,\n",
    "    early_stopping=True,\n",
    "    rm_eos=True,\n",
    ")\n",
    "\n",
    "for i, (caption_batch, embedding_batch, ytid_batch) in tqdm(enumerate(eval_test_dataloader)):\n",
    "    pred = eval(list(caption_batch), embedding_batch, **generation_params)\n",
    "    eval_true_captions.append(caption_batch[0])\n",
    "    eval_pred_captions.append(pred[0])\n",
    "    eval_ytids.append(ytid_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9b85a-ec12-41da-b711-a6e1935e887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_eval_true_captions = [re.sub(r'[^\\w\\s]','',x).lower() for x in eval_true_captions]\n",
    "lower_eval_pred_captions = [re.sub(r'[^\\w\\s]','',x).lower() for x in eval_pred_captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1136d-7fc5-4e1d-a58d-4f009e5cbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu_score = google_bleu.compute(predictions=lower_eval_pred_captions, \n",
    "                                 references=lower_eval_true_captions)['google_bleu']\n",
    "meteor_score = meteor.compute(predictions=lower_eval_pred_captions, \n",
    "                              references=lower_eval_true_captions)['meteor']\n",
    "print(gleu_score, meteor_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a1e31-dbef-43aa-b367-351241340ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu_score = google_bleu.compute(predictions=lower_eval_pred_captions, \n",
    "                                 references=lower_eval_true_captions)['google_bleu']\n",
    "meteor_score = meteor.compute(predictions=lower_eval_pred_captions, \n",
    "                              references=lower_eval_true_captions)['meteor']\n",
    "print(gleu_score, meteor_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138ee71-172a-4eff-8c93-cdfd42873698",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dict(\n",
    "    eval_true_captions=eval_true_captions,\n",
    "    eval_pred_captions=eval_pred_captions,\n",
    "    tracks_ids = eval_ytids\n",
    "), open('outputs/preds_gpt2_enc_noaug.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f72be-d4e1-4c4b-a34b-39b32558c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_true_captions = []\n",
    "eval_pred_captions = []\n",
    "eval_ytids = []\n",
    "\n",
    "model_path = \"saved-models/best-enc-noaug.pt\"\n",
    "data_dict = torch.load(model_path)\n",
    "model.load_state_dict(data_dict['model'])\n",
    "b2t.load_state_dict(data_dict['b2t'])\n",
    "\n",
    "test_results_data = CaptionMusicDataset(muscaps_ds=test_ds, music_files=test_music_files, preds_mode=True) \n",
    "eval_test_dataloader = DataLoader(test_results_data, 1, shuffle=False)\n",
    "\n",
    "generation_params = dict(\n",
    "    max_new_tokens=200,\n",
    "    num_beams=8,\n",
    "    do_sample=True,\n",
    "    temperature=1.1,\n",
    "    bos_token_id=eos_id,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=mask_id,\n",
    "    early_stopping=True,\n",
    "    rm_eos=True,\n",
    ")\n",
    "\n",
    "for i, (caption_batch, embedding_batch, ytid_batch) in tqdm(enumerate(eval_test_dataloader)):\n",
    "    pred = eval(list(caption_batch), embedding_batch, **generation_params)\n",
    "    eval_true_captions.append(caption_batch[0])\n",
    "    eval_pred_captions.append(pred[0])\n",
    "    eval_ytids.append(ytid_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34861432-30ac-458a-85ba-fce0e06e671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_eval_true_captions = [re.sub(r'[^\\w\\s]','',x).lower() for x in eval_true_captions]\n",
    "lower_eval_pred_captions = [re.sub(r'[^\\w\\s]','',x).lower() for x in eval_pred_captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26dc38d-fcbd-4bfc-a8da-43eb3610ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu_score = google_bleu.compute(predictions=lower_eval_pred_captions, \n",
    "                                 references=lower_eval_true_captions)['google_bleu']\n",
    "meteor_score = meteor.compute(predictions=lower_eval_pred_captions, \n",
    "                              references=lower_eval_true_captions)['meteor']\n",
    "print(gleu_score, meteor_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938b3f7-b9fb-47f5-a1f7-17ccfc832c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
