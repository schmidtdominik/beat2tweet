{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U yt-dlp==2023.1.6 matplotlib==3.6.0 datasets[audio]\n",
    "# !pip install transformers\n",
    "# !pip install rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiccaps import load_musiccaps\n",
    "import numpy as np\n",
    "from rich import print as printr\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the dataset for the ones we have embeddings for. These are the harmonic CNN embeddings from feature_extraction.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_muscaps_with_embeddings(ds, embeddings):\n",
    "    '''Some clips weren't downloaded so we couldn't embed them, get rid of that'''\n",
    "    exclude_ids = set()\n",
    "    for i in range(len(ds)):\n",
    "        if ds[i]['ytid'] not in embeddings.keys():\n",
    "            exclude_ids.add(i)\n",
    "    ds = ds.select(\n",
    "        (\n",
    "            i for i in range(len(ds)) \n",
    "            if i not in set(exclude_ids)\n",
    "        )\n",
    "    )\n",
    "    assert len(ds) == len(embeddings)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration google--MusicCaps-7925612b943f961b\n",
      "Found cached dataset csv (/Users/alexandrasouly/.cache/huggingface/datasets/google___csv/google--MusicCaps-7925612b943f961b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    }
   ],
   "source": [
    "ds = load_musiccaps(\n",
    "    './music_data',\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True\n",
    ")\n",
    "embeddings = np.load('embeddings.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a pytorch Dataset that yields captions and embeddings. This will make it easier to switch embeddings when we want to, create train-test splits and batch with Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionEmbedding(Dataset):\n",
    "    '''Returns a torch Dataset of paired captions and embeddings'''\n",
    "    def __init__(self, muscaps_ds, embeddings):\n",
    "        ds = filter_muscaps_with_embeddings(muscaps_ds, embeddings)\n",
    "        self.captions = ds.sort(column='ytid')['caption']\n",
    "        sorted_embs = [ value for _, value in sorted(embeddings.items())]\n",
    "        self.embeddings = torch.from_numpy(np.stack(sorted_embs)).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.captions[idx], self.embeddings[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class B2T(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(512, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object filter_muscaps_with_embeddings.<locals>.<genexpr> at 0x169640c80> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "dataset = CaptionEmbedding(muscaps_ds=ds, embeddings=embeddings)\n",
    "# quick check did not mess up ordering of caption-embedding pairs\n",
    "# for cap, emb in dataset:\n",
    "#     for i in range(len(ds)):\n",
    "#         if cap == ds[i]['caption']:\n",
    "#             assert torch.allclose(emb,torch.from_numpy(embeddings[ds[i]['ytid']]))\n",
    "        \n",
    "\n",
    "train_size = math.floor(0.8*len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "training_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "batch_size = 3\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrasouly/miniconda3/envs/beat_to_tweet/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "b2t = B2T()\n",
    "\n",
    "b2t.to(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# either update the b2t layers only, or the whole model depending on which opt you\n",
    "# uncomment\n",
    "\n",
    "# opt = torch.optim.Adam([*b2t.parameters()], lr=0.0001) # , *model.decoder.parameters()]\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': b2t.parameters(), 'lr': 0.0001},\n",
    "    {'params': model.parameters(), 'lr': 0.000001}\n",
    "])\n",
    "\n",
    "losses = []\n",
    "fake_pixel_values = torch.zeros((batch_size, 3, 224, 224)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_forward = model.encoder.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patched_forward(*args, **kwargs):\n",
    "    result = encoder_forward(*args, **kwargs) # this is just to appease the HuggingFace gods\n",
    "    result.last_hidden_state = b2t(EMBS).repeat(1, 197, 1) # overwrite with actual embedding we use\n",
    "    return result\n",
    "\n",
    "# the original model uses a vision transformer in the encoder forward, so we get rid of that \n",
    "# and use the embeddings we have for the music\n",
    "\n",
    "model.encoder.forward = patched_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9665486d1345579964187bdc34fdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc2b5b1448e442cbd1c226886f69c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> PREDICTION1: a person standing next to a table with a bunch of food</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m PREDICTION1: a person standing next to a table with a bunch of food\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> PREDICTION2: a large group of people standing around a table</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m PREDICTION2: a large group of people standing around a table\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> TRUE CAPTION: The low quality recording features a flat male vocal talking over an acoustic guitar solo melody </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">playing in the background. It sounds like a tutorial and the recording is in mono.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m TRUE CAPTION: The low quality recording features a flat male vocal talking over an acoustic guitar solo melody \u001b[0m\n",
       "\u001b[1;32mplaying in the background. It sounds like a tutorial and the recording is in mono.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> PREDICTION1: a guitar player singing a song on a microphone</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m PREDICTION1: a guitar player singing a song on a microphone\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> PREDICTION2: a guitar player singing a song with a microphone</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m PREDICTION2: a guitar player singing a song with a microphone\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> TRUE CAPTION: This song contains digital drums playing a ragga groove with a light pitch shifting sub bass on the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">kick and snare. A soft synth pad is creating an atmosphere. A deep male voice is singing/rapping over the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">instrumental. This song may be playing in an urban club.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m TRUE CAPTION: This song contains digital drums playing a ragga groove with a light pitch shifting sub bass on the \u001b[0m\n",
       "\u001b[1;32mkick and snare. A soft synth pad is creating an atmosphere. A deep male voice is singing/rapping over the \u001b[0m\n",
       "\u001b[1;32minstrumental. This song may be playing in an urban club.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> PREDICTION1: a music video is playing on the wii</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m PREDICTION1: a music video is playing on the wii\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> PREDICTION2: a music video is being played on a guitar</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m PREDICTION2: a music video is being played on a guitar\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> TRUE CAPTION: This reggae song features male voices singing the main melody in harmony. The clip starts off with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">the sound of a chalk writing on a board. This is followed by a percussion roll. The voices start singing the main </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">melody. This is accompanied by percussion playing a reggae beat. The bass plays a reggae style bassline. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">keyboard plays staccato chords in a reggae rhythm. The mood of this song is happy. This song can be played in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">birthday scene in a movie when a family is on a holiday on an island.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m TRUE CAPTION: This reggae song features male voices singing the main melody in harmony. The clip starts off with \u001b[0m\n",
       "\u001b[1;32mthe sound of a chalk writing on a board. This is followed by a percussion roll. The voices start singing the main \u001b[0m\n",
       "\u001b[1;32mmelody. This is accompanied by percussion playing a reggae beat. The bass plays a reggae style bassline. The \u001b[0m\n",
       "\u001b[1;32mkeyboard plays staccato chords in a reggae rhythm. The mood of this song is happy. This song can be played in a \u001b[0m\n",
       "\u001b[1;32mbirthday scene in a movie when a family is on a holiday on an island.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m captions_tok \u001b[39m=\u001b[39m tokenizer(captions, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlongest\u001b[39m\u001b[39m'\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m loss \u001b[39m=\u001b[39m model(fake_pixel_values, labels\u001b[39m=\u001b[39mcaptions_tok)\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> 10\u001b[0m opt\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m     11\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m opt\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/beat_to_tweet/lib/python3.9/site-packages/torch/optim/optimizer.py:279\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    277\u001b[0m     p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m foreach \u001b[39mor\u001b[39;00m p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mis_sparse):\n\u001b[0;32m--> 279\u001b[0m     p\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mzero_()\n\u001b[1;32m    280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     per_device_and_dtype_grads[p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdevice][p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdtype]\u001b[39m.\u001b[39mappend(p\u001b[39m.\u001b[39mgrad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for step in tqdm(range(len(train_dataloader))):\n",
    "\n",
    "\n",
    "        captions, EMBS = next(iter(train_dataloader)) # patched forward is using this EMBS\n",
    "        captions_tok = tokenizer(captions, padding='longest', return_tensors='pt')['input_ids'].to(device)\n",
    "        loss = model(fake_pixel_values, labels=captions_tok).loss\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % 20 == 0:\n",
    "            EMBS = EMBS[0:1]\n",
    "            fake_eval_pixel_values = torch.zeros((1,3, 224, 224)).to(device)\n",
    "            output_ids = model.generate(fake_eval_pixel_values, max_length=128, num_beams=2)\n",
    "            printr('[blue bold] PREDICTION1: ' + tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip())\n",
    "            output_ids = model.generate(fake_eval_pixel_values, max_length=128, num_beams=4, do_sample=True, temperature=0.8)\n",
    "            printr('[blue bold] PREDICTION2: ' + tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip())\n",
    "            printr('[green bold] TRUE CAPTION: ' + captions[0])\n",
    "            print()\n",
    "        \n",
    "\n",
    "        if step % 200 == 199:\n",
    "            plt.plot(losses)\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf6fc484f1bff5675a62812d70362f007097959da2f99b24a0abacdd47f25564"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
