{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa39aacd-f77d-4cc9-ba72-dcfee9789a02",
   "metadata": {},
   "source": [
    "**Relevant huggingface gpt2 code**\n",
    "\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py\n",
    "- https://github.com/huggingface/transformers/issues/6535\n",
    "- bos/eos discussion: https://github.com/huggingface/transformers/issues/3311"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc102bb-e79b-44e8-8abc-56d4e67ffd87",
   "metadata": {},
   "source": [
    "**Some options for our main model**\n",
    "\n",
    "- different gpt2 sizes\n",
    "- gpt2 self-att vs gpt-2 cross-att (image captioning)\n",
    "- which gpt2 layers to finetune?\n",
    "- first pretrain on labels, then captions? or at the same time with different prompt/`<bos>` token?\n",
    "- make b2t output a bunch of 768 dimensional vectors that gpt2 self-att attends to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd60a10-aba2-4f3e-b251-77d4f100179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-s7g2bso_\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-s7g2bso_\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit b90fbc7e0ba41dfd6b343e7e2274443f19087f36\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0) (23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0) (0.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0) (1.23.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.27.0.dev0) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.0.dev0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.27.0.dev0) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.0.dev0) (2.1.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.27.0.dev0-py3-none-any.whl size=6708062 sha256=4a0e5491300dfc96be1e139e020ee7a27f5e0ea84372cb8c6639d28bf03859a9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-94eje9dc/wheels/f7/92/8c/752ff3bfcd3439805d8bbf641614da38ef3226e127ebea86ee\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "Successfully installed transformers-4.27.0.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install -U yt-dlp==2023.1.6 matplotlib==3.6.0 datasets[audio] rich\n",
    "\n",
    "# install newest transformers build to be able to pass `inputs_embeds` through generate()\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f84841-b150-445f-8cf3-a9e5f6da2cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.13 in /usr/local/lib/python3.9/dist-packages (1.13.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch==1.13) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch==1.13) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch==1.13) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch==1.13) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13) (66.1.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13) (0.35.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3b32b1-a434-479e-8c62-92e8da10e672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.9/dist-packages (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.23.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.4.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.12.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.1.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (18.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f7c26e4-1d78-4d2e-a49c-a578ba0b6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiccaps import load_musiccaps\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "\n",
    "from rich import print as printr\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import evaluate\n",
    "import itertools\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "f = open('logs.txt','a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485da5-3e40-42f1-873e-ccb244a74bc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load musiccaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6684ae9-0e53-415d-8be8-a67f90f7030e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_muscaps_with_embeddings(ds, embeddings):\n",
    "    \"\"\"Some clips weren't downloaded so we couldn't embed them, get rid of that\"\"\"\n",
    "    exclude_ids = set()\n",
    "    for i in range(len(ds)):\n",
    "        if ds[i][\"ytid\"] not in embeddings.keys():\n",
    "            exclude_ids.add(i)\n",
    "    ds = ds.select((i for i in range(len(ds)) if i not in set(exclude_ids)))\n",
    "    assert len(ds) == len(embeddings)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5615f1a1-60e9-4c71-bccc-ed90674e3b96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.npy     embeddings_75.npy  tag_scores_51.npy  tags_26.json\n",
      "embeddings_00.npy  embeddings_76.npy  tag_scores_52.npy  tags_27.json\n",
      "embeddings_01.npy  embeddings_77.npy  tag_scores_53.npy  tags_28.json\n",
      "embeddings_02.npy  embeddings_78.npy  tag_scores_54.npy  tags_29.json\n",
      "embeddings_03.npy  embeddings_79.npy  tag_scores_55.npy  tags_30.json\n",
      "embeddings_04.npy  embeddings_80.npy  tag_scores_56.npy  tags_31.json\n",
      "embeddings_05.npy  embeddings_81.npy  tag_scores_57.npy  tags_32.json\n",
      "embeddings_06.npy  embeddings_82.npy  tag_scores_58.npy  tags_33.json\n",
      "embeddings_07.npy  embeddings_83.npy  tag_scores_59.npy  tags_34.json\n",
      "embeddings_08.npy  embeddings_84.npy  tag_scores_60.npy  tags_36.json\n",
      "embeddings_09.npy  embeddings_85.npy  tag_scores_61.npy  tags_37.json\n",
      "embeddings_10.npy  embeddings_86.npy  tag_scores_62.npy  tags_38.json\n",
      "embeddings_11.npy  embeddings_87.npy  tag_scores_63.npy  tags_39.json\n",
      "embeddings_12.npy  embeddings_88.npy  tag_scores_64.npy  tags_40.json\n",
      "embeddings_13.npy  embeddings_89.npy  tag_scores_65.npy  tags_41.json\n",
      "embeddings_14.npy  embeddings_90.npy  tag_scores_66.npy  tags_42.json\n",
      "embeddings_15.npy  embeddings_91.npy  tag_scores_67.npy  tags_43.json\n",
      "embeddings_16.npy  embeddings_92.npy  tag_scores_68.npy  tags_44.json\n",
      "embeddings_17.npy  embeddings_93.npy  tag_scores_69.npy  tags_45.json\n",
      "embeddings_18.npy  embeddings_94.npy  tag_scores_70.npy  tags_46.json\n",
      "embeddings_19.npy  embeddings_95.npy  tag_scores_71.npy  tags_47.json\n",
      "embeddings_20.npy  embeddings_96.npy  tag_scores_72.npy  tags_48.json\n",
      "embeddings_21.npy  embeddings_97.npy  tag_scores_73.npy  tags_49.json\n",
      "embeddings_22.npy  embeddings_98.npy  tag_scores_74.npy  tags_50.json\n",
      "embeddings_23.npy  embeddings_99.npy  tag_scores_75.npy  tags_51.json\n",
      "embeddings_24.npy  tag_scores_00.npy  tag_scores_76.npy  tags_52.json\n",
      "embeddings_25.npy  tag_scores_01.npy  tag_scores_77.npy  tags_53.json\n",
      "embeddings_26.npy  tag_scores_02.npy  tag_scores_78.npy  tags_54.json\n",
      "embeddings_27.npy  tag_scores_03.npy  tag_scores_79.npy  tags_55.json\n",
      "embeddings_28.npy  tag_scores_04.npy  tag_scores_80.npy  tags_56.json\n",
      "embeddings_29.npy  tag_scores_05.npy  tag_scores_81.npy  tags_57.json\n",
      "embeddings_30.npy  tag_scores_06.npy  tag_scores_82.npy  tags_58.json\n",
      "embeddings_31.npy  tag_scores_07.npy  tag_scores_83.npy  tags_59.json\n",
      "embeddings_32.npy  tag_scores_08.npy  tag_scores_84.npy  tags_60.json\n",
      "embeddings_33.npy  tag_scores_09.npy  tag_scores_85.npy  tags_61.json\n",
      "embeddings_34.npy  tag_scores_10.npy  tag_scores_86.npy  tags_62.json\n",
      "embeddings_36.npy  tag_scores_11.npy  tag_scores_87.npy  tags_63.json\n",
      "embeddings_37.npy  tag_scores_12.npy  tag_scores_88.npy  tags_64.json\n",
      "embeddings_38.npy  tag_scores_13.npy  tag_scores_89.npy  tags_65.json\n",
      "embeddings_39.npy  tag_scores_14.npy  tag_scores_90.npy  tags_66.json\n",
      "embeddings_40.npy  tag_scores_15.npy  tag_scores_91.npy  tags_67.json\n",
      "embeddings_41.npy  tag_scores_16.npy  tag_scores_92.npy  tags_68.json\n",
      "embeddings_42.npy  tag_scores_17.npy  tag_scores_93.npy  tags_69.json\n",
      "embeddings_43.npy  tag_scores_18.npy  tag_scores_94.npy  tags_70.json\n",
      "embeddings_44.npy  tag_scores_19.npy  tag_scores_95.npy  tags_71.json\n",
      "embeddings_45.npy  tag_scores_20.npy  tag_scores_96.npy  tags_72.json\n",
      "embeddings_46.npy  tag_scores_21.npy  tag_scores_97.npy  tags_73.json\n",
      "embeddings_47.npy  tag_scores_22.npy  tag_scores_98.npy  tags_74.json\n",
      "embeddings_48.npy  tag_scores_23.npy  tag_scores_99.npy  tags_75.json\n",
      "embeddings_49.npy  tag_scores_24.npy  tags_00.json       tags_76.json\n",
      "embeddings_50.npy  tag_scores_25.npy  tags_01.json       tags_77.json\n",
      "embeddings_51.npy  tag_scores_26.npy  tags_02.json       tags_78.json\n",
      "embeddings_52.npy  tag_scores_27.npy  tags_03.json       tags_79.json\n",
      "embeddings_53.npy  tag_scores_28.npy  tags_04.json       tags_80.json\n",
      "embeddings_54.npy  tag_scores_29.npy  tags_05.json       tags_81.json\n",
      "embeddings_55.npy  tag_scores_30.npy  tags_06.json       tags_82.json\n",
      "embeddings_56.npy  tag_scores_31.npy  tags_07.json       tags_83.json\n",
      "embeddings_57.npy  tag_scores_32.npy  tags_08.json       tags_84.json\n",
      "embeddings_58.npy  tag_scores_33.npy  tags_09.json       tags_85.json\n",
      "embeddings_59.npy  tag_scores_34.npy  tags_10.json       tags_86.json\n",
      "embeddings_60.npy  tag_scores_36.npy  tags_11.json       tags_87.json\n",
      "embeddings_61.npy  tag_scores_37.npy  tags_12.json       tags_88.json\n",
      "embeddings_62.npy  tag_scores_38.npy  tags_13.json       tags_89.json\n",
      "embeddings_63.npy  tag_scores_39.npy  tags_14.json       tags_90.json\n",
      "embeddings_64.npy  tag_scores_40.npy  tags_15.json       tags_91.json\n",
      "embeddings_65.npy  tag_scores_41.npy  tags_16.json       tags_92.json\n",
      "embeddings_66.npy  tag_scores_42.npy  tags_17.json       tags_93.json\n",
      "embeddings_67.npy  tag_scores_43.npy  tags_18.json       tags_94.json\n",
      "embeddings_68.npy  tag_scores_44.npy  tags_19.json       tags_95.json\n",
      "embeddings_69.npy  tag_scores_45.npy  tags_20.json       tags_96.json\n",
      "embeddings_70.npy  tag_scores_46.npy  tags_21.json       tags_97.json\n",
      "embeddings_71.npy  tag_scores_47.npy  tags_22.json       tags_98.json\n",
      "embeddings_72.npy  tag_scores_48.npy  tags_23.json       tags_99.json\n",
      "embeddings_73.npy  tag_scores_49.npy  tags_24.json\n",
      "embeddings_74.npy  tag_scores_50.npy  tags_25.json\n"
     ]
    }
   ],
   "source": [
    "ls /datasets/beat2tweet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8926e06-ef8c-40a7-8675-f4d1285d0afe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration google--MusicCaps-bedc2a0fd7888f2f\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    }
   ],
   "source": [
    "ds = load_musiccaps(\n",
    "    \"./music_data\",\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True,\n",
    ")\n",
    "embeddings = np.load(\"embeddings.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78549b6c-f6f7-4d5e-94e8-efc52ebe686c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of music-related words to use for evaluation\n",
    "aspects = []\n",
    "for x in ds:\n",
    "    aspect_str = x[\"aspect_list\"]\n",
    "    for t in \"[]\\\"'\":\n",
    "        aspect_str = aspect_str.replace(t, \"\")\n",
    "    aspects.extend(aspect_str.split(\", \"))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# only pick aspects that show up somewhat frequently\n",
    "aspects = {s for s, count in Counter(aspects).most_common() if count >= 25}\n",
    "len(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15420704-a6bd-4315-b6b9-ba41fee54a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CaptionEmbedding(Dataset):\n",
    "    \"\"\"Returns a torch Dataset of paired captions and embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, muscaps_ds, embeddings):\n",
    "        ds = filter_muscaps_with_embeddings(muscaps_ds, embeddings)\n",
    "        self.captions = ds.sort(\"ytid\")[\"caption\"]\n",
    "        sorted_embs = [value for _, value in sorted(embeddings.items())]\n",
    "        self.embeddings = torch.from_numpy(np.stack(sorted_embs)).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.embeddings[idx]\n",
    "        assert len(emb) == 512\n",
    "        emb = (emb[:256] + emb[256:]) / 2\n",
    "\n",
    "        return self.captions[idx], emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0c18681-2a4f-41aa-b0ec-8d77a6880cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-65e36bf881a776b6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-b43b658bbefe199e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-f4182c0212b9d53a.arrow\n",
      "Parameter 'indices'=<generator object filter_muscaps_with_embeddings.<locals>.<genexpr> at 0x7fda767d7a50> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "with open('musiccaps_split.json', 'r') as fp:\n",
    "    musiccaps_split = json.load(fp)\n",
    "\n",
    "train_ytids, valid_ytids, test_ytids = musiccaps_split['train'], musiccaps_split['valid'], musiccaps_split['test']\n",
    "\n",
    "train_ds = ds.filter(lambda x: x['ytid'] in train_ytids)\n",
    "valid_ds = ds.filter(lambda x: x['ytid'] in valid_ytids)\n",
    "test_ds = ds.filter(lambda x: x['ytid'] in test_ytids)\n",
    "\n",
    "train_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in train_ytids}\n",
    "valid_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in valid_ytids}\n",
    "test_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in test_ytids}\n",
    "\n",
    "training_data = CaptionEmbedding(muscaps_ds=train_ds, embeddings=train_embeddings)\n",
    "valid_data = CaptionEmbedding(muscaps_ds=valid_ds, embeddings=valid_embeddings)\n",
    "test_data = CaptionEmbedding(muscaps_ds=test_ds, embeddings=test_embeddings) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4056d-c021-4438-8882-628aa11f7e42",
   "metadata": {},
   "source": [
    "N = len(ds['ytid'])\n",
    "shuffled_indices = torch.randperm(N)\n",
    "train_N, valid_N = round(N*0.8), round(N*0.1)\n",
    "train_indices, valid_indices, test_indices = shuffled_indices[:train_N], shuffled_indices[train_N:train_N+valid_N], shuffled_indices[train_N+valid_N:]\n",
    "train_ids, valid_ids, test_ids = ds[train_indices]['ytid'], ds[valid_indices]['ytid'], ds[test_indices]['ytid']\n",
    "split_json = {'train':train_ids, 'valid':valid_ids, 'test':test_ids}\n",
    "with open('musiccaps_split.json', 'w') as fp:\n",
    "    json.dump(split_json, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3d9b-6745-43f7-91b4-1860ca8ae81b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55718f8-fe01-463a-99ef-ab3dc223ed3f",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "target should be:\n",
    "\n",
    "`\"<bos> <pad> caption <eos> <mask...>\"` (first element is dropped in transformer.forward)\n",
    "\n",
    "input should be:\n",
    "\n",
    "`\"<bos> <music-emb> caption <eos> <pad...>\"` (last element is dropped in transformer.forward)\n",
    "\n",
    "where\n",
    "\n",
    "- `<bos>` = `<eos>` (for gpt2, see https://github.com/huggingface/transformers/issues/2026)\n",
    "- `<mask>` is -100 (masked in cross-entropy, see https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- `<pad>` is arbitrary (but needs to be valid embedding index)\n",
    "- `<music-emb>` is the encoded music\n",
    "\n",
    "to use a `<bos>` token, prepend it in `tokenize()`, set `music_emb_ind = 1`, and update the caption slicing in `eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82925589-cbb3-4f35-9099-3eb29831a29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.fc(torch.nn.functional.relu(x))\n",
    "    \n",
    "def B2T():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(256, 768),\n",
    "        ResidualLinear(768),\n",
    "        nn.Dropout(0.6),\n",
    "        ResidualLinear(768),\n",
    "        nn.Dropout(0.4),\n",
    "        ResidualLinear(768),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e0d47c-b1e4-4837-bc6b-5f9b20d84ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(captions_batch):\n",
    "    input_ids = tokenizer(captions_batch)[\"input_ids\"]\n",
    "\n",
    "    # wrap in eos and add placeholder for music embedding/mask\n",
    "    input_ids = [torch.tensor([placeholder_id] + x + [eos_id]) for x in input_ids]\n",
    "    # pad with -100, this index is masked in the cross-entropy loss\n",
    "    input_ids_target = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=mask_id\n",
    "    ).to(device)\n",
    "\n",
    "    # index -100 isn't valid as model input however, since the token embedding lookup fails,\n",
    "    # so we need a second version as model input, with -100 replaced with another token (shouldn't matter which)\n",
    "    input_ids = input_ids_target.clone()\n",
    "    input_ids[input_ids == mask_id] = eos_id\n",
    "\n",
    "    return input_ids, input_ids_target\n",
    "\n",
    "\n",
    "def transform_input_ids(music_embedding, input_ids, input_ids_target):\n",
    "    music_emb_ind = 0  # 1 if using <bos>, otherwise 0\n",
    "    assert (input_ids[:, music_emb_ind] == placeholder_id).all()\n",
    "    assert (input_ids_target[:, music_emb_ind] == placeholder_id).all()\n",
    "\n",
    "    input_ids_target[:, music_emb_ind] = eos_id  # mask_id\n",
    "    input_ids[:, music_emb_ind] = eos_id  # temp placeholder to make the embedding lookup work\n",
    "    inputs_embeds = model.transformer.wte(input_ids)\n",
    "\n",
    "    inputs_embeds[:, music_emb_ind] = b2t(music_embedding)  # insert music embedding\n",
    "\n",
    "    return inputs_embeds, input_ids_target\n",
    "\n",
    "\n",
    "def strip_eos(pred):\n",
    "    \"\"\" \n",
    "    remove eos tokens from predicted captions \n",
    "    discards everything after the first <eos> that isn't the very first token\n",
    "    (the hf can only skip eos but not stop at eos) \n",
    "    \"\"\"\n",
    "    pred = [p.removeprefix(\"<|endoftext|>\") for p in pred]\n",
    "    pred = [p[: p.find(\"<|endoftext|>\")] if \"<|endoftext|>\" in p else p for p in pred]\n",
    "    return pred\n",
    "\n",
    "\n",
    "def eval(caption_batch, embedding_batch, rm_eos=False, **kwargs):\n",
    "    model.eval()\n",
    "    input_ids, input_ids_target = tokenize(caption_batch)\n",
    "    inputs_embeds, _ = transform_input_ids(embedding_batch, input_ids, input_ids_target)\n",
    "\n",
    "    # only include <bos> (optional) and music_embedding, don't include true caption\n",
    "    inputs_embeds = inputs_embeds[:, :1]\n",
    "    output_ids = model.generate(inputs_embeds=inputs_embeds, **kwargs)\n",
    "    pred = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "    pred = [p.replace(\"\\n\", \"\").strip() for p in pred]\n",
    "    return strip_eos(pred) if rm_eos else pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eafbbd8e-d913-4059-b28a-ee995e78de74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_step(inputs_embeds, input_ids_target, apply_grad):\n",
    "    model.train()\n",
    "    loss = model.forward(inputs_embeds=inputs_embeds, labels=input_ids_target).loss\n",
    "    loss.backward()\n",
    "    \n",
    "    if apply_grad:\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def eval_step(string_info=\"\"):\n",
    "    \n",
    "    caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "    pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "    printr('[green bold]TRAIN TRUE: ' + caption_batch[0])\n",
    "    printr('[yellow]TRAIN PRED: ' + pred[0])\n",
    "    wlog('TRAIN TRUE: ' + caption_batch[0])\n",
    "    wlog('TRAIN PRED: ' + pred[0])\n",
    "    caption_batch, embedding_batch = next(iter(eval_valid_dataloader))\n",
    "    pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "    printr('[green bold]TEST TRUE: ' + caption_batch[0])\n",
    "    printr('[yellow]TEST PRED: ' + pred[0])\n",
    "    wlog('TEST TRUE: ' + caption_batch[0])\n",
    "    wlog('TEST PRED: ' + pred[0])\n",
    "    print()\n",
    "    \n",
    "def metrics_step(n=5, shuffles=10):\n",
    "    \n",
    "    shuffled_meteor_valid, shuffled_bleu_valid = 0., 0.\n",
    "    train_captions, train_preds = [], []\n",
    "    valid_captions, valid_preds = [], []\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "\n",
    "        caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        train_captions.append(caption_batch[0])\n",
    "        train_preds.append(pred[0])\n",
    "\n",
    "        caption_batch, embedding_batch = next(iter(eval_valid_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        valid_captions.append(caption_batch[0])\n",
    "        valid_preds.append(pred[0])\n",
    "\n",
    "    meteor_train = meteor.compute(predictions=train_preds, references=train_captions)['meteor']\n",
    "    bleu_train = google_bleu.compute(predictions=train_preds, references=train_captions)['google_bleu']\n",
    "\n",
    "    meteor_valid = meteor.compute(predictions=valid_preds, references=valid_captions)['meteor']\n",
    "    bleu_valid = google_bleu.compute(predictions=valid_preds, references=valid_captions)['google_bleu']\n",
    "    \n",
    "    for j in range(shuffles):\n",
    "        shuffled_captions = sorted(valid_captions, key=lambda k: random.random())\n",
    "        shuffled_meteor_valid += (1./(shuffles))*meteor.compute(predictions=valid_preds, references=shuffled_captions)['meteor']\n",
    "        shuffled_bleu_valid += (1./(shuffles))*google_bleu.compute(predictions=valid_preds, references=shuffled_captions)['google_bleu']\n",
    "\n",
    "    spec_meteor = meteor_valid - shuffled_meteor_valid\n",
    "    spec_bleu = bleu_valid - shuffled_bleu_valid\n",
    "        \n",
    "    print(f\"Train meteor: {meteor_train:.4f}, Train bleu: {bleu_train:.4f}\")\n",
    "    wlog(f\"Train meteor: {meteor_train:.4f}, Train bleu: {bleu_train:.4f}\")\n",
    "    print(f\"Valid meteor: {meteor_valid:.4f}, Valid bleu: {bleu_valid:.4f}\")\n",
    "    wlog(f\"Valid meteor: {meteor_valid:.4f}, Valid bleu: {bleu_valid:.4f}\")\n",
    "    print(f\"Valid spec-meteor: {spec_meteor:.4f}, Valid spec-bleu: {spec_bleu:.4f}\")\n",
    "    \n",
    "    return {'meteor_train': meteor_train,\n",
    "            'meteor_valid': meteor_valid,\n",
    "            'bleu_train': bleu_train,\n",
    "            'bleu_valid': bleu_valid,\n",
    "            'spec_meteor': spec_meteor,\n",
    "            'spec_bleu': spec_bleu} \n",
    "\n",
    "m_styles = {\"meteor_train\": (\"tab:orange\", \"solid\"),\n",
    "    \"meteor_valid\": (\"tab:orange\", \"dashed\"),\n",
    "    \"bleu_train\": (\"tab:blue\", \"solid\"),\n",
    "    \"bleu_valid\": (\"tab:blue\", \"dashed\"),\n",
    "    \"spec_bleu\": (\"tab:blue\", \"dashdot\"),\n",
    "    \"spec_meteor\": (\"tab:orange\", \"dashdot\")\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23c6b337-a156-4125-80fe-27229603dd46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'no_0.0001_0_0.0001'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'gpt2' # gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.config.is_decoder = True # not sure if necessary\n",
    "\n",
    "mask_id = -100 # don't change, this is fixed in torch cross-entropy loss!\n",
    "eos_id = tokenizer.eos_token_id\n",
    "placeholder_id = -200\n",
    "b2t = B2T().cuda()\n",
    "b2t_lr = 1e-4\n",
    "gpt2_finetune_lr = 1e-4\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "    # b2t needs to be the first parameter group\n",
    "    {'params': b2t.parameters(), 'lr': b2t_lr},\n",
    "    \n",
    "    # disable AdamW weight decay for gpt2 layer finetuning\n",
    "    {'params': model.transformer.h[1].parameters(), 'lr': gpt2_finetune_lr, 'weight_decay': 0},\n",
    "    {'params': model.transformer.h[2].parameters(), 'lr': gpt2_finetune_lr, 'weight_decay': 0},\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 500\n",
    "epoch = 185\n",
    "gradient_acc_fact = 1\n",
    "gpt2_finetune_start_epoch = 0\n",
    "\n",
    "generation_params = dict(\n",
    "    max_new_tokens=64,\n",
    "    num_beams=4,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    bos_token_id=eos_id,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=mask_id,\n",
    "    early_stopping=True,\n",
    "    rm_eos=True,\n",
    ")\n",
    "\n",
    "losses = []\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "eval_train_dataloader = DataLoader(training_data, 1, shuffle=True)\n",
    "eval_valid_dataloader = DataLoader(valid_data, 1, shuffle=True)\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load(\"google_bleu\")\n",
    "metrics = {'step': [], 'bleu_train': [], 'bleu_valid': [], 'meteor_train': [], 'meteor_valid': [],\n",
    "          'spec_bleu': [], 'spec_meteor':[]}\n",
    "\n",
    "checkpoint_dir = Path('checkpoints')\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "load_pretrain = False\n",
    "pretrain_model =  \"checkpoints/chkp_no_0.0001_0_0.0001_e245.pt\"\n",
    "if load_pretrain:\n",
    "    data_dict = torch.load(pretrain_model)\n",
    "    model.load_state_dict(data_dict['model'])\n",
    "    b2t.load_state_dict(data_dict['b2t'])\n",
    "    \n",
    "keep_training = False\n",
    "if keep_training:\n",
    "    with open(f'outputs/train_metrics_{string_info}.npy', 'rb') as f:\n",
    "        train_metrics = np.load(f, allow_pickle=True).item()\n",
    "    \n",
    "    losses = list(train_metrics['loss'])\n",
    "    metrics = {k: list(v) for k,v in train_metrics.items()}\n",
    "\n",
    "model_name_info = \"small\" if model_name == 'gpt2' else model_name\n",
    "pretraining_info = \"yes\" if load_pretrain else \"no\"\n",
    "string_info = f\"{pretraining_info}_{gpt2_finetune_lr}_{gpt2_finetune_start_epoch}_{b2t_lr}\"\n",
    "\n",
    "def wlog(s):\n",
    "    f = open(f\"outputs/logs_{string_info}.txt\",'a')\n",
    "    f.write(s+\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "string_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb04ec-e6a7-4afd-afaa-c1e15df0ed5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epoch, num_epochs)):\n",
    "    wlog(f\"\\nEpoch {epoch}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(), \n",
    "        \"b2t\": b2t.state_dict(), \n",
    "        \"opt\": opt\n",
    "    }, checkpoint_dir / f\"chkp_{string_info}.pt\")\n",
    "    #}, checkpoint_dir / f\"chkp_{epoch}.pt\")\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(), \n",
    "            \"b2t\": b2t.state_dict(), \n",
    "            \"opt\": opt\n",
    "        }, checkpoint_dir / f\"chkp_{string_info}_e{epoch}.pt\")\n",
    "\n",
    "    if epoch == gpt2_finetune_start_epoch:\n",
    "        print(\"Started finetuning b2t\")\n",
    "        wlog(\"Started finetuning b2t\")\n",
    "        torch.save({\n",
    "        \"model\": model.state_dict(), \n",
    "        \"b2t\": b2t.state_dict(), \n",
    "        \"opt\": opt\n",
    "        }, checkpoint_dir / f\"chkp_{string_info}.pt\")\n",
    "        opt.param_groups[0][\"lr\"] = b2t_lr\n",
    "        for pg in opt.param_groups[1:]:\n",
    "            pg[\"lr\"] = gpt2_finetune_lr\n",
    "\n",
    "    for step, (caption_batch, embedding_batch) in enumerate(tqdm(train_dataloader)):\n",
    "        # tokenize and prepare inputs for forward\n",
    "        input_ids, input_ids_target = tokenize(list(caption_batch))\n",
    "        inputs_embeds, input_ids_target = transform_input_ids(\n",
    "            embedding_batch, input_ids, input_ids_target\n",
    "        )\n",
    "\n",
    "        apply_grad_cond = step % gradient_acc_fact == 0\n",
    "        losses.append(\n",
    "            update_step(inputs_embeds, input_ids_target, apply_grad=apply_grad_cond)\n",
    "        )\n",
    "\n",
    "        if epoch % 5 == 0 and step % 500 == 0:\n",
    "            wlog(f\"Loss {np.mean(losses[-500:])}\\n\")\n",
    "            eval_step(string_info=string_info)\n",
    "                \n",
    "            plt.plot(losses, label='train_loss')\n",
    "            plt.savefig(f\"plots/plot_loss_{string_info}.png\")\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "            \n",
    "            metrics_results = metrics_step(n=50)\n",
    "            metrics['step'].append(len(losses))\n",
    "            for m in ['meteor_train', 'meteor_valid', 'bleu_train', 'bleu_valid', 'spec_bleu', 'spec_meteor']:\n",
    "                metrics[m].append(metrics_results[m])\n",
    "                plt.plot(metrics['step'], metrics[m], label=m, linestyle=m_styles[m][1], color=m_styles[m][0])\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"plots/plot_metrics_{string_info}.png\")\n",
    "            plt.show()\n",
    "            \n",
    "            with open(f'outputs/train_metrics_{string_info}.npy', 'wb') as f:\n",
    "                metrics_to_save = {k: np.array(a) for k, a in metrics.items()}\n",
    "                metrics_to_save['loss'] = np.array(losses)\n",
    "                np.save(f, metrics_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c89913-a442-40e1-b3a6-e6aa92219eb8",
   "metadata": {},
   "source": [
    "# Generate eval captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f98828e0-da74-4608-9528-493d1c462cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load('google_bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3eea662-6dd1-4e3f-ab68-5adef14597d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa22853c4ea4f2fb265e414c5b95c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01e105f91024041877d73acf99b9614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 0.09176385070004109 0.21041236368091049\n",
      "240 0.013262491707257809 0.02459959648551288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980e1f3b7ede450c926fd857c427ffd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245 0.09164398088063055 0.21363346802230665\n",
      "245 0.01235321176950216 0.024470595294925124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d5fb66655f4b279e4d3fe63c5efbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 0.0916267564865007 0.2151841683066235\n",
      "250 0.014104659248265997 0.02897780008661796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124142f4e0dd4b6583df44a062a92e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 0.09191405725832551 0.21574687256667727\n",
      "255 0.013624711813703463 0.029421440448513164\n"
     ]
    }
   ],
   "source": [
    "eval_true_captions = []\n",
    "eval_pred_captions = []\n",
    "\n",
    "for ep in tqdm([\"240\", \"245\", \"250\", \"255\"]):\n",
    "    # load epoch checkpoint\n",
    "    model_path = f\"checkpoints/chkp_no_0.0001_0_0.0001_e{ep}.pt\"\n",
    "    data_dict = torch.load(model_path)\n",
    "    model.load_state_dict(data_dict['model'])\n",
    "    b2t.load_state_dict(data_dict['b2t'])\n",
    "\n",
    "    eval_test_dataloader = DataLoader(test_data, 1, shuffle=False)\n",
    "\n",
    "    # generate a bunch of captions with this checkpoint\n",
    "    # for some reason hf generate() breaks atm when using batched captions, idk why\n",
    "    for i, (caption_batch, embedding_batch) in enumerate(tqdm(eval_test_dataloader)):\n",
    "        pred = eval(list(caption_batch), embedding_batch, **generation_params)\n",
    "        eval_true_captions.append(caption_batch[0])\n",
    "        eval_pred_captions.append(pred[0])\n",
    "        if i >= 1000:\n",
    "            break\n",
    "\n",
    "    gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_captions)['google_bleu']\n",
    "    meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_captions)['meteor']\n",
    "    print(ep, gleu_score, meteor_score)\n",
    "    \n",
    "    eval_true_shuffled = sorted(eval_true_captions, key=lambda k: random.random())\n",
    "\n",
    "    shuffled_gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['google_bleu']\n",
    "    shuffled_meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['meteor']\n",
    "    print(ep, gleu_score-shuffled_gleu_score, meteor_score-shuffled_meteor_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f65d1ba-a529-42aa-a032-78b525cbc986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302a84c8d5944258a70b107207b848e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f7c2bb6d5a480392666981fa80fd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 0.09301781424515645 0.21948975562326373\n",
      "260 0.012793587918638452 0.026416504783445094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955725d4cb2442879c3f4d989034c529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270 0.09167655415742967 0.21664909750262723\n",
      "270 0.01294979324549847 0.028667107157780647\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41962aeac7b14ee192ee91dc786855ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 0.09089179808423928 0.21627594449332305\n",
      "275 0.01227726257075086 0.026467379028567106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb74827391f45f19d555292696fa67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280 0.09235137201067033 0.2190253816800136\n",
      "280 0.013874978322179321 0.03065359068924778\n"
     ]
    }
   ],
   "source": [
    "eval_true_captions = []\n",
    "eval_pred_captions = []\n",
    "\n",
    "for ep in tqdm([\"260\", \"270\", \"275\", \"280\"]):\n",
    "    # load epoch checkpoint\n",
    "    model_path = f\"checkpoints/chkp_no_0.0001_0_0.0001_e{ep}.pt\"\n",
    "    data_dict = torch.load(model_path)\n",
    "    model.load_state_dict(data_dict['model'])\n",
    "    b2t.load_state_dict(data_dict['b2t'])\n",
    "\n",
    "    eval_test_dataloader = DataLoader(test_data, 1, shuffle=False)\n",
    "\n",
    "    # generate a bunch of captions with this checkpoint\n",
    "    # for some reason hf generate() breaks atm when using batched captions, idk why\n",
    "    for i, (caption_batch, embedding_batch) in enumerate(tqdm(eval_test_dataloader)):\n",
    "        pred = eval(list(caption_batch), embedding_batch, **generation_params)\n",
    "        eval_true_captions.append(caption_batch[0])\n",
    "        eval_pred_captions.append(pred[0])\n",
    "        if i >= 1000:\n",
    "            break\n",
    "\n",
    "    gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_captions)['google_bleu']\n",
    "    meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_captions)['meteor']\n",
    "    print(ep, gleu_score, meteor_score)\n",
    "    \n",
    "    eval_true_shuffled = sorted(eval_true_captions, key=lambda k: random.random())\n",
    "\n",
    "    shuffled_gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['google_bleu']\n",
    "    shuffled_meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['meteor']\n",
    "    print(ep, gleu_score-shuffled_gleu_score, meteor_score-shuffled_meteor_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3312cfe-2489-4d35-aa50-4be805f73236",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dict(\n",
    "    eval_true_captions=eval_true_captions,\n",
    "    eval_pred_captions=eval_pred_captions\n",
    "), open('preds.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2006b99-f8fa-4ea8-a590-d47ffe77950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_captions)['google_bleu']\n",
    "meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_captions)['meteor']\n",
    "\n",
    "gleu_score, meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b12edd-8f8a-4fed-a705-e1ee45975233",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_true_shuffled = sorted(eval_true_captions, key=lambda k: random.random())\n",
    "\n",
    "gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['google_bleu']\n",
    "meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['meteor']\n",
    "    \n",
    "gleu_score, meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f8eef5-c556-431a-b879-f844bb9e686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dict(\n",
    "    eval_true_captions=eval_true_captions,\n",
    "    eval_pred_captions=eval_pred_captions\n",
    "), open('preds.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2021731-e5ce-4a46-b107-d49b0f0da84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
