{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa39aacd-f77d-4cc9-ba72-dcfee9789a02",
   "metadata": {},
   "source": [
    "**Relevant huggingface gpt2 code**\n",
    "\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py\n",
    "- https://github.com/huggingface/transformers/issues/6535\n",
    "- bos/eos discussion: https://github.com/huggingface/transformers/issues/3311"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc102bb-e79b-44e8-8abc-56d4e67ffd87",
   "metadata": {},
   "source": [
    "**Some options for our main model**\n",
    "\n",
    "- different gpt2 sizes\n",
    "- gpt2 self-att vs gpt-2 cross-att (image captioning)\n",
    "- which gpt2 layers to finetune?\n",
    "- first pretrain on labels, then captions? or at the same time with different prompt/`<bos>` token?\n",
    "- make b2t output a bunch of 768 dimensional vectors that gpt2 self-att attends to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd60a10-aba2-4f3e-b251-77d4f100179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U yt-dlp==2023.1.6 matplotlib==3.6.0 datasets[audio] rich\n",
    "\n",
    "# install newest transformers build to be able to pass `inputs_embeds` through generate()\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f84841-b150-445f-8cf3-a9e5f6da2cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b32b1-a434-479e-8c62-92e8da10e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c26e4-1d78-4d2e-a49c-a578ba0b6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiccaps import load_musiccaps\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "\n",
    "from rich import print as printr\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import evaluate\n",
    "import itertools\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "f = open('logs.txt','a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485da5-3e40-42f1-873e-ccb244a74bc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load musiccaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6684ae9-0e53-415d-8be8-a67f90f7030e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_muscaps_with_embeddings(ds, embeddings):\n",
    "    \"\"\"Some clips weren't downloaded so we couldn't embed them, get rid of that\"\"\"\n",
    "    exclude_ids = set()\n",
    "    for i in range(len(ds)):\n",
    "        if ds[i][\"ytid\"] not in embeddings.keys():\n",
    "            exclude_ids.add(i)\n",
    "    ds = ds.select((i for i in range(len(ds)) if i not in set(exclude_ids)))\n",
    "    assert len(ds) == len(embeddings)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615f1a1-60e9-4c71-bccc-ed90674e3b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls /datasets/beat2tweet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8926e06-ef8c-40a7-8675-f4d1285d0afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = load_musiccaps(\n",
    "    \"./music_data\",\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True,\n",
    ")\n",
    "embeddings = np.load(\"embeddings.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78549b6c-f6f7-4d5e-94e8-efc52ebe686c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a list of music-related words to use for evaluation\n",
    "aspects = []\n",
    "for x in ds:\n",
    "    aspect_str = x[\"aspect_list\"]\n",
    "    for t in \"[]\\\"'\":\n",
    "        aspect_str = aspect_str.replace(t, \"\")\n",
    "    aspects.extend(aspect_str.split(\", \"))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# only pick aspects that show up somewhat frequently\n",
    "aspects = {s for s, count in Counter(aspects).most_common() if count >= 25}\n",
    "len(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15420704-a6bd-4315-b6b9-ba41fee54a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CaptionEmbedding(Dataset):\n",
    "    \"\"\"Returns a torch Dataset of paired captions and embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, muscaps_ds, embeddings):\n",
    "        ds = filter_muscaps_with_embeddings(muscaps_ds, embeddings)\n",
    "        self.captions = ds.sort(\"ytid\")[\"caption\"]\n",
    "        sorted_embs = [value for _, value in sorted(embeddings.items())]\n",
    "        self.embeddings = torch.from_numpy(np.stack(sorted_embs)).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.embeddings[idx]\n",
    "        assert len(emb) == 512\n",
    "        emb = (emb[:256] + emb[256:]) / 2\n",
    "\n",
    "        return self.captions[idx], emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c18681-2a4f-41aa-b0ec-8d77a6880cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('musiccaps_split.json', 'r') as fp:\n",
    "    musiccaps_split = json.load(fp)\n",
    "\n",
    "train_ytids, valid_ytids, test_ytids = musiccaps_split['train'], musiccaps_split['valid'], musiccaps_split['test']\n",
    "\n",
    "train_ds = ds.filter(lambda x: x['ytid'] in train_ytids)\n",
    "valid_ds = ds.filter(lambda x: x['ytid'] in valid_ytids)\n",
    "test_ds = ds.filter(lambda x: x['ytid'] in test_ytids)\n",
    "\n",
    "train_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in train_ytids}\n",
    "valid_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in valid_ytids}\n",
    "test_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in test_ytids}\n",
    "\n",
    "training_data = CaptionEmbedding(muscaps_ds=train_ds, embeddings=train_embeddings)\n",
    "valid_data = CaptionEmbedding(muscaps_ds=valid_ds, embeddings=valid_embeddings)\n",
    "test_data = CaptionEmbedding(muscaps_ds=test_ds, embeddings=test_embeddings) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4056d-c021-4438-8882-628aa11f7e42",
   "metadata": {},
   "source": [
    "N = len(ds['ytid'])\n",
    "shuffled_indices = torch.randperm(N)\n",
    "train_N, valid_N = round(N*0.8), round(N*0.1)\n",
    "train_indices, valid_indices, test_indices = shuffled_indices[:train_N], shuffled_indices[train_N:train_N+valid_N], shuffled_indices[train_N+valid_N:]\n",
    "train_ids, valid_ids, test_ids = ds[train_indices]['ytid'], ds[valid_indices]['ytid'], ds[test_indices]['ytid']\n",
    "split_json = {'train':train_ids, 'valid':valid_ids, 'test':test_ids}\n",
    "with open('musiccaps_split.json', 'w') as fp:\n",
    "    json.dump(split_json, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3d9b-6745-43f7-91b4-1860ca8ae81b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55718f8-fe01-463a-99ef-ab3dc223ed3f",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "target should be:\n",
    "\n",
    "`\"<bos> <pad> caption <eos> <mask...>\"` (first element is dropped in transformer.forward)\n",
    "\n",
    "input should be:\n",
    "\n",
    "`\"<bos> <music-emb> caption <eos> <pad...>\"` (last element is dropped in transformer.forward)\n",
    "\n",
    "where\n",
    "\n",
    "- `<bos>` = `<eos>` (for gpt2, see https://github.com/huggingface/transformers/issues/2026)\n",
    "- `<mask>` is -100 (masked in cross-entropy, see https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- `<pad>` is arbitrary (but needs to be valid embedding index)\n",
    "- `<music-emb>` is the encoded music\n",
    "\n",
    "to use a `<bos>` token, prepend it in `tokenize()`, set `music_emb_ind = 1`, and update the caption slicing in `eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82925589-cbb3-4f35-9099-3eb29831a29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.fc(torch.nn.functional.relu(x))\n",
    "    \n",
    "def B2T():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(256, 768),\n",
    "        ResidualLinear(768),\n",
    "        nn.Dropout(0.5),\n",
    "        ResidualLinear(768),\n",
    "        nn.Dropout(0.3),\n",
    "        ResidualLinear(768),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0d47c-b1e4-4837-bc6b-5f9b20d84ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(captions_batch):\n",
    "    input_ids = tokenizer(captions_batch)[\"input_ids\"]\n",
    "\n",
    "    # wrap in eos and add placeholder for music embedding/mask\n",
    "    input_ids = [torch.tensor([placeholder_id] + x + [eos_id]) for x in input_ids]\n",
    "    # pad with -100, this index is masked in the cross-entropy loss\n",
    "    input_ids_target = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=mask_id\n",
    "    ).to(device)\n",
    "\n",
    "    # index -100 isn't valid as model input however, since the token embedding lookup fails,\n",
    "    # so we need a second version as model input, with -100 replaced with another token (shouldn't matter which)\n",
    "    input_ids = input_ids_target.clone()\n",
    "    input_ids[input_ids == mask_id] = eos_id\n",
    "\n",
    "    return input_ids, input_ids_target\n",
    "\n",
    "\n",
    "def transform_input_ids(music_embedding, input_ids, input_ids_target):\n",
    "    music_emb_ind = 0  # 1 if using <bos>, otherwise 0\n",
    "    assert (input_ids[:, music_emb_ind] == placeholder_id).all()\n",
    "    assert (input_ids_target[:, music_emb_ind] == placeholder_id).all()\n",
    "\n",
    "    input_ids_target[:, music_emb_ind] = eos_id  # mask_id\n",
    "    input_ids[:, music_emb_ind] = eos_id  # temp placeholder to make the embedding lookup work\n",
    "    inputs_embeds = model.transformer.wte(input_ids)\n",
    "\n",
    "    inputs_embeds[:, music_emb_ind] = b2t(music_embedding)  # insert music embedding\n",
    "\n",
    "    return inputs_embeds, input_ids_target\n",
    "\n",
    "\n",
    "def strip_eos(pred):\n",
    "    \"\"\" \n",
    "    remove eos tokens from predicted captions \n",
    "    discards everything after the first <eos> that isn't the very first token\n",
    "    (the hf can only skip eos but not stop at eos) \n",
    "    \"\"\"\n",
    "    pred = [p.removeprefix(\"<|endoftext|>\") for p in pred]\n",
    "    pred = [p[: p.find(\"<|endoftext|>\")] if \"<|endoftext|>\" in p else p for p in pred]\n",
    "    return pred\n",
    "\n",
    "\n",
    "def eval(caption_batch, embedding_batch, rm_eos=False, **kwargs):\n",
    "    model.eval()\n",
    "    input_ids, input_ids_target = tokenize(caption_batch)\n",
    "    inputs_embeds, _ = transform_input_ids(embedding_batch, input_ids, input_ids_target)\n",
    "\n",
    "    # only include <bos> (optional) and music_embedding, don't include true caption\n",
    "    inputs_embeds = inputs_embeds[:, :1]\n",
    "    output_ids = model.generate(inputs_embeds=inputs_embeds, **kwargs)\n",
    "    pred = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "    pred = [p.replace(\"\\n\", \"\").strip() for p in pred]\n",
    "    return strip_eos(pred) if rm_eos else pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbbd8e-d913-4059-b28a-ee995e78de74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_step(inputs_embeds, input_ids_target, apply_grad):\n",
    "    model.train()\n",
    "    loss = model.forward(inputs_embeds=inputs_embeds, labels=input_ids_target).loss\n",
    "    loss.backward()\n",
    "    \n",
    "    if apply_grad:\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def eval_step(string_info=\"\"):\n",
    "    \n",
    "    caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "    pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "    printr('[green bold]TRAIN TRUE: ' + caption_batch[0])\n",
    "    printr('[yellow]TRAIN PRED: ' + pred[0])\n",
    "    wlog('TRAIN TRUE: ' + caption_batch[0])\n",
    "    wlog('TRAIN PRED: ' + pred[0])\n",
    "    caption_batch, embedding_batch = next(iter(eval_valid_dataloader))\n",
    "    pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "    printr('[green bold]TEST TRUE: ' + caption_batch[0])\n",
    "    printr('[yellow]TEST PRED: ' + pred[0])\n",
    "    wlog('TEST TRUE: ' + caption_batch[0])\n",
    "    wlog('TEST PRED: ' + pred[0])\n",
    "    print()\n",
    "    \n",
    "def metrics_step(n=5):\n",
    "    \n",
    "    meteor_train, meteor_valid = 0., 0.\n",
    "    bleu_train, bleu_valid = 0., 0.\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        caption_batch, embedding_batch = next(iter(eval_train_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        meteor_train += (1./n)*meteor.compute(predictions=pred, references=caption_batch)['meteor']\n",
    "        bleu_train += (1./n)*google_bleu.compute(predictions=pred, references=caption_batch)['google_bleu']\n",
    "        \n",
    "        caption_batch, embedding_batch = next(iter(eval_valid_dataloader))\n",
    "        pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "        meteor_valid += (1./n)*meteor.compute(predictions=pred, references=caption_batch)['meteor']\n",
    "        bleu_valid += (1./n)*google_bleu.compute(predictions=pred, references=caption_batch)['google_bleu']\n",
    "        \n",
    "    print(f\"Train meteor: {meteor_train:.4f}, Train google bleu: {bleu_train:.4f}\")\n",
    "    wlog(f\"Train meteor: {meteor_train:.4f}, Train google bleu: {bleu_train:.4f}\")\n",
    "    print(f\"Valid meteor: {meteor_valid:.4f}, Valid google bleu: {bleu_valid:.4f}\")\n",
    "    wlog(f\"Valid meteor: {meteor_valid:.4f}, Valid google bleu: {bleu_valid:.4f}\")\n",
    "    \n",
    "    return {'meteor_train': meteor_train,\n",
    "            'meteor_valid': meteor_valid,\n",
    "            'bleu_train': bleu_train,\n",
    "            'bleu_valid': bleu_valid} \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6b337-a156-4125-80fe-27229603dd46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'gpt2' # gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.config.is_decoder = True # not sure if necessary\n",
    "\n",
    "mask_id = -100 # don't change, this is fixed in torch cross-entropy loss!\n",
    "eos_id = tokenizer.eos_token_id\n",
    "placeholder_id = -200\n",
    "b2t = B2T().cuda()\n",
    "b2t_lr = 1e-4\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "    # b2t needs to be the first parameter group\n",
    "    {'params': b2t.parameters(), 'lr': b2t_lr},\n",
    "    \n",
    "    # disable AdamW weight decay for gpt2 layer finetuning\n",
    "    {'params': model.transformer.h[1].parameters(), 'lr': 0, 'weight_decay': 0},\n",
    "    {'params': model.transformer.h[2].parameters(), 'lr': 0, 'weight_decay': 0},\n",
    "])\n",
    "\n",
    "\n",
    "gpt2_finetune_lr = 2e-4\n",
    "batch_size = 32\n",
    "num_epochs = 1000\n",
    "epoch = 197\n",
    "gradient_acc_fact = 4\n",
    "gpt2_finetune_start_epoch = 0\n",
    "\n",
    "generation_params = dict(\n",
    "    max_new_tokens=64,\n",
    "    num_beams=4,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    bos_token_id=eos_id,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=mask_id,\n",
    "    early_stopping=True,\n",
    "    rm_eos=True,\n",
    ")\n",
    "\n",
    "losses = []\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "eval_train_dataloader = DataLoader(training_data, 1, shuffle=True)\n",
    "eval_valid_dataloader = DataLoader(valid_data, 1, shuffle=True)\n",
    "eval_test_dataloader = DataLoader(test_data, 1, shuffle=True)\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load(\"google_bleu\")\n",
    "metrics = {'step': [], 'bleu_train': [], 'bleu_valid': [], 'meteor_train': [], 'meteor_valid': []}\n",
    "\n",
    "checkpoint_dir = Path('checkpoints')\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "load_pretrain = True\n",
    "pretrain_model = f\"checkpoints/chkp_{string_info}-round1.pt\"\n",
    "if load_pretrain:\n",
    "    data_dict = torch.load(pretrain_model)\n",
    "    model.load_state_dict(data_dict['model'])\n",
    "    b2t.load_state_dict(data_dict['b2t'])\n",
    "\n",
    "model_name_info = \"small\" if model_name == 'gpt2' else model_name\n",
    "pretraining_info = \"yes\" if load_pretrain else \"no\"\n",
    "string_info = f\"{model_name_info}_{pretraining_info}_{gpt2_finetune_lr}_{gpt2_finetune_start_epoch}_{b2t_lr}\"\n",
    "\n",
    "def wlog(s):\n",
    "    f = open(f\"logs_{string_info}.txt\",'a')\n",
    "    f.write(s+\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "string_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac5131-8b97-4754-98ad-621f887509df",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_training = True\n",
    "if keep_training:\n",
    "    with open(f'train_metrics_{string_info}.npy', 'rb') as f:\n",
    "        train_metrics = np.load(f, allow_pickle=True).item()\n",
    "    \n",
    "    losses = list(train_metrics['loss'])\n",
    "    metrics = {k: list(v) for k,v in train_metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb04ec-e6a7-4afd-afaa-c1e15df0ed5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epoch, num_epochs)):\n",
    "    wlog(f\"\\nEpoch {epoch}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(), \n",
    "        \"b2t\": b2t.state_dict(), \n",
    "        \"opt\": opt\n",
    "    }, checkpoint_dir / f\"chkp_{string_info}.pt\")\n",
    "    #}, checkpoint_dir / f\"chkp_{epoch}.pt\")\n",
    "\n",
    "    if epoch == gpt2_finetune_start_epoch:\n",
    "        print(\"Started finetuning b2t\")\n",
    "        wlog(\"Started finetuning b2t\")\n",
    "        torch.save({\n",
    "        \"model\": model.state_dict(), \n",
    "        \"b2t\": b2t.state_dict(), \n",
    "        \"opt\": opt\n",
    "        }, checkpoint_dir / f\"chkp_{string_info}.pt\")\n",
    "        opt.param_groups[0][\"lr\"] = b2t_lr\n",
    "        for pg in opt.param_groups[1:]:\n",
    "            pg[\"lr\"] = gpt2_finetune_lr\n",
    "\n",
    "    for step, (caption_batch, embedding_batch) in enumerate(tqdm(train_dataloader)):\n",
    "        # tokenize and prepare inputs for forward\n",
    "        input_ids, input_ids_target = tokenize(list(caption_batch))\n",
    "        inputs_embeds, input_ids_target = transform_input_ids(\n",
    "            embedding_batch, input_ids, input_ids_target\n",
    "        )\n",
    "\n",
    "        apply_grad_cond = step % gradient_acc_fact == 0\n",
    "        losses.append(\n",
    "            update_step(inputs_embeds, input_ids_target, apply_grad=apply_grad_cond)\n",
    "        )\n",
    "\n",
    "        if epoch % 5 == 0 and step % 500 == 0:\n",
    "            wlog(f\"Loss {np.mean(losses[-500:])}\\n\")\n",
    "            eval_step(string_info=string_info)\n",
    "                \n",
    "            plt.plot(losses, label='train_loss')\n",
    "            plt.savefig(f\"plot_loss_{string_info}.png\")\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "            \n",
    "            metrics_results = metrics_step(n=20)\n",
    "            metrics['step'].append(len(losses))\n",
    "            for m in ['meteor_train', 'meteor_valid', 'bleu_train', 'bleu_valid']:\n",
    "                metrics[m].append(metrics_results[m])\n",
    "                plt.plot(metrics['step'], metrics[m], label=m)\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"plot_metrics_{string_info}.png\")\n",
    "            plt.show()\n",
    "            \n",
    "            with open(f'train_metrics_{string_info}.npy', 'wb') as f:\n",
    "                metrics_to_save = {k: np.array(a) for k, a in metrics.items()}\n",
    "                metrics_to_save['loss'] = np.array(losses)\n",
    "                np.save(f, metrics_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c89913-a442-40e1-b3a6-e6aa92219eb8",
   "metadata": {},
   "source": [
    "# Generate eval captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eea662-6dd1-4e3f-ab68-5adef14597d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_true_captions = []\n",
    "eval_pred_captions = []\n",
    "\n",
    "# load epoch checkpoint\n",
    "data_dict = torch.load(f\"checkpoints/chkp_{string_info}.pt\")\n",
    "model.load_state_dict(data_dict['model'])\n",
    "b2t.load_state_dict(data_dict['b2t'])\n",
    "\n",
    "# generate a bunch of captions with this checkpoint\n",
    "# for some reason hf generate() breaks atm when using batched captions, idk why\n",
    "for i, (caption_batch, embedding_batch) in enumerate(tqdm(eval_test_dataloader)):\n",
    "    pred = eval(caption_batch, embedding_batch, **generation_params)\n",
    "    eval_true_captions.append(caption_batch[0])\n",
    "    eval_pred_captions.append(pred[0])\n",
    "    if i >= 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3312cfe-2489-4d35-aa50-4be805f73236",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dict(\n",
    "    eval_true_captions=eval_true_captions,\n",
    "    eval_pred_captions=eval_pred_captions\n",
    "), open('preds.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2006b99-f8fa-4ea8-a590-d47ffe77950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load('google_bleu')\n",
    "\n",
    "gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_captions)['google_bleu']\n",
    "meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_captions)['meteor']\n",
    "    \n",
    "gleu_score, meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b12edd-8f8a-4fed-a705-e1ee45975233",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_true_shuffled = sorted(eval_true_captions, key=lambda k: random.random())\n",
    "\n",
    "gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['google_bleu']\n",
    "meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['meteor']\n",
    "    \n",
    "gleu_score, meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f8eef5-c556-431a-b879-f844bb9e686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dict(\n",
    "    eval_true_captions=eval_true_captions,\n",
    "    eval_pred_captions=eval_pred_captions\n",
    "), open('preds.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2021731-e5ce-4a46-b107-d49b0f0da84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
