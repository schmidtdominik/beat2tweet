{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd60a10-aba2-4f3e-b251-77d4f100179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#!pip install -U yt-dlp==2023.1.6 matplotlib==3.6.0 datasets[audio] rich\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
    "!pip install torch==1.13\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c26e4-1d78-4d2e-a49c-a578ba0b6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiccaps import load_musiccaps\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "\n",
    "from rich import print as printr\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import evaluate\n",
    "import itertools\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "f = open('logs.txt','a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485da5-3e40-42f1-873e-ccb244a74bc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load musiccaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6684ae9-0e53-415d-8be8-a67f90f7030e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_muscaps_with_embeddings(ds, embeddings):\n",
    "    \"\"\"Some clips weren't downloaded so we couldn't embed them, get rid of that\"\"\"\n",
    "    exclude_ids = set()\n",
    "    for i in range(len(ds)):\n",
    "        if ds[i][\"ytid\"] not in embeddings.keys():\n",
    "            exclude_ids.add(i)\n",
    "    ds = ds.select((i for i in range(len(ds)) if i not in set(exclude_ids)))\n",
    "    assert len(ds) == len(embeddings)\n",
    "    return ds\n",
    "\n",
    "ds = load_musiccaps(\n",
    "    \"./music_data\",\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True,\n",
    ")\n",
    "embeddings = np.load(\"embeddings.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78549b6c-f6f7-4d5e-94e8-efc52ebe686c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a list of music-related words to use for evaluation\n",
    "aspects = []\n",
    "for x in ds:\n",
    "    aspect_str = x[\"aspect_list\"]\n",
    "    for t in \"[]\\\"'\":\n",
    "        aspect_str = aspect_str.replace(t, \"\")\n",
    "    aspects.extend(aspect_str.split(\", \"))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# only pick aspects that show up somewhat frequently\n",
    "aspects = {s for s, count in Counter(aspects).most_common() if count >= 25}\n",
    "len(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15420704-a6bd-4315-b6b9-ba41fee54a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CaptionEmbedding(Dataset):\n",
    "    \"\"\"Returns a torch Dataset of paired captions and embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, muscaps_ds, embeddings):\n",
    "        ds = filter_muscaps_with_embeddings(muscaps_ds, embeddings)\n",
    "        self.captions = ds.sort(\"ytid\")[\"caption\"]\n",
    "        sorted_embs = [value for _, value in sorted(embeddings.items())]\n",
    "        self.embeddings = torch.from_numpy(np.stack(sorted_embs)).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.embeddings[idx]\n",
    "        assert len(emb) == 512\n",
    "        emb = (emb[:256] + emb[256:]) / 2\n",
    "\n",
    "        return self.captions[idx], emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c18681-2a4f-41aa-b0ec-8d77a6880cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('musiccaps_split.json', 'r') as fp:\n",
    "    musiccaps_split = json.load(fp)\n",
    "\n",
    "train_ytids, valid_ytids, test_ytids = musiccaps_split['train'], musiccaps_split['valid'], musiccaps_split['test']\n",
    "\n",
    "train_ds = ds.filter(lambda x: x['ytid'] in train_ytids)\n",
    "valid_ds = ds.filter(lambda x: x['ytid'] in valid_ytids)\n",
    "test_ds = ds.filter(lambda x: x['ytid'] in test_ytids)\n",
    "\n",
    "train_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in train_ytids}\n",
    "valid_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in valid_ytids}\n",
    "test_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in test_ytids}\n",
    "\n",
    "training_data = CaptionEmbedding(muscaps_ds=train_ds, embeddings=train_embeddings)\n",
    "valid_data = CaptionEmbedding(muscaps_ds=valid_ds, embeddings=valid_embeddings)\n",
    "test_data = CaptionEmbedding(muscaps_ds=test_ds, embeddings=test_embeddings) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3d9b-6745-43f7-91b4-1860ca8ae81b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82925589-cbb3-4f35-9099-3eb29831a29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.fc(torch.nn.functional.relu(x))\n",
    "    \n",
    "def B2T():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(256, 768),\n",
    "        ResidualLinear(768),\n",
    "        nn.Dropout(0.6),\n",
    "        ResidualLinear(768),\n",
    "        nn.Dropout(0.4),\n",
    "        ResidualLinear(768),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0d47c-b1e4-4837-bc6b-5f9b20d84ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(captions_batch):\n",
    "    input_ids = tokenizer(captions_batch)[\"input_ids\"]\n",
    "\n",
    "    # wrap in eos and add placeholder for music embedding/mask\n",
    "    input_ids = [torch.tensor([placeholder_id] + x + [eos_id]) for x in input_ids]\n",
    "    # pad with -100, this index is masked in the cross-entropy loss\n",
    "    input_ids_target = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=mask_id\n",
    "    ).to(device)\n",
    "\n",
    "    # index -100 isn't valid as model input however, since the token embedding lookup fails,\n",
    "    # so we need a second version as model input, with -100 replaced with another token (shouldn't matter which)\n",
    "    input_ids = input_ids_target.clone()\n",
    "    input_ids[input_ids == mask_id] = eos_id\n",
    "\n",
    "    return input_ids, input_ids_target\n",
    "\n",
    "\n",
    "def transform_input_ids(music_embedding, input_ids, input_ids_target):\n",
    "    music_emb_ind = 0  # 1 if using <bos>, otherwise 0\n",
    "    assert (input_ids[:, music_emb_ind] == placeholder_id).all()\n",
    "    assert (input_ids_target[:, music_emb_ind] == placeholder_id).all()\n",
    "\n",
    "    input_ids_target[:, music_emb_ind] = eos_id  # mask_id\n",
    "    input_ids[:, music_emb_ind] = eos_id  # temp placeholder to make the embedding lookup work\n",
    "    inputs_embeds = model.transformer.wte(input_ids)\n",
    "\n",
    "    inputs_embeds[:, music_emb_ind] = b2t(music_embedding)  # insert music embedding\n",
    "\n",
    "    return inputs_embeds, input_ids_target\n",
    "\n",
    "\n",
    "def strip_eos(pred):\n",
    "    \"\"\" \n",
    "    remove eos tokens from predicted captions \n",
    "    discards everything after the first <eos> that isn't the very first token\n",
    "    (the hf can only skip eos but not stop at eos) \n",
    "    \"\"\"\n",
    "    pred = [p.removeprefix(\"<|endoftext|>\") for p in pred]\n",
    "    pred = [p[: p.find(\"<|endoftext|>\")] if \"<|endoftext|>\" in p else p for p in pred]\n",
    "    return pred\n",
    "\n",
    "\n",
    "def eval(caption_batch, embedding_batch, rm_eos=False, **kwargs):\n",
    "    model.eval()\n",
    "    input_ids, input_ids_target = tokenize(caption_batch)\n",
    "    inputs_embeds, _ = transform_input_ids(embedding_batch, input_ids, input_ids_target)\n",
    "\n",
    "    # only include <bos> (optional) and music_embedding, don't include true caption\n",
    "    inputs_embeds = inputs_embeds[:, :1]\n",
    "    output_ids = model.generate(inputs_embeds=inputs_embeds, **kwargs)\n",
    "    pred = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "    pred = [p.replace(\"\\n\", \"\").strip() for p in pred]\n",
    "    return strip_eos(pred) if rm_eos else pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6b337-a156-4125-80fe-27229603dd46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'gpt2' # gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.config.is_decoder = True # not sure if necessary\n",
    "\n",
    "mask_id = -100 # don't change, this is fixed in torch cross-entropy loss!\n",
    "eos_id = tokenizer.eos_token_id\n",
    "placeholder_id = -200\n",
    "b2t = B2T().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c89913-a442-40e1-b3a6-e6aa92219eb8",
   "metadata": {},
   "source": [
    "# Generate eval captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aacacb-2361-4768-a600-23dde081f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saved-models/best_notag_noaug.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e40d5-f25e-4ae8-8cf3-d2bb1ef31198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "eval_train_dataloader = DataLoader(training_data, 1, shuffle=False)\n",
    "eval_valid_dataloader = DataLoader(valid_data, 1, shuffle=False)\n",
    "eval_test_dataloader = DataLoader(test_data, 1, shuffle=False)\n",
    "\n",
    "generation_params = dict(\n",
    "    max_new_tokens=64,\n",
    "    num_beams=4,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    bos_token_id=eos_id,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=mask_id,\n",
    "    early_stopping=True,\n",
    "    rm_eos=True,\n",
    ")\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load(\"google_bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebef6e7-e0a1-4d11-993a-e59cd2f94b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This song features an electric guitar as the main instrument. The guitar plays a descending run in the beginning then plays an arpeggiated chord followed by a double stop hammer on to a higher note and a descending slide followed by a descending chord run. The percussion plays a simple beat using rim shots. The percussion plays in common time. The bass plays only one note on the first count of each bar. The piano plays backing chords. There are no voices in this song. The mood of this song is relaxing. This song can be played in a coffee shop.\n",
      "\n",
      " low fidelity audio from a live performance featuring a solo direct input acoustic guitar strumming airy, suspended open chords. Also present are occasional ambient sounds, perhaps papers being shuffled.\n",
      "\n",
      " This middle eastern folk song features a male voice. This is accompanied by a string instrument called the oud playing the melody in between lines. A variety of middle-eastern percussion instruments are played in the background. A tambourine is played on every count. A darbuka plays a simple beat. This folk song can be played in a movie scene set in a Moroccan market.\n",
      "\n",
      " The low quality recording features a cover of a ballad song that consists of passionate, muddy male vocal singing over arpeggiated acoustic guitar melody. It sounds noisy, muddy and messy, as the frequencies are clashing, but it is also emotional and raw.\n",
      "\n",
      " The low quality recording features a R&B song playing on a speaker, judging by the reverberant and boomy sound of it. The song consists of passionate male vocal singing over shimmering hi hats, claps, groovy bass and \"4 on the floor\" kick pattern, It sounds passionate, like something you would dance to with your partner.\n",
      "\n",
      " This is an amateur recording of a female voice beatboxing. It is a funny recording. It could be playing in the background of comedic social media content.\n",
      "\n",
      " The low quality recording features a rock song instrumental that consists of shimmering hi hats, punchy snare, groovy bass guitar, acoustic rhythm guitar in the left channel and electric guitar chord progression in the right channel of the stereo image. It sounds energetic and addictive.\n",
      "\n",
      " This is a hip-hop music piece. There is a crisp synth sound playing the melody. The piece has a strong bass sound. There is a melodic pad with beat repeat effect that uses a piano sample. The rhythmic background is provided by a loud electronic drum beat. The piece is energetic and modern sounding. It could be used in the soundtrack of a sci-fi movie where there is a lot of action going on.\n",
      "\n",
      " Digital drums are playing a four on the floor rhythm with a kick on every beat along with a bassline and a keyboard sound playing short rhythmic chords and a e-guitar playing a simple melody along. A male voice is singing in a higher key. This song may be playing at a folkfest.\n",
      "\n",
      " This is an instrumental sitar music piece. The sitar is being played at a medium-to-high range. There is a calming and relaxing atmosphere to this piece. This piece can be used in the background of a meditation video. It could also be played during a meditation session at a course.\n"
     ]
    }
   ],
   "source": [
    "for caption in test_ds['caption'][:10]:\n",
    "    print(\"\\n\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f65d1ba-a529-42aa-a032-78b525cbc986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdd446750594cf4a1fd8a3e796fc1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['This is an instrumental progressive/math rock piece. The electric guitar is playing a complex solo with rapid procession of notes using the tapping technique. There is a reverberating piano sample in the background. The atmosphere is trippy. This piece can be used in the soundtrack of a video game that takes place in outer space.']\n",
      "\n",
      " ['The low quality recording features a shimmering bell melody that sounds haunting. There is a metallic impact. The recording sounds very noisy as it was probably recorded with a bad quality microphone.']\n",
      "\n",
      " ['The low quality recording features a live performance of a country song and it consists of harmonizing male vocals singing over groovy steel guitar, groovy mandoline melody and groovy acoustic rhythm guitar chords. There are some crowd chattering noises in the background. It sounds passionate, happy and fun.']\n",
      "\n",
      " ['The low quality recording features a lullaby sung by passionate male vocalists over an arpeggiated acoustic guitar melody. It sounds mellow, soft, sad and so emotional that you forget about how noisy the recording actually is.']\n",
      "\n",
      " ['This is an amateur recording of a boy singing along to a kids song playing on a TV. It is an educational song with a female voice in the lead. The melody is being played by an acoustic guitar while the rhythm is played by the electronic drums. The quality of the recording is quite poor. Samples lifted from']\n",
      "\n",
      " ['The low quality recording features a kid and a male laughing back and forth. There are some plastic impacts throughout it. The recording ends with a wide synth pad layered with sustained strings over which there is a low male vocal singing. It sounds joyful, happy and heartful.']\n",
      "\n",
      " ['The low quality recording features a live performance of a rock song and it features passionate male vocals, alongside harmonizing background vocals, singing over wide electric guitar melody, groovy bass guitar, punchy kick and snare hits and shimmering hi hats. It sounds groovy.']\n",
      "\n",
      " ['This song contains digital drums playing along with a funky synth bassline. Other synth sounds are playing pads and a lead melody. This song sounds like it was made for dancing. This song may be playing in a club.']\n",
      "\n",
      " ['A female singer sings this cool pop melody. The song is medium with a steady drumming rhythm, percussive bass line, keyboard accompaniment and various percussion hits. The song is modern K-pop. The song is romantic and cheerful.']\n",
      "\n",
      " ['The low quality recording features a live performance that consists of harmonizing mixed vocals singing over distorted bass and arpeggiated synth keys melody. It sounds crushed, harsh and the stereo image is unbalanced since the sound is leaning towards the right channel of the stereo image. There are some crowd excitement sounds in the background.']\n"
     ]
    }
   ],
   "source": [
    "eval_true_captions, eval_pred_captions = [], []\n",
    "\n",
    "data_dict = torch.load(MODEL_PATH)\n",
    "model.load_state_dict(data_dict['model'])\n",
    "b2t.load_state_dict(data_dict['b2t'])\n",
    "\n",
    "for i, (caption_batch, embedding_batch) in enumerate(tqdm(eval_test_dataloader)):\n",
    "    pred = eval(list(caption_batch), embedding_batch, **generation_params)\n",
    "    eval_true_captions.append(caption_batch[0])\n",
    "    eval_pred_captions.append(pred[0])\n",
    "    if i < 10: print(\"\\n\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3312cfe-2489-4d35-aa50-4be805f73236",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dict(\n",
    "    eval_true_captions=eval_true_captions,\n",
    "    eval_pred_captions=eval_pred_captions\n",
    "), open('outputs/preds.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2006b99-f8fa-4ea8-a590-d47ffe77950d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09148752549819551, 0.21553326620955834)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20b12edd-8f8a-4fed-a705-e1ee45975233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f42e562ffa844b39b88bb54958858b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_captions)['google_bleu']\n",
    "meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_captions)['meteor']\n",
    "\n",
    "spec_bleu, spec_meteor = [], []\n",
    "for _ in tqdm(range(10)):\n",
    "    eval_true_shuffled = sorted(eval_true_captions, key=lambda k: random.random())\n",
    "\n",
    "    shuffled_gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['google_bleu']\n",
    "    shuffled_meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['meteor']\n",
    "\n",
    "    spec_bleu.append(gleu_score-shuffled_gleu_score)\n",
    "    spec_meteor.append(meteor_score-shuffled_meteor_score)\n",
    "\n",
    "spec_bleu = np.array(spec_bleu)\n",
    "spec_meteor = np.array(spec_meteor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c010979-720f-4b27-ad07-896092f8516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test GLEU = {gleu_score:.3f}±{spec_bleu.std():.3f}\")\n",
    "print(f\"Test METEOR = {spec_meteor.mean():.3f}±{spec_meteor.std():.3f}\")\n",
    "print(f\"Test Spec-GLEU = {spec_bleu.mean():.3f}±{spec_bleu.std():.3f}\")\n",
    "print(f\"Test Spec-METEOR = {spec_meteor.mean():.3f}±{spec_meteor.std():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
