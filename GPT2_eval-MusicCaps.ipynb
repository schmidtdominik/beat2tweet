{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd60a10-aba2-4f3e-b251-77d4f100179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#!pip install -U yt-dlp==2023.1.6 matplotlib==3.6.0 datasets[audio] rich\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
    "!pip install torch==1.13\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7c26e4-1d78-4d2e-a49c-a578ba0b6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiccaps import load_musiccaps\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "\n",
    "from rich import print as printr\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import evaluate\n",
    "import itertools\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "f = open('logs.txt','a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485da5-3e40-42f1-873e-ccb244a74bc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load musiccaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6684ae9-0e53-415d-8be8-a67f90f7030e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration google--MusicCaps-bedc2a0fd7888f2f\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    }
   ],
   "source": [
    "def filter_muscaps_with_embeddings(ds, embeddings):\n",
    "    \"\"\"Some clips weren't downloaded so we couldn't embed them, get rid of that\"\"\"\n",
    "    exclude_ids = set()\n",
    "    for i in range(len(ds)):\n",
    "        if ds[i][\"ytid\"] not in embeddings.keys():\n",
    "            exclude_ids.add(i)\n",
    "    ds = ds.select((i for i in range(len(ds)) if i not in set(exclude_ids)))\n",
    "    assert len(ds) == len(embeddings)\n",
    "    return ds\n",
    "\n",
    "ds = load_musiccaps(\n",
    "    \"./music_data\",\n",
    "    sampling_rate=16000,\n",
    "    limit=None,\n",
    "    num_proc=8,\n",
    "    writer_batch_size=1000,\n",
    "    return_without_audio=True,\n",
    ")\n",
    "embeddings = np.load(\"embeddings.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78549b6c-f6f7-4d5e-94e8-efc52ebe686c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of music-related words to use for evaluation\n",
    "aspects = []\n",
    "for x in ds:\n",
    "    aspect_str = x[\"aspect_list\"]\n",
    "    for t in \"[]\\\"'\":\n",
    "        aspect_str = aspect_str.replace(t, \"\")\n",
    "    aspects.extend(aspect_str.split(\", \"))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# only pick aspects that show up somewhat frequently\n",
    "aspects = {s for s, count in Counter(aspects).most_common() if count >= 25}\n",
    "len(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15420704-a6bd-4315-b6b9-ba41fee54a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CaptionEmbedding(Dataset):\n",
    "    \"\"\"Returns a torch Dataset of paired captions and embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, muscaps_ds, embeddings):\n",
    "        ds = filter_muscaps_with_embeddings(muscaps_ds, embeddings)\n",
    "        self.captions = ds.sort(\"ytid\")[\"caption\"]\n",
    "        sorted_embs = [value for _, value in sorted(embeddings.items())]\n",
    "        self.embeddings = torch.from_numpy(np.stack(sorted_embs)).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.embeddings[idx]\n",
    "        assert len(emb) == 512\n",
    "        emb = (emb[:256] + emb[256:]) / 2\n",
    "\n",
    "        return self.captions[idx], emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c18681-2a4f-41aa-b0ec-8d77a6880cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-65e36bf881a776b6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-b43b658bbefe199e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/google___csv/google--MusicCaps-bedc2a0fd7888f2f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-f4182c0212b9d53a.arrow\n",
      "Parameter 'indices'=<generator object filter_muscaps_with_embeddings.<locals>.<genexpr> at 0x7f3f1e371900> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "with open('musiccaps_split.json', 'r') as fp:\n",
    "    musiccaps_split = json.load(fp)\n",
    "\n",
    "train_ytids, valid_ytids, test_ytids = musiccaps_split['train'], musiccaps_split['valid'], musiccaps_split['test']\n",
    "\n",
    "train_ds = ds.filter(lambda x: x['ytid'] in train_ytids)\n",
    "valid_ds = ds.filter(lambda x: x['ytid'] in valid_ytids)\n",
    "test_ds = ds.filter(lambda x: x['ytid'] in test_ytids)\n",
    "\n",
    "train_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in train_ytids}\n",
    "valid_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in valid_ytids}\n",
    "test_embeddings = {ytid: e for ytid, e in embeddings.items() if ytid in test_ytids}\n",
    "\n",
    "training_data = CaptionEmbedding(muscaps_ds=train_ds, embeddings=train_embeddings)\n",
    "valid_data = CaptionEmbedding(muscaps_ds=valid_ds, embeddings=valid_embeddings)\n",
    "test_data = CaptionEmbedding(muscaps_ds=test_ds, embeddings=test_embeddings) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3d9b-6745-43f7-91b4-1860ca8ae81b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82925589-cbb3-4f35-9099-3eb29831a29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.fc(torch.nn.functional.relu(x))\n",
    "    \n",
    "def B2T():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(256, 768),\n",
    "        ResidualLinear(768),\n",
    "        nn.Dropout(0.6),\n",
    "        ResidualLinear(768),\n",
    "        nn.Dropout(0.4),\n",
    "        ResidualLinear(768),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33e0d47c-b1e4-4837-bc6b-5f9b20d84ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(captions_batch):\n",
    "    input_ids = tokenizer(captions_batch)[\"input_ids\"]\n",
    "\n",
    "    # wrap in eos and add placeholder for music embedding/mask\n",
    "    input_ids = [torch.tensor([placeholder_id] + x + [eos_id]) for x in input_ids]\n",
    "    # pad with -100, this index is masked in the cross-entropy loss\n",
    "    input_ids_target = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=mask_id\n",
    "    ).to(device)\n",
    "\n",
    "    # index -100 isn't valid as model input however, since the token embedding lookup fails,\n",
    "    # so we need a second version as model input, with -100 replaced with another token (shouldn't matter which)\n",
    "    input_ids = input_ids_target.clone()\n",
    "    input_ids[input_ids == mask_id] = eos_id\n",
    "\n",
    "    return input_ids, input_ids_target\n",
    "\n",
    "\n",
    "def transform_input_ids(music_embedding, input_ids, input_ids_target):\n",
    "    music_emb_ind = 0  # 1 if using <bos>, otherwise 0\n",
    "    assert (input_ids[:, music_emb_ind] == placeholder_id).all()\n",
    "    assert (input_ids_target[:, music_emb_ind] == placeholder_id).all()\n",
    "\n",
    "    input_ids_target[:, music_emb_ind] = eos_id  # mask_id\n",
    "    input_ids[:, music_emb_ind] = eos_id  # temp placeholder to make the embedding lookup work\n",
    "    inputs_embeds = model.transformer.wte(input_ids)\n",
    "\n",
    "    inputs_embeds[:, music_emb_ind] = b2t(music_embedding)  # insert music embedding\n",
    "\n",
    "    return inputs_embeds, input_ids_target\n",
    "\n",
    "\n",
    "def strip_eos(pred):\n",
    "    \"\"\" \n",
    "    remove eos tokens from predicted captions \n",
    "    discards everything after the first <eos> that isn't the very first token\n",
    "    (the hf can only skip eos but not stop at eos) \n",
    "    \"\"\"\n",
    "    pred = [p.removeprefix(\"<|endoftext|>\") for p in pred]\n",
    "    pred = [p[: p.find(\"<|endoftext|>\")] if \"<|endoftext|>\" in p else p for p in pred]\n",
    "    return pred\n",
    "\n",
    "\n",
    "def eval(caption_batch, embedding_batch, rm_eos=False, **kwargs):\n",
    "    model.eval()\n",
    "    input_ids, input_ids_target = tokenize(caption_batch)\n",
    "    inputs_embeds, _ = transform_input_ids(embedding_batch, input_ids, input_ids_target)\n",
    "\n",
    "    # only include <bos> (optional) and music_embedding, don't include true caption\n",
    "    inputs_embeds = inputs_embeds[:, :1]\n",
    "    output_ids = model.generate(inputs_embeds=inputs_embeds, **kwargs)\n",
    "    pred = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "    pred = [p.replace(\"\\n\", \"\").strip() for p in pred]\n",
    "    return strip_eos(pred) if rm_eos else pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c6b337-a156-4125-80fe-27229603dd46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'gpt2' # gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.config.is_decoder = True # not sure if necessary\n",
    "\n",
    "mask_id = -100 # don't change, this is fixed in torch cross-entropy loss!\n",
    "eos_id = tokenizer.eos_token_id\n",
    "placeholder_id = -200\n",
    "b2t = B2T().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c89913-a442-40e1-b3a6-e6aa92219eb8",
   "metadata": {},
   "source": [
    "# Generate eval captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4aacacb-2361-4768-a600-23dde081f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saved-models/best_notag_noaug.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e47e40d5-f25e-4ae8-8cf3-d2bb1ef31198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "eval_train_dataloader = DataLoader(training_data, 1, shuffle=False)\n",
    "eval_valid_dataloader = DataLoader(valid_data, 1, shuffle=False)\n",
    "eval_test_dataloader = DataLoader(test_data, 1, shuffle=False)\n",
    "\n",
    "generation_params = dict(\n",
    "    max_new_tokens=64,\n",
    "    num_beams=4,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    bos_token_id=eos_id,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=mask_id,\n",
    "    early_stopping=True,\n",
    "    rm_eos=True,\n",
    ")\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "google_bleu = evaluate.load(\"google_bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ebef6e7-e0a1-4d11-993a-e59cd2f94b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This song features an electric guitar as the main instrument. The guitar plays a descending run in the beginning then plays an arpeggiated chord followed by a double stop hammer on to a higher note and a descending slide followed by a descending chord run. The percussion plays a simple beat using rim shots. The percussion plays in common time. The bass plays only one note on the first count of each bar. The piano plays backing chords. There are no voices in this song. The mood of this song is relaxing. This song can be played in a coffee shop.\n",
      "\n",
      " low fidelity audio from a live performance featuring a solo direct input acoustic guitar strumming airy, suspended open chords. Also present are occasional ambient sounds, perhaps papers being shuffled.\n",
      "\n",
      " This middle eastern folk song features a male voice. This is accompanied by a string instrument called the oud playing the melody in between lines. A variety of middle-eastern percussion instruments are played in the background. A tambourine is played on every count. A darbuka plays a simple beat. This folk song can be played in a movie scene set in a Moroccan market.\n",
      "\n",
      " The low quality recording features a cover of a ballad song that consists of passionate, muddy male vocal singing over arpeggiated acoustic guitar melody. It sounds noisy, muddy and messy, as the frequencies are clashing, but it is also emotional and raw.\n",
      "\n",
      " The low quality recording features a R&B song playing on a speaker, judging by the reverberant and boomy sound of it. The song consists of passionate male vocal singing over shimmering hi hats, claps, groovy bass and \"4 on the floor\" kick pattern, It sounds passionate, like something you would dance to with your partner.\n",
      "\n",
      " This is an amateur recording of a female voice beatboxing. It is a funny recording. It could be playing in the background of comedic social media content.\n",
      "\n",
      " The low quality recording features a rock song instrumental that consists of shimmering hi hats, punchy snare, groovy bass guitar, acoustic rhythm guitar in the left channel and electric guitar chord progression in the right channel of the stereo image. It sounds energetic and addictive.\n",
      "\n",
      " This is a hip-hop music piece. There is a crisp synth sound playing the melody. The piece has a strong bass sound. There is a melodic pad with beat repeat effect that uses a piano sample. The rhythmic background is provided by a loud electronic drum beat. The piece is energetic and modern sounding. It could be used in the soundtrack of a sci-fi movie where there is a lot of action going on.\n",
      "\n",
      " Digital drums are playing a four on the floor rhythm with a kick on every beat along with a bassline and a keyboard sound playing short rhythmic chords and a e-guitar playing a simple melody along. A male voice is singing in a higher key. This song may be playing at a folkfest.\n",
      "\n",
      " This is an instrumental sitar music piece. The sitar is being played at a medium-to-high range. There is a calming and relaxing atmosphere to this piece. This piece can be used in the background of a meditation video. It could also be played during a meditation session at a course.\n"
     ]
    }
   ],
   "source": [
    "for caption in test_ds['caption'][:10]:\n",
    "    print(\"\\n\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65d1ba-a529-42aa-a032-78b525cbc986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d76219cfe542c6b151bc08a6fba797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['This is an instrumental rock music piece. The piece is being performed on a clean sounding electric guitar. There is an easygoing guitar solo being played. The atmosphere is groovy. This piece could be used as an advertisement jingle.']\n",
      "\n",
      " ['This song features an electric guitar playing arpeggiated chords. There are open strings being played in the arpeggios. A chorus effect is added to the guitar sound. At the end of the song, one guitar chord is played. There are no voices in this song. This is an instrumental song. There']\n",
      "\n",
      " ['The low quality recording features a traditional song that consists of passionate female vocal singing over wooden percussive elements, strings and flute melody. It sounds passionate, emotional and soulful and the recording is noisy.']\n",
      "\n",
      " ['The low quality recording features a lullaby sung by passionate male vocalists over an arpeggiated acoustic guitar melody. It sounds mellow, soft, sad and so emotional that you forget about how noisy the recording actually is.']\n",
      "\n",
      " ['The low quality recording features a live performance of a pop song that consists of harmonizing male vocals singing, while there are crowd cheering noises in the background. It sounds passionate, heartfelt and joyful.']\n",
      "\n",
      " ['The low quality recording features a didgeridoo melody being played. The recording is noisy and in mono.']\n",
      "\n",
      " ['The low quality recording features a live performance of a rock song and it features passionate male vocals, alongside harmonizing background vocals, singing over wide electric guitar melody, groovy bass guitar, punchy kick and snare hits and shimmering hi hats. It sounds groovy and addictive.']\n",
      "\n",
      " ['The low quality recording features an electro song that consists of a widely spread stuttering synth melody, buzzy synth bass and distorted electric guitar melody. It sounds distorted, loud, suspenseful and energetic.']\n",
      "\n",
      " ['This pop song features a female voice singing the main melody. Other voices sing backing vocals in harmony. A synth plays a repetitive melody. This is accompanied by percussion playing a simple beat. The bass plays the root notes of the chords. Synth sounds are played in the background. This song can be played in a romantic']\n",
      "\n",
      " ['This is a meditation music piece. There is a didgeridoo playing a vague melody at the forefront while repetitive shamanic drum hits can be heard in the background. There is a spiritual atmosphere to this piece. It could be used in the soundtrack of a meditation video.']\n"
     ]
    }
   ],
   "source": [
    "eval_true_captions, eval_pred_captions = [], []\n",
    "\n",
    "data_dict = torch.load(MODEL_PATH)\n",
    "model.load_state_dict(data_dict['model'])\n",
    "b2t.load_state_dict(data_dict['b2t'])\n",
    "\n",
    "for i, (caption_batch, embedding_batch) in enumerate(tqdm(eval_test_dataloader)):\n",
    "    pred = eval(list(caption_batch), embedding_batch, **generation_params)\n",
    "    eval_true_captions.append(caption_batch[0])\n",
    "    eval_pred_captions.append(pred[0])\n",
    "    if i < 10: print(\"\\n\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3312cfe-2489-4d35-aa50-4be805f73236",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dict(\n",
    "    eval_true_captions=eval_true_captions,\n",
    "    eval_pred_captions=eval_pred_captions\n",
    "), open('outputs/preds.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2006b99-f8fa-4ea8-a590-d47ffe77950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_captions)['google_bleu']\n",
    "meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_captions)['meteor']\n",
    "\n",
    "gleu_score, meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b12edd-8f8a-4fed-a705-e1ee45975233",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    eval_true_shuffled = sorted(eval_true_captions, key=lambda k: random.random())\n",
    "\n",
    "    shuffled_gleu_score = google_bleu.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['google_bleu']\n",
    "    shuffled_meteor_score = meteor.compute(predictions=eval_pred_captions, references=eval_true_shuffled)['meteor']\n",
    "\n",
    "    shuffled_gleu_score-gleu_score, shuffled_meteor_score-meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2021731-e5ce-4a46-b107-d49b0f0da84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
